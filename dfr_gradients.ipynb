{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-04T09:37:46.166803100Z",
     "start_time": "2024-10-04T09:37:39.065201400Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.models import vit_b_16\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from surgeon_pytorch import Extract, get_nodes\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from captum.attr import LayerGradCam, LayerAttribution"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.reshape_transforms import vit_reshape_transform"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-04T09:37:48.670301100Z",
     "start_time": "2024-10-04T09:37:46.171832300Z"
    }
   },
   "id": "d3670c341d9acf83",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elmop\\anaconda3\\envs\\dfr\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elmop\\anaconda3\\envs\\dfr\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": "VisionTransformer(\n  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n  (encoder): Encoder(\n    (dropout): Dropout(p=0.0, inplace=False)\n    (layers): Sequential(\n      (encoder_layer_0): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_1): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_2): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_3): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_4): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_5): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_6): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_7): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_8): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_9): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_10): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_11): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  )\n  (heads): Sequential(\n    (head): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = vit_b_16(pretrained=True)\n",
    "NUM_CLASSES = 2\n",
    "model.heads.head = nn.Linear(768, NUM_CLASSES)\n",
    "model.load_state_dict(torch.load(\"logs/vit_waterbirds.pth\",weights_only=False,map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "model2 = vit_b_16(pretrained=True)\n",
    "model2.heads.head = nn.Linear(768, NUM_CLASSES)\n",
    "model2.load_state_dict(torch.load(\"logs/dfr_model.pth\", weights_only=False, map_location=torch.device('cpu')))\n",
    "model2.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-04T09:38:01.231462400Z",
     "start_time": "2024-10-04T09:37:48.674326400Z"
    }
   },
   "id": "1d8e65ba8924c34a",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def tensorize(img_path):\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    return preprocess(image).unsqueeze(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-04T09:38:01.365542800Z",
     "start_time": "2024-10-04T09:38:01.324543400Z"
    }
   },
   "id": "bae7943b39b3b45c",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0]) tensor([1])\n"
     ]
    }
   ],
   "source": [
    "img = tensorize(\"notebooks/data/054.Blue_Grosbeak/Blue_Grosbeak_0002_36648.jpg\")\n",
    "img2 = tensorize(\"notebooks/data/001.Black_footed_Albatross/Black_Footed_Albatross_0007_796138.jpg\")\n",
    "out = model(img)\n",
    "out2 = model(img2)\n",
    "_, predicted_class = out.max(dim=1)\n",
    "_, predicted_class2 = out2.max(dim=1)\n",
    "print(predicted_class, predicted_class2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-04T09:38:07.964875100Z",
     "start_time": "2024-10-04T09:38:01.392552800Z"
    }
   },
   "id": "a193984aa83ee99b",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-04T09:38:08.001891300Z",
     "start_time": "2024-10-04T09:38:07.961879Z"
    }
   },
   "id": "c3d9f13930ed2c85",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def reshape_transform(tensor):\n",
    "    tensor = tensor[:, 1:, :]\n",
    "    seq_len = tensor.size(1)\n",
    "    height = width = int(seq_len ** 0.5)\n",
    "    result = tensor.reshape(tensor.size(0), height, width, tensor.size(2))\n",
    "    # Permute to (batch_size, channels, height, width)\n",
    "    result = result.permute(0, 3, 1, 2)\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-04T09:38:08.033402500Z",
     "start_time": "2024-10-04T09:38:07.979883900Z"
    }
   },
   "id": "43601836ffd0c9ec",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "target_layers = [model.encoder.layers[-1].ln_1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-04T09:38:08.055404200Z",
     "start_time": "2024-10-04T09:38:08.025411100Z"
    }
   },
   "id": "2329a36edae77ed2",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "targets = [ClassifierOutputTarget(predicted_class.item())]\n",
    "targets2 = [ClassifierOutputTarget(predicted_class2.item())]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-04T09:38:08.102422700Z",
     "start_time": "2024-10-04T09:38:08.054402800Z"
    }
   },
   "id": "6bd3b61f1567c27e",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cam = GradCAM(model=model, target_layers=target_layers,reshape_transform=reshape_transform)\n",
    "cam2 = GradCAM(model=model2, target_layers=target_layers,reshape_transform=reshape_transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-04T09:38:08.170741100Z",
     "start_time": "2024-10-04T09:38:08.102422700Z"
    }
   },
   "id": "60bccff8879ee9a",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 3, 224, 224])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-04T09:38:08.171737500Z",
     "start_time": "2024-10-04T09:38:08.137745600Z"
    }
   },
   "id": "73bbd51fce89eac8",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "grayscale_cam = cam(input_tensor=img, targets=targets)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-04T09:38:10.450274400Z",
     "start_time": "2024-10-04T09:38:08.162739600Z"
    }
   },
   "id": "a876797708cb4c5c",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m greyscale_cam2 \u001B[38;5;241m=\u001B[39m \u001B[43mcam2\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_tensor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtargets\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\dfr\\lib\\site-packages\\pytorch_grad_cam\\base_cam.py:186\u001B[0m, in \u001B[0;36mBaseCAM.__call__\u001B[1;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001B[0m\n\u001B[0;32m    183\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m aug_smooth \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    184\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward_augmentation_smoothing(input_tensor, targets, eigen_smooth)\n\u001B[1;32m--> 186\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meigen_smooth\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\dfr\\lib\\site-packages\\pytorch_grad_cam\\base_cam.py:110\u001B[0m, in \u001B[0;36mBaseCAM.forward\u001B[1;34m(self, input_tensor, targets, eigen_smooth)\u001B[0m\n\u001B[0;32m     99\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward(retain_graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    101\u001B[0m \u001B[38;5;66;03m# In most of the saliency attribution papers, the saliency is\u001B[39;00m\n\u001B[0;32m    102\u001B[0m \u001B[38;5;66;03m# computed with a single target layer.\u001B[39;00m\n\u001B[0;32m    103\u001B[0m \u001B[38;5;66;03m# Commonly it is the last convolutional layer.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    108\u001B[0m \u001B[38;5;66;03m# use all conv layers for example, all Batchnorm layers,\u001B[39;00m\n\u001B[0;32m    109\u001B[0m \u001B[38;5;66;03m# or something else.\u001B[39;00m\n\u001B[1;32m--> 110\u001B[0m cam_per_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_cam_per_layer\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meigen_smooth\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maggregate_multi_layers(cam_per_layer)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\dfr\\lib\\site-packages\\pytorch_grad_cam\\base_cam.py:141\u001B[0m, in \u001B[0;36mBaseCAM.compute_cam_per_layer\u001B[1;34m(self, input_tensor, targets, eigen_smooth)\u001B[0m\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mlen\u001B[39m(grads_list):\n\u001B[0;32m    139\u001B[0m     layer_grads \u001B[38;5;241m=\u001B[39m grads_list[i]\n\u001B[1;32m--> 141\u001B[0m cam \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_cam_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_layer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlayer_activations\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlayer_grads\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meigen_smooth\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    142\u001B[0m cam \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmaximum(cam, \u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m    143\u001B[0m scaled \u001B[38;5;241m=\u001B[39m scale_cam_image(cam, target_size)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\dfr\\lib\\site-packages\\pytorch_grad_cam\\base_cam.py:66\u001B[0m, in \u001B[0;36mBaseCAM.get_cam_image\u001B[1;34m(self, input_tensor, target_layer, targets, activations, grads, eigen_smooth)\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_cam_image\u001B[39m(\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m     59\u001B[0m     input_tensor: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     64\u001B[0m     eigen_smooth: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m     65\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39mndarray:\n\u001B[1;32m---> 66\u001B[0m     weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_cam_weights\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_layer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactivations\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     67\u001B[0m     \u001B[38;5;66;03m# 2D conv\u001B[39;00m\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(activations\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m4\u001B[39m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\dfr\\lib\\site-packages\\pytorch_grad_cam\\grad_cam.py:23\u001B[0m, in \u001B[0;36mGradCAM.get_cam_weights\u001B[1;34m(self, input_tensor, target_layer, target_category, activations, grads)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_cam_weights\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m     17\u001B[0m                     input_tensor,\n\u001B[0;32m     18\u001B[0m                     target_layer,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     21\u001B[0m                     grads):\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;66;03m# 2D image\u001B[39;00m\n\u001B[1;32m---> 23\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[43mgrads\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m4\u001B[39m:\n\u001B[0;32m     24\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mmean(grads, axis\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m))\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;66;03m# 3D image\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "greyscale_cam2 = cam2(input_tensor=img, targets=targets)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-04T09:39:11.178557700Z",
     "start_time": "2024-10-04T09:39:08.036285900Z"
    }
   },
   "id": "dea3593fbb35f70f",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(img.shape)\n",
    "img_tensor = img[0,:,:,:]\n",
    "img_np = img_tensor.permute(1, 2, 0).numpy()\n",
    "img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "# Overlay heatmap\n",
    "visualization = show_cam_on_image(img_np, grayscale_cam[0, :], use_rgb=True)\n",
    "\n",
    "# Display the result\n",
    "plt.imshow(visualization)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-10-04T09:38:20.147788100Z"
    }
   },
   "id": "30dfee374dcd9402",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "mlp_block = model.encoder.layers[-1].mlp\n",
    "\n",
    "activations = {}\n",
    "gradients = {}\n",
    "\n",
    "def save_activation(name):\n",
    "    def hook(module, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "def save_gradient(name):\n",
    "    def hook(module, input, output):\n",
    "        gradients[name] = output[0].detach()\n",
    "    return hook\n",
    "\n",
    "mlp_block.register_forward_hook(save_activation(\"mlp_block\"))\n",
    "mlp_block.register_full_backward_hook(save_gradient(\"mlp_block\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-04T09:38:20.154791Z",
     "start_time": "2024-10-04T09:38:20.153787900Z"
    }
   },
   "id": "ecbf8bddafc5e8c7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "attention_module = model.encoder.layers[-1].self_attention\n",
    "\n",
    "attention_weights = {}\n",
    "\n",
    "def get_attention_weights(name):\n",
    "    def hook(module, input, output):\n",
    "        attn_output, attn_output_weights = output\n",
    "        attention_weights[name] = attn_output_weights.detach()\n",
    "    return hook\n",
    "\n",
    "attention_module.register_forward_hook(get_attention_weights(\"attn_weights\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-10-04T09:38:20.157793300Z"
    }
   },
   "id": "cbe0465c0e20e969",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "out = model(img)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-04T09:38:20.174784Z",
     "start_time": "2024-10-04T09:38:20.165790800Z"
    }
   },
   "id": "ba927c874a93506b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.encoder.layers[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-10-04T09:38:20.168789100Z"
    }
   },
   "id": "6a4b0b3bf8075d1b",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
