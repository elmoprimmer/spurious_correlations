{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83cd856c-7584-4f75-bc0b-13627e83d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import argparse\n",
    "\n",
    "external_repo_path = os.path.abspath('../../ide/external/transformer_explainability')\n",
    "\n",
    "if external_repo_path not in sys.path:\n",
    "    sys.path.append(external_repo_path)\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from baselines.ViT.ViT_LRP import vit_base_patch16_224 as vit_LRP\n",
    "from baselines.ViT.ViT_explanation_generator import LRP\n",
    "\n",
    "from external.pruning_by_explaining.my_datasets import WaterBirds, get_sample_indices_for_group, WaterBirdSubset, ISIC, ISICSubset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44eaacc-aad3-4e13-b2b0-0536cf03492a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "531bca15-e848-47a7-8dbd-890980fc4204",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    \"isic_30_3\": \"/home/primmere/ide/external/pruning_by_explaining/my_experiments/checkpoints/isic_30_3_r0.96.pth\",\n",
    "    \"isic_30_0123_minus_3\": \"/home/primmere/ide/external/pruning_by_explaining/my_experiments/checkpoints/isic_30_0123_minus_3_r0.96.pth\",\n",
    "    \"wb_30_12\": \"/home/primmere/ide/external/pruning_by_explaining/my_experiments/checkpoints/wb_30_12_r0.1.pth\",\n",
    "    \"wb_30_0123_minus_2_div_2\": \"/home/primmere/ide/external/pruning_by_explaining/my_experiments/checkpoints/wb_30_0123_minus_2_div_2_r0.3.pth\",\n",
    "    \"wb_30_0123_minus_2_div_2_0.1\": \"/home/primmere/ide/external/pruning_by_explaining/my_experiments/checkpoints/wb_30_0123_minus_2_div_2_r0.1.pth\",\n",
    "    \"wb_30_0123\": \"/home/primmere/ide/external/pruning_by_explaining/my_experiments/checkpoints/wb_30_0123_r0.1.pth\",\n",
    "    \"wb_30_0123_minus_1_div_2_0.1\": \"/home/primmere/ide/external/pruning_by_explaining/my_experiments/checkpoints/wb_30_0123_minus_1_div_2_r0.1.pth\",\n",
    "    \"wb_attn_30_0123\": \"/home/primmere/ide/external/pruning_by_explaining/my_experiments/checkpoints/wb_attn_30_0123_r0.3.pth\",\n",
    "    \"wb_attn_30_0123_0.1\": \"/home/primmere/ide/external/pruning_by_explaining/my_experiments/checkpoints/wb_attn_30_0123_r0.1.pth\",\n",
    "    \"wb_attn_30_0123_minus_1_div_2\": \"/home/primmere/ide/external/pruning_by_explaining/my_experiments/checkpoints/wb_30_0123_minus_1_div_2_r0.1.pth\",\n",
    "        \n",
    "    \"wb_linear_0123\": \"/home/primmere/ide/external/pruning_by_explaining/my_experiments/f_experiments/wb_Linear_g-0_1_2_3_n-100_seed-1/model_pruned_linear_r0.4.pth\",\n",
    "    \"wb_softmax_0123\": \"/home/primmere/ide/external/pruning_by_explaining/my_experiments/f_experiments/wb_Softmax_g-0_1_2_3_n-100_seed-1/model_pruned_softmax_r0.6.pth\"\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7c37901-b3fb-40fc-9c34-a68f8044dc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n",
      "Number of unique labels: 2, Number of unique places: 2, Total groups: 4\n",
      "group 0: 3518\n",
      "group 1: 185\n",
      "group 2: 55\n",
      "group 3: 1037\n",
      "target groups: [3]\n"
     ]
    }
   ],
   "source": [
    "dataset=\"wb\"\n",
    "pruned = True\n",
    "convert_bool = True\n",
    "n = 50\n",
    "model_name = 'wb_linear_0123'\n",
    "groups = [3]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "chkpt_path = d[model_name]\n",
    "\n",
    "if dataset == \"wb\":\n",
    "    if pruned:\n",
    "        chkpt_path = chkpt_path\n",
    "    else:\n",
    "        chkpt_path = \"/home/primmere/ide/dfr/logs/vit_waterbirds.pth\"\n",
    "if dataset == \"isic\":\n",
    "    if pruned:\n",
    "        chkpt_path = chkpt_path\n",
    "    else:\n",
    "        chkpt_path = '/home/primmere/logs/isic_logs_4/vit_isic_v2.pt'\n",
    "\n",
    "\n",
    "if dataset == \"wb\":\n",
    "    waterbirds = WaterBirds('/scratch_shared/primmere/waterbird', seed = 1, num_workers = 8)\n",
    "    train = waterbirds.get_train_set()\n",
    "    chosen_set = train\n",
    "if dataset == \"isic\":\n",
    "    isic = ISIC(\n",
    "        \"/scratch_shared/primmere/isic/isic_224/raw_224_with_selected\", \n",
    "        metadata_path='/scratch_shared/primmere/isic/metadata_w_split_v2_w_elmos_modifications.csv', \n",
    "        seed=1, \n",
    "        num_workers=8\n",
    "            )\n",
    "    val = isic.get_valid_set()\n",
    "    chosen_set = val\n",
    "\n",
    "\n",
    "indices = get_sample_indices_for_group(chosen_set, n, \"cuda\", groups)\n",
    "if dataset == \"wb\":\n",
    "    train_subset = WaterBirdSubset(chosen_set, indices)\n",
    "if dataset == \"isic\":\n",
    "    train_subset = ISICSubset(chosen_set, indices)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_subset, batch_size=n, shuffle=False, num_workers=8)\n",
    "x, y, g = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a922841-5e31-430f-a28b-d4c87005702c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b26c90fe-85e9-40ff-9a04-37adf3fda24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, Any\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def convert(sd: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Convert a ViT-style checkpoint that uses:\n",
    "      - LayerNorm stored under *.ln_*.module.{weight,bias}\n",
    "      - Attention split into q/k/v submodules with *.{q,k,v}_proj.proj_{weight,bias}\n",
    "      - Out proj under *.out_proj.proj_{weight,bias}\n",
    "    into a checkpoint that uses:\n",
    "      - *.ln_*.{weight,bias}  (no '.module')\n",
    "      - Self-attention fused 'in_proj_{weight,bias}' with Q,K,V concatenated on dim=0\n",
    "      - Out proj under *.out_proj.{weight,bias}\n",
    "\n",
    "    All other keys are copied as-is.\n",
    "\n",
    "    Args:\n",
    "        sd: source state_dict from \"checkpoint 2\".\n",
    "\n",
    "    Returns:\n",
    "        A new state_dict matching \"checkpoint 1\" naming/shapes.\n",
    "    \"\"\"\n",
    "    new_sd: Dict[str, torch.Tensor] = {}\n",
    "\n",
    "    # 1) Copy keys while normalizing simple renames and skipping q/k/v parts\n",
    "    for k, v in sd.items():\n",
    "        # Strip the extra '.module' in layer norms (per-layer and final encoder.ln)\n",
    "        k2 = (k.replace(\".ln_1.module.\", \".ln_1.\")\n",
    "                .replace(\".ln_2.module.\", \".ln_2.\")\n",
    "                .replace(\"encoder.ln.module.\", \"encoder.ln.\"))\n",
    "\n",
    "        # Rename out proj from *.proj_weight/bias -> *.weight/bias\n",
    "        k2 = (k2.replace(\".self_attention.out_proj.proj_weight\", \".self_attention.out_proj.weight\")\n",
    "                .replace(\".self_attention.out_proj.proj_bias\", \".self_attention.out_proj.bias\"))\n",
    "\n",
    "        # Skip q/k/v pieces; we'll fuse them after this loop.\n",
    "        if (\".self_attention.q_proj.proj_\" in k\n",
    "                or \".self_attention.k_proj.proj_\" in k\n",
    "                or \".self_attention.v_proj.proj_\" in k):\n",
    "            continue\n",
    "\n",
    "        # Regular copy (clone to avoid accidental aliasing)\n",
    "        new_sd[k2] = v.clone() if isinstance(v, torch.Tensor) else v\n",
    "\n",
    "    # 2) Build fused in_proj_{weight,bias} per encoder layer\n",
    "    #    Detect all layer indices that have q_proj weights.\n",
    "    layer_ids = set()\n",
    "    for k in sd.keys():\n",
    "        m = re.match(r\"encoder\\.layers\\.encoder_layer_(\\d+)\\.self_attention\\.q_proj\\.proj_weight\", k)\n",
    "        if m:\n",
    "            layer_ids.add(int(m.group(1)))\n",
    "\n",
    "    for i in sorted(layer_ids):\n",
    "        base = f\"encoder.layers.encoder_layer_{i}.self_attention\"\n",
    "\n",
    "        # Fetch q/k/v weights & biases\n",
    "        qw = sd[f\"{base}.q_proj.proj_weight\"]\n",
    "        kw = sd[f\"{base}.k_proj.proj_weight\"]\n",
    "        vw = sd[f\"{base}.v_proj.proj_weight\"]\n",
    "\n",
    "        qb = sd[f\"{base}.q_proj.proj_bias\"]\n",
    "        kb = sd[f\"{base}.k_proj.proj_bias\"]\n",
    "        vb = sd[f\"{base}.v_proj.proj_bias\"]\n",
    "\n",
    "        # Sanity checks (optional but helpful)\n",
    "        assert qw.shape == kw.shape == vw.shape, f\"q/k/v weight shapes differ at layer {i}\"\n",
    "        assert qb.shape == kb.shape == vb.shape, f\"q/k/v bias shapes differ at layer {i}\"\n",
    "        assert qw.dim() == 2 and qb.dim() == 1, f\"unexpected dims at layer {i}\"\n",
    "\n",
    "        # Concatenate in [Q; K; V] order along output-dim (dim=0)\n",
    "        in_w = torch.cat([qw, kw, vw], dim=0)  # [3*D, D]\n",
    "        in_b = torch.cat([qb, kb, vb], dim=0)  # [3*D]\n",
    "\n",
    "        new_sd[f\"{base}.in_proj_weight\"] = in_w\n",
    "        new_sd[f\"{base}.in_proj_bias\"] = in_b\n",
    "\n",
    "    return new_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02136a98-9380-4496-a241-1ae8f5164161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def _to_pil(img):\n",
    "    \"\"\"\n",
    "    Convert a variety of common image formats (PIL, np.ndarray, torch.Tensor)\n",
    "    into a PIL.Image without changing the spatial content.\n",
    "\n",
    "    Assumptions handled:\n",
    "    - Channel-first arrays/tensors (C,H,W) with C in {1,3}\n",
    "    - Channel-last arrays (H,W,C)\n",
    "    - Float data in [0,1] or [-1,1] (auto-clipped to [0,1])\n",
    "    - uint8 arrays already in [0,255]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        is_tensor = isinstance(img, torch.Tensor)\n",
    "    except Exception:\n",
    "        is_tensor = False\n",
    "\n",
    "    if isinstance(img, Image.Image):\n",
    "        return img\n",
    "\n",
    "    if is_tensor:\n",
    "        img = img.detach().cpu().numpy()\n",
    "\n",
    "    if isinstance(img, np.ndarray):\n",
    "        arr = img\n",
    "\n",
    "        # Move channel-first -> channel-last if needed\n",
    "        if arr.ndim == 3 and arr.shape[0] in (1, 3) and arr.shape[0] != arr.shape[-1]:\n",
    "            arr = np.transpose(arr, (1, 2, 0))\n",
    "\n",
    "        # If single-channel with shape (H,W,1), squeeze to (H,W)\n",
    "        if arr.ndim == 3 and arr.shape[-1] == 1:\n",
    "            arr = arr[..., 0]\n",
    "\n",
    "        # Normalize to uint8 if float\n",
    "        if np.issubdtype(arr.dtype, np.floating):\n",
    "            # Handle possible [-1,1] range by clipping to [0,1]\n",
    "            arr = np.clip(arr, 0.0, 1.0)\n",
    "            arr = (arr * 255.0 + 0.5).astype(np.uint8)\n",
    "        elif arr.dtype != np.uint8:\n",
    "            # Generic cast (e.g., int16) with clipping\n",
    "            info = np.iinfo(arr.dtype) if np.issubdtype(arr.dtype, np.integer) else None\n",
    "            if info is not None:\n",
    "                arr = np.clip(arr, info.min, info.max)\n",
    "                # Try to scale to 0..255 if not already in that range\n",
    "                arr = arr.astype(np.float32)\n",
    "                arr -= arr.min()\n",
    "                maxv = arr.max() if arr.max() != 0 else 1.0\n",
    "                arr = (arr / maxv * 255.0 + 0.5).astype(np.uint8)\n",
    "            else:\n",
    "                # Fallback\n",
    "                arr = arr.astype(np.uint8)\n",
    "\n",
    "        return Image.fromarray(arr)\n",
    "\n",
    "    raise TypeError(f\"Unsupported image type: {type(img)}\")\n",
    "\n",
    "\n",
    "def save_original_image(original_image, filename, out_dir):\n",
    "    \"\"\"\n",
    "    Save the original (pre-heatmap) image into a sibling folder named 'orig_imgs'\n",
    "    using the SAME filename you use for the heatmap (e.g., '{group}_{ID}.png').\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    original_image : PIL.Image | np.ndarray | torch.Tensor\n",
    "        The raw/original image before overlays.\n",
    "    filename : str\n",
    "        The exact filename you use for the heatmap image (e.g., f\"{group}_{sample_id}.png\").\n",
    "    out_dir : str | Path\n",
    "        The folder where you currently save your heatmaps. 'orig_imgs' will be created inside this folder.\n",
    "    \"\"\"\n",
    "    orig_dir = os.path.join(out_dir, \"orig_imgs\")\n",
    "    os.makedirs(orig_dir, exist_ok=True)\n",
    "\n",
    "    save_path = os.path.join(orig_dir, filename)\n",
    "    pil_img = _to_pil(original_image)\n",
    "    pil_img.save(save_path)\n",
    "    return save_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd2c7130-848c-473f-bbd4-aae6e83509ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-2438_0_w.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-2438_1_w.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1217_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1217_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-3387_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-3387_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-3453_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-3453_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1571_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1571_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-57_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-57_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-111_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-111_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1052_0_w.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1052_1_w.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-3380_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-3380_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-3420_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-3420_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-2416_0_w.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-2416_1_w.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-2062_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-2062_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-2387_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-2387_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1575_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1575_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1536_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1536_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-2411_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-2411_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-571_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-571_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1083_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1083_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1117_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1117_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-2413_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-2413_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-180_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-180_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1532_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1532_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-2424_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-2424_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-2535_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-2535_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-2117_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-2117_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1206_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1206_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-2118_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-2118_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-40_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-40_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1726_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1726_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-2063_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-2063_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-2089_0_w.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-2089_1_w.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-3358_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-3358_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-21_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-21_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1119_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1119_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1412_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1412_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-3505_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-3505_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-3468_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-3468_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-569_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-569_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-157_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-157_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-64_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-64_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1202_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1202_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-28_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-28_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1550_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1550_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-3517_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-3517_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-155_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-155_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-2396_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-2396_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1686_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1686_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1544_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-1544_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-3448_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-3448_1_c.png\n",
      "Visualization for '0' saved at: imgs_wb_pruned_wb_linear_0123/group-3-3480_0_c.png\n",
      "Visualization for '1' saved at: imgs_wb_pruned_wb_linear_0123/group-3-3480_1_c.png\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(chkpt_path, map_location=torch.device('cuda'), weights_only=False)\n",
    "if pruned and convert_bool:\n",
    "    checkpoint = convert(checkpoint)\n",
    "model = vit_LRP(pretrained=True, num_classes=2)\n",
    "\n",
    "new_checkpoint = {}\n",
    "\n",
    "# fix ckpt to be suitable for their model\n",
    "for k, v in checkpoint.items():\n",
    "    new_key = k\n",
    "    new_key = new_key.replace('class_token', 'cls_token')\n",
    "    new_key = new_key.replace('conv_proj', 'patch_embed.proj')\n",
    "    new_key = new_key.replace('encoder.pos_embedding', 'pos_embed')\n",
    "    new_key = new_key.replace('encoder.ln.weight', 'norm.weight')\n",
    "    new_key = new_key.replace('encoder.ln.bias', 'norm.bias')\n",
    "    new_key = new_key.replace('heads.head.weight', 'head.weight')\n",
    "    new_key = new_key.replace('heads.head.bias', 'head.bias')\n",
    "\n",
    "    new_key = new_key.replace('encoder.layers.encoder_layer_', 'blocks.')\n",
    "    new_key = new_key.replace('.ln_1', '.norm1')\n",
    "    new_key = new_key.replace('.ln_2', '.norm2')\n",
    "    new_key = new_key.replace('.self_attention.in_proj_weight', '.attn.qkv.weight')\n",
    "    new_key = new_key.replace('.self_attention.in_proj_bias', '.attn.qkv.bias')\n",
    "    new_key = new_key.replace('.self_attention.out_proj.weight', '.attn.proj.weight')\n",
    "    new_key = new_key.replace('.self_attention.out_proj.bias', '.attn.proj.bias')\n",
    "    new_key = new_key.replace('.mlp.0', '.mlp.fc1')\n",
    "    new_key = new_key.replace('.mlp.3', '.mlp.fc2')\n",
    "\n",
    "    new_checkpoint[new_key] = v\n",
    "\n",
    "model.load_state_dict(new_checkpoint)\n",
    "model = model.cuda()\n",
    "\n",
    "attribution_generator = LRP(model)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "def tensorize(img_path):\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    return preprocess(image)\n",
    "\n",
    "\n",
    "def show_cam_on_image(img, mask):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    return cam\n",
    "\n",
    "\n",
    "def generate_visualization(original_image, class_index=None):\n",
    "    transformer_attribution = attribution_generator.generate_LRP(original_image.unsqueeze(0).cuda(),\n",
    "                                                                 method=\"transformer_attribution\",\n",
    "                                                                 index=class_index).detach()\n",
    "    transformer_attribution = transformer_attribution.reshape(1, 1, 14, 14)\n",
    "    transformer_attribution = torch.nn.functional.interpolate(transformer_attribution, scale_factor=16, mode='bilinear')\n",
    "    transformer_attribution = transformer_attribution.reshape(224, 224).data.cpu().numpy()\n",
    "    transformer_attribution = (transformer_attribution - transformer_attribution.min()) / (\n",
    "            transformer_attribution.max() - transformer_attribution.min())\n",
    "    # flip values\n",
    "    transformer_attribution = 1 - transformer_attribution\n",
    "\n",
    "    image_transformer_attribution = original_image.permute(1, 2, 0).data.cpu().numpy()\n",
    "    image_transformer_attribution = (image_transformer_attribution - image_transformer_attribution.min()) / (\n",
    "            image_transformer_attribution.max() - image_transformer_attribution.min())\n",
    "    vis = show_cam_on_image(image_transformer_attribution, transformer_attribution)\n",
    "    vis = np.uint8(255 * vis)\n",
    "    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
    "    return vis\n",
    "\n",
    "\n",
    "for i in range(len(x)):\n",
    "    idx = i\n",
    "    img = x[idx]\n",
    "\n",
    "    if pruned:\n",
    "        output_dir = f\"imgs_{dataset}_pruned_{model_name}\"\n",
    "    else:\n",
    "        output_dir = f\"imgs_{dataset}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # check if model is correct\n",
    "    logits = model(x[idx].unsqueeze(0).cuda())\n",
    "    pred = torch.argmax(logits, dim=1)\n",
    "    if pred == y[idx].item():\n",
    "        corr = \"c\"\n",
    "    else:\n",
    "        corr = \"w\"\n",
    "    \n",
    "    img_name = f\"group-{g[idx]}-{indices[idx]}\"\n",
    "    save_original_image(img, img_name+\".png\", \"\")\n",
    "    # Generate visualization for the class predicted by the model (e.g., 'land')\n",
    "    cat = generate_visualization(img, class_index=0)\n",
    "    land_filename = img_name + f\"_0_{corr}.png\"\n",
    "    cat_output_path = os.path.join(output_dir, land_filename)\n",
    "    cv2.imwrite(cat_output_path, cat)\n",
    "    \n",
    "    # Generate visualization for class index 1 (e.g., 'water')\n",
    "    dog = generate_visualization(img, class_index=1)\n",
    "    water_filename = img_name + f\"_1_{corr}.png\"\n",
    "    dog_output_path = os.path.join(output_dir, water_filename)\n",
    "    cv2.imwrite(dog_output_path, dog)\n",
    "    \n",
    "    print(f\"Visualization for '0' saved at: {cat_output_path}\")  # cat\n",
    "    print(f\"Visualization for '1' saved at: {dog_output_path}\")  # dog\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfr2",
   "language": "python",
   "name": "dfr2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
