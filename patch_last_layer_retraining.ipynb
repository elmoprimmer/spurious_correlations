{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-20T13:15:16.771918100Z",
     "start_time": "2024-08-20T13:15:06.682364400Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vit_b_16\n",
    "import torch\n",
    "\n",
    "import argparse\n",
    "\n",
    "from wb_data import WaterBirdsDataset, get_loader, get_transform_cub, log_data\n",
    "from utils import evaluate, get_y_p"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-20T13:22:18.207366300Z",
     "start_time": "2024-08-20T13:22:17.146289Z"
    }
   },
   "id": "9235fc13369662c2",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = vit_b_16(weights='DEFAULT')\n",
    "model.heads.head = nn.Linear(model.heads.head.in_features, NUM_CLASSES)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-20T13:25:52.135739600Z",
     "start_time": "2024-08-20T13:25:43.670607400Z"
    }
   },
   "id": "dc9a14f6594c9ad6",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "VisionTransformer(\n  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n  (encoder): Encoder(\n    (dropout): Dropout(p=0.0, inplace=False)\n    (layers): Sequential(\n      (encoder_layer_0): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_1): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_2): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_3): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_4): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_5): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_6): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_7): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_8): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_9): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_10): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_11): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  )\n  (heads): Sequential(\n    (head): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-22T13:19:40.336959400Z",
     "start_time": "2024-08-22T13:19:40.275923300Z"
    }
   },
   "id": "e38aa178dabda7a3",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_token False\n",
      "conv_proj.weight False\n",
      "conv_proj.bias False\n",
      "encoder.pos_embedding False\n",
      "encoder.layers.encoder_layer_0.ln_1.weight False\n",
      "encoder.layers.encoder_layer_0.ln_1.bias False\n",
      "encoder.layers.encoder_layer_0.self_attention.in_proj_weight False\n",
      "encoder.layers.encoder_layer_0.self_attention.in_proj_bias False\n",
      "encoder.layers.encoder_layer_0.self_attention.out_proj.weight False\n",
      "encoder.layers.encoder_layer_0.self_attention.out_proj.bias False\n",
      "encoder.layers.encoder_layer_0.ln_2.weight False\n",
      "encoder.layers.encoder_layer_0.ln_2.bias False\n",
      "encoder.layers.encoder_layer_0.mlp.0.weight True\n",
      "encoder.layers.encoder_layer_0.mlp.0.bias True\n",
      "encoder.layers.encoder_layer_0.mlp.3.weight True\n",
      "encoder.layers.encoder_layer_0.mlp.3.bias True\n",
      "encoder.layers.encoder_layer_1.ln_1.weight False\n",
      "encoder.layers.encoder_layer_1.ln_1.bias False\n",
      "encoder.layers.encoder_layer_1.self_attention.in_proj_weight False\n",
      "encoder.layers.encoder_layer_1.self_attention.in_proj_bias False\n",
      "encoder.layers.encoder_layer_1.self_attention.out_proj.weight False\n",
      "encoder.layers.encoder_layer_1.self_attention.out_proj.bias False\n",
      "encoder.layers.encoder_layer_1.ln_2.weight False\n",
      "encoder.layers.encoder_layer_1.ln_2.bias False\n",
      "encoder.layers.encoder_layer_1.mlp.0.weight True\n",
      "encoder.layers.encoder_layer_1.mlp.0.bias True\n",
      "encoder.layers.encoder_layer_1.mlp.3.weight True\n",
      "encoder.layers.encoder_layer_1.mlp.3.bias True\n",
      "encoder.layers.encoder_layer_2.ln_1.weight False\n",
      "encoder.layers.encoder_layer_2.ln_1.bias False\n",
      "encoder.layers.encoder_layer_2.self_attention.in_proj_weight False\n",
      "encoder.layers.encoder_layer_2.self_attention.in_proj_bias False\n",
      "encoder.layers.encoder_layer_2.self_attention.out_proj.weight False\n",
      "encoder.layers.encoder_layer_2.self_attention.out_proj.bias False\n",
      "encoder.layers.encoder_layer_2.ln_2.weight False\n",
      "encoder.layers.encoder_layer_2.ln_2.bias False\n",
      "encoder.layers.encoder_layer_2.mlp.0.weight True\n",
      "encoder.layers.encoder_layer_2.mlp.0.bias True\n",
      "encoder.layers.encoder_layer_2.mlp.3.weight True\n",
      "encoder.layers.encoder_layer_2.mlp.3.bias True\n",
      "encoder.layers.encoder_layer_3.ln_1.weight False\n",
      "encoder.layers.encoder_layer_3.ln_1.bias False\n",
      "encoder.layers.encoder_layer_3.self_attention.in_proj_weight False\n",
      "encoder.layers.encoder_layer_3.self_attention.in_proj_bias False\n",
      "encoder.layers.encoder_layer_3.self_attention.out_proj.weight False\n",
      "encoder.layers.encoder_layer_3.self_attention.out_proj.bias False\n",
      "encoder.layers.encoder_layer_3.ln_2.weight False\n",
      "encoder.layers.encoder_layer_3.ln_2.bias False\n",
      "encoder.layers.encoder_layer_3.mlp.0.weight True\n",
      "encoder.layers.encoder_layer_3.mlp.0.bias True\n",
      "encoder.layers.encoder_layer_3.mlp.3.weight True\n",
      "encoder.layers.encoder_layer_3.mlp.3.bias True\n",
      "encoder.layers.encoder_layer_4.ln_1.weight False\n",
      "encoder.layers.encoder_layer_4.ln_1.bias False\n",
      "encoder.layers.encoder_layer_4.self_attention.in_proj_weight False\n",
      "encoder.layers.encoder_layer_4.self_attention.in_proj_bias False\n",
      "encoder.layers.encoder_layer_4.self_attention.out_proj.weight False\n",
      "encoder.layers.encoder_layer_4.self_attention.out_proj.bias False\n",
      "encoder.layers.encoder_layer_4.ln_2.weight False\n",
      "encoder.layers.encoder_layer_4.ln_2.bias False\n",
      "encoder.layers.encoder_layer_4.mlp.0.weight True\n",
      "encoder.layers.encoder_layer_4.mlp.0.bias True\n",
      "encoder.layers.encoder_layer_4.mlp.3.weight True\n",
      "encoder.layers.encoder_layer_4.mlp.3.bias True\n",
      "encoder.layers.encoder_layer_5.ln_1.weight False\n",
      "encoder.layers.encoder_layer_5.ln_1.bias False\n",
      "encoder.layers.encoder_layer_5.self_attention.in_proj_weight False\n",
      "encoder.layers.encoder_layer_5.self_attention.in_proj_bias False\n",
      "encoder.layers.encoder_layer_5.self_attention.out_proj.weight False\n",
      "encoder.layers.encoder_layer_5.self_attention.out_proj.bias False\n",
      "encoder.layers.encoder_layer_5.ln_2.weight False\n",
      "encoder.layers.encoder_layer_5.ln_2.bias False\n",
      "encoder.layers.encoder_layer_5.mlp.0.weight True\n",
      "encoder.layers.encoder_layer_5.mlp.0.bias True\n",
      "encoder.layers.encoder_layer_5.mlp.3.weight True\n",
      "encoder.layers.encoder_layer_5.mlp.3.bias True\n",
      "encoder.layers.encoder_layer_6.ln_1.weight False\n",
      "encoder.layers.encoder_layer_6.ln_1.bias False\n",
      "encoder.layers.encoder_layer_6.self_attention.in_proj_weight False\n",
      "encoder.layers.encoder_layer_6.self_attention.in_proj_bias False\n",
      "encoder.layers.encoder_layer_6.self_attention.out_proj.weight False\n",
      "encoder.layers.encoder_layer_6.self_attention.out_proj.bias False\n",
      "encoder.layers.encoder_layer_6.ln_2.weight False\n",
      "encoder.layers.encoder_layer_6.ln_2.bias False\n",
      "encoder.layers.encoder_layer_6.mlp.0.weight True\n",
      "encoder.layers.encoder_layer_6.mlp.0.bias True\n",
      "encoder.layers.encoder_layer_6.mlp.3.weight True\n",
      "encoder.layers.encoder_layer_6.mlp.3.bias True\n",
      "encoder.layers.encoder_layer_7.ln_1.weight False\n",
      "encoder.layers.encoder_layer_7.ln_1.bias False\n",
      "encoder.layers.encoder_layer_7.self_attention.in_proj_weight False\n",
      "encoder.layers.encoder_layer_7.self_attention.in_proj_bias False\n",
      "encoder.layers.encoder_layer_7.self_attention.out_proj.weight False\n",
      "encoder.layers.encoder_layer_7.self_attention.out_proj.bias False\n",
      "encoder.layers.encoder_layer_7.ln_2.weight False\n",
      "encoder.layers.encoder_layer_7.ln_2.bias False\n",
      "encoder.layers.encoder_layer_7.mlp.0.weight True\n",
      "encoder.layers.encoder_layer_7.mlp.0.bias True\n",
      "encoder.layers.encoder_layer_7.mlp.3.weight True\n",
      "encoder.layers.encoder_layer_7.mlp.3.bias True\n",
      "encoder.layers.encoder_layer_8.ln_1.weight False\n",
      "encoder.layers.encoder_layer_8.ln_1.bias False\n",
      "encoder.layers.encoder_layer_8.self_attention.in_proj_weight False\n",
      "encoder.layers.encoder_layer_8.self_attention.in_proj_bias False\n",
      "encoder.layers.encoder_layer_8.self_attention.out_proj.weight False\n",
      "encoder.layers.encoder_layer_8.self_attention.out_proj.bias False\n",
      "encoder.layers.encoder_layer_8.ln_2.weight False\n",
      "encoder.layers.encoder_layer_8.ln_2.bias False\n",
      "encoder.layers.encoder_layer_8.mlp.0.weight True\n",
      "encoder.layers.encoder_layer_8.mlp.0.bias True\n",
      "encoder.layers.encoder_layer_8.mlp.3.weight True\n",
      "encoder.layers.encoder_layer_8.mlp.3.bias True\n",
      "encoder.layers.encoder_layer_9.ln_1.weight False\n",
      "encoder.layers.encoder_layer_9.ln_1.bias False\n",
      "encoder.layers.encoder_layer_9.self_attention.in_proj_weight False\n",
      "encoder.layers.encoder_layer_9.self_attention.in_proj_bias False\n",
      "encoder.layers.encoder_layer_9.self_attention.out_proj.weight False\n",
      "encoder.layers.encoder_layer_9.self_attention.out_proj.bias False\n",
      "encoder.layers.encoder_layer_9.ln_2.weight False\n",
      "encoder.layers.encoder_layer_9.ln_2.bias False\n",
      "encoder.layers.encoder_layer_9.mlp.0.weight True\n",
      "encoder.layers.encoder_layer_9.mlp.0.bias True\n",
      "encoder.layers.encoder_layer_9.mlp.3.weight True\n",
      "encoder.layers.encoder_layer_9.mlp.3.bias True\n",
      "encoder.layers.encoder_layer_10.ln_1.weight False\n",
      "encoder.layers.encoder_layer_10.ln_1.bias False\n",
      "encoder.layers.encoder_layer_10.self_attention.in_proj_weight False\n",
      "encoder.layers.encoder_layer_10.self_attention.in_proj_bias False\n",
      "encoder.layers.encoder_layer_10.self_attention.out_proj.weight False\n",
      "encoder.layers.encoder_layer_10.self_attention.out_proj.bias False\n",
      "encoder.layers.encoder_layer_10.ln_2.weight False\n",
      "encoder.layers.encoder_layer_10.ln_2.bias False\n",
      "encoder.layers.encoder_layer_10.mlp.0.weight True\n",
      "encoder.layers.encoder_layer_10.mlp.0.bias True\n",
      "encoder.layers.encoder_layer_10.mlp.3.weight True\n",
      "encoder.layers.encoder_layer_10.mlp.3.bias True\n",
      "encoder.layers.encoder_layer_11.ln_1.weight False\n",
      "encoder.layers.encoder_layer_11.ln_1.bias False\n",
      "encoder.layers.encoder_layer_11.self_attention.in_proj_weight False\n",
      "encoder.layers.encoder_layer_11.self_attention.in_proj_bias False\n",
      "encoder.layers.encoder_layer_11.self_attention.out_proj.weight False\n",
      "encoder.layers.encoder_layer_11.self_attention.out_proj.bias False\n",
      "encoder.layers.encoder_layer_11.ln_2.weight False\n",
      "encoder.layers.encoder_layer_11.ln_2.bias False\n",
      "encoder.layers.encoder_layer_11.mlp.0.weight True\n",
      "encoder.layers.encoder_layer_11.mlp.0.bias True\n",
      "encoder.layers.encoder_layer_11.mlp.3.weight True\n",
      "encoder.layers.encoder_layer_11.mlp.3.bias True\n",
      "encoder.ln.weight False\n",
      "encoder.ln.bias False\n",
      "heads.head.weight False\n",
      "heads.head.bias False\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-22T12:20:57.047318700Z",
     "start_time": "2024-08-22T12:20:56.997319500Z"
    }
   },
   "id": "72c19055aade3a8c",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters and their trainable status:\n",
      ": Trainable\n",
      "conv_proj: Frozen\n",
      "encoder: Trainable\n",
      "encoder.dropout: Frozen\n",
      "encoder.layers: Trainable\n",
      "encoder.layers.encoder_layer_0: Trainable\n",
      "encoder.layers.encoder_layer_0.ln_1: Frozen\n",
      "encoder.layers.encoder_layer_0.self_attention: Frozen\n",
      "encoder.layers.encoder_layer_0.self_attention.out_proj: Frozen\n",
      "encoder.layers.encoder_layer_0.dropout: Frozen\n",
      "encoder.layers.encoder_layer_0.ln_2: Frozen\n",
      "encoder.layers.encoder_layer_0.mlp: Trainable\n",
      "encoder.layers.encoder_layer_0.mlp.0: Trainable\n",
      "encoder.layers.encoder_layer_0.mlp.1: Frozen\n",
      "encoder.layers.encoder_layer_0.mlp.2: Frozen\n",
      "encoder.layers.encoder_layer_0.mlp.3: Trainable\n",
      "encoder.layers.encoder_layer_0.mlp.4: Frozen\n",
      "encoder.layers.encoder_layer_1: Trainable\n",
      "encoder.layers.encoder_layer_1.ln_1: Frozen\n",
      "encoder.layers.encoder_layer_1.self_attention: Frozen\n",
      "encoder.layers.encoder_layer_1.self_attention.out_proj: Frozen\n",
      "encoder.layers.encoder_layer_1.dropout: Frozen\n",
      "encoder.layers.encoder_layer_1.ln_2: Frozen\n",
      "encoder.layers.encoder_layer_1.mlp: Trainable\n",
      "encoder.layers.encoder_layer_1.mlp.0: Trainable\n",
      "encoder.layers.encoder_layer_1.mlp.1: Frozen\n",
      "encoder.layers.encoder_layer_1.mlp.2: Frozen\n",
      "encoder.layers.encoder_layer_1.mlp.3: Trainable\n",
      "encoder.layers.encoder_layer_1.mlp.4: Frozen\n",
      "encoder.layers.encoder_layer_2: Trainable\n",
      "encoder.layers.encoder_layer_2.ln_1: Frozen\n",
      "encoder.layers.encoder_layer_2.self_attention: Frozen\n",
      "encoder.layers.encoder_layer_2.self_attention.out_proj: Frozen\n",
      "encoder.layers.encoder_layer_2.dropout: Frozen\n",
      "encoder.layers.encoder_layer_2.ln_2: Frozen\n",
      "encoder.layers.encoder_layer_2.mlp: Trainable\n",
      "encoder.layers.encoder_layer_2.mlp.0: Trainable\n",
      "encoder.layers.encoder_layer_2.mlp.1: Frozen\n",
      "encoder.layers.encoder_layer_2.mlp.2: Frozen\n",
      "encoder.layers.encoder_layer_2.mlp.3: Trainable\n",
      "encoder.layers.encoder_layer_2.mlp.4: Frozen\n",
      "encoder.layers.encoder_layer_3: Trainable\n",
      "encoder.layers.encoder_layer_3.ln_1: Frozen\n",
      "encoder.layers.encoder_layer_3.self_attention: Frozen\n",
      "encoder.layers.encoder_layer_3.self_attention.out_proj: Frozen\n",
      "encoder.layers.encoder_layer_3.dropout: Frozen\n",
      "encoder.layers.encoder_layer_3.ln_2: Frozen\n",
      "encoder.layers.encoder_layer_3.mlp: Trainable\n",
      "encoder.layers.encoder_layer_3.mlp.0: Trainable\n",
      "encoder.layers.encoder_layer_3.mlp.1: Frozen\n",
      "encoder.layers.encoder_layer_3.mlp.2: Frozen\n",
      "encoder.layers.encoder_layer_3.mlp.3: Trainable\n",
      "encoder.layers.encoder_layer_3.mlp.4: Frozen\n",
      "encoder.layers.encoder_layer_4: Trainable\n",
      "encoder.layers.encoder_layer_4.ln_1: Frozen\n",
      "encoder.layers.encoder_layer_4.self_attention: Frozen\n",
      "encoder.layers.encoder_layer_4.self_attention.out_proj: Frozen\n",
      "encoder.layers.encoder_layer_4.dropout: Frozen\n",
      "encoder.layers.encoder_layer_4.ln_2: Frozen\n",
      "encoder.layers.encoder_layer_4.mlp: Trainable\n",
      "encoder.layers.encoder_layer_4.mlp.0: Trainable\n",
      "encoder.layers.encoder_layer_4.mlp.1: Frozen\n",
      "encoder.layers.encoder_layer_4.mlp.2: Frozen\n",
      "encoder.layers.encoder_layer_4.mlp.3: Trainable\n",
      "encoder.layers.encoder_layer_4.mlp.4: Frozen\n",
      "encoder.layers.encoder_layer_5: Trainable\n",
      "encoder.layers.encoder_layer_5.ln_1: Frozen\n",
      "encoder.layers.encoder_layer_5.self_attention: Frozen\n",
      "encoder.layers.encoder_layer_5.self_attention.out_proj: Frozen\n",
      "encoder.layers.encoder_layer_5.dropout: Frozen\n",
      "encoder.layers.encoder_layer_5.ln_2: Frozen\n",
      "encoder.layers.encoder_layer_5.mlp: Trainable\n",
      "encoder.layers.encoder_layer_5.mlp.0: Trainable\n",
      "encoder.layers.encoder_layer_5.mlp.1: Frozen\n",
      "encoder.layers.encoder_layer_5.mlp.2: Frozen\n",
      "encoder.layers.encoder_layer_5.mlp.3: Trainable\n",
      "encoder.layers.encoder_layer_5.mlp.4: Frozen\n",
      "encoder.layers.encoder_layer_6: Trainable\n",
      "encoder.layers.encoder_layer_6.ln_1: Frozen\n",
      "encoder.layers.encoder_layer_6.self_attention: Frozen\n",
      "encoder.layers.encoder_layer_6.self_attention.out_proj: Frozen\n",
      "encoder.layers.encoder_layer_6.dropout: Frozen\n",
      "encoder.layers.encoder_layer_6.ln_2: Frozen\n",
      "encoder.layers.encoder_layer_6.mlp: Trainable\n",
      "encoder.layers.encoder_layer_6.mlp.0: Trainable\n",
      "encoder.layers.encoder_layer_6.mlp.1: Frozen\n",
      "encoder.layers.encoder_layer_6.mlp.2: Frozen\n",
      "encoder.layers.encoder_layer_6.mlp.3: Trainable\n",
      "encoder.layers.encoder_layer_6.mlp.4: Frozen\n",
      "encoder.layers.encoder_layer_7: Trainable\n",
      "encoder.layers.encoder_layer_7.ln_1: Frozen\n",
      "encoder.layers.encoder_layer_7.self_attention: Frozen\n",
      "encoder.layers.encoder_layer_7.self_attention.out_proj: Frozen\n",
      "encoder.layers.encoder_layer_7.dropout: Frozen\n",
      "encoder.layers.encoder_layer_7.ln_2: Frozen\n",
      "encoder.layers.encoder_layer_7.mlp: Trainable\n",
      "encoder.layers.encoder_layer_7.mlp.0: Trainable\n",
      "encoder.layers.encoder_layer_7.mlp.1: Frozen\n",
      "encoder.layers.encoder_layer_7.mlp.2: Frozen\n",
      "encoder.layers.encoder_layer_7.mlp.3: Trainable\n",
      "encoder.layers.encoder_layer_7.mlp.4: Frozen\n",
      "encoder.layers.encoder_layer_8: Trainable\n",
      "encoder.layers.encoder_layer_8.ln_1: Frozen\n",
      "encoder.layers.encoder_layer_8.self_attention: Frozen\n",
      "encoder.layers.encoder_layer_8.self_attention.out_proj: Frozen\n",
      "encoder.layers.encoder_layer_8.dropout: Frozen\n",
      "encoder.layers.encoder_layer_8.ln_2: Frozen\n",
      "encoder.layers.encoder_layer_8.mlp: Trainable\n",
      "encoder.layers.encoder_layer_8.mlp.0: Trainable\n",
      "encoder.layers.encoder_layer_8.mlp.1: Frozen\n",
      "encoder.layers.encoder_layer_8.mlp.2: Frozen\n",
      "encoder.layers.encoder_layer_8.mlp.3: Trainable\n",
      "encoder.layers.encoder_layer_8.mlp.4: Frozen\n",
      "encoder.layers.encoder_layer_9: Trainable\n",
      "encoder.layers.encoder_layer_9.ln_1: Frozen\n",
      "encoder.layers.encoder_layer_9.self_attention: Frozen\n",
      "encoder.layers.encoder_layer_9.self_attention.out_proj: Frozen\n",
      "encoder.layers.encoder_layer_9.dropout: Frozen\n",
      "encoder.layers.encoder_layer_9.ln_2: Frozen\n",
      "encoder.layers.encoder_layer_9.mlp: Trainable\n",
      "encoder.layers.encoder_layer_9.mlp.0: Trainable\n",
      "encoder.layers.encoder_layer_9.mlp.1: Frozen\n",
      "encoder.layers.encoder_layer_9.mlp.2: Frozen\n",
      "encoder.layers.encoder_layer_9.mlp.3: Trainable\n",
      "encoder.layers.encoder_layer_9.mlp.4: Frozen\n",
      "encoder.layers.encoder_layer_10: Trainable\n",
      "encoder.layers.encoder_layer_10.ln_1: Frozen\n",
      "encoder.layers.encoder_layer_10.self_attention: Frozen\n",
      "encoder.layers.encoder_layer_10.self_attention.out_proj: Frozen\n",
      "encoder.layers.encoder_layer_10.dropout: Frozen\n",
      "encoder.layers.encoder_layer_10.ln_2: Frozen\n",
      "encoder.layers.encoder_layer_10.mlp: Trainable\n",
      "encoder.layers.encoder_layer_10.mlp.0: Trainable\n",
      "encoder.layers.encoder_layer_10.mlp.1: Frozen\n",
      "encoder.layers.encoder_layer_10.mlp.2: Frozen\n",
      "encoder.layers.encoder_layer_10.mlp.3: Trainable\n",
      "encoder.layers.encoder_layer_10.mlp.4: Frozen\n",
      "encoder.layers.encoder_layer_11: Trainable\n",
      "encoder.layers.encoder_layer_11.ln_1: Frozen\n",
      "encoder.layers.encoder_layer_11.self_attention: Frozen\n",
      "encoder.layers.encoder_layer_11.self_attention.out_proj: Frozen\n",
      "encoder.layers.encoder_layer_11.dropout: Frozen\n",
      "encoder.layers.encoder_layer_11.ln_2: Frozen\n",
      "encoder.layers.encoder_layer_11.mlp: Trainable\n",
      "encoder.layers.encoder_layer_11.mlp.0: Trainable\n",
      "encoder.layers.encoder_layer_11.mlp.1: Frozen\n",
      "encoder.layers.encoder_layer_11.mlp.2: Frozen\n",
      "encoder.layers.encoder_layer_11.mlp.3: Trainable\n",
      "encoder.layers.encoder_layer_11.mlp.4: Frozen\n",
      "encoder.ln: Trainable\n",
      "heads: Trainable\n",
      "heads.head: Trainable\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.heads.head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if \"mlp\" in name and isinstance(module, torch.nn.Linear):\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    " \n",
    "for name, module in model.named_modules():\n",
    "    if \"encoder.ln\" in name and isinstance(module, torch.nn.LayerNorm):\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "print(\"Model parameters and their trainable status:\")\n",
    "for name, module in model.named_modules():\n",
    "    is_trainable = any(param.requires_grad for param in module.parameters())\n",
    "    print(f\"{name}: {'Trainable' if is_trainable else 'Frozen'}\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-23T09:59:46.139924700Z",
     "start_time": "2024-08-23T09:59:45.987666900Z"
    }
   },
   "id": "c50e85dd6adc885e",
   "execution_count": 65
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
