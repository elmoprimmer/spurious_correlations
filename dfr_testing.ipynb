{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-27T08:01:47.560797900Z",
     "start_time": "2024-08-27T08:01:47.534810800Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vit_b_16\n",
    "import torch\n",
    "\n",
    "import argparse\n",
    "\n",
    "from wb_data import WaterBirdsDataset, get_loader, get_transform_cub, log_data\n",
    "from utils import evaluate, get_y_p\n",
    "\n",
    "from wb_data import WaterBirdsDataset, get_loader, get_transform_cub, log_data\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-27T08:01:49.277135Z",
     "start_time": "2024-08-27T08:01:49.258142700Z"
    }
   },
   "id": "9235fc13369662c2",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = vit_b_16(weights='DEFAULT')\n",
    "model.heads.head = nn.Linear(model.heads.head.in_features, NUM_CLASSES)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-27T08:01:52.070429900Z",
     "start_time": "2024-08-27T08:01:50.616231900Z"
    }
   },
   "id": "dc9a14f6594c9ad6",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "VisionTransformer(\n  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n  (encoder): Encoder(\n    (dropout): Dropout(p=0.0, inplace=False)\n    (layers): Sequential(\n      (encoder_layer_0): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_1): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_2): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_3): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_4): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_5): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_6): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_7): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_8): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_9): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_10): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (encoder_layer_11): EncoderBlock(\n        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (self_attention): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=3072, out_features=768, bias=True)\n          (4): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  )\n  (heads): Sequential(\n    (head): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-27T08:01:55.378312800Z",
     "start_time": "2024-08-27T08:01:55.353292500Z"
    }
   },
   "id": "e38aa178dabda7a3",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_token True\n",
      "conv_proj.weight True\n",
      "conv_proj.bias True\n",
      "encoder.pos_embedding True\n",
      "encoder.layers.encoder_layer_0.ln_1.weight True\n",
      "encoder.layers.encoder_layer_0.ln_1.bias True\n",
      "encoder.layers.encoder_layer_0.self_attention.in_proj_weight True\n",
      "encoder.layers.encoder_layer_0.self_attention.in_proj_bias True\n",
      "encoder.layers.encoder_layer_0.self_attention.out_proj.weight True\n",
      "encoder.layers.encoder_layer_0.self_attention.out_proj.bias True\n",
      "encoder.layers.encoder_layer_0.ln_2.weight True\n",
      "encoder.layers.encoder_layer_0.ln_2.bias True\n",
      "encoder.layers.encoder_layer_0.mlp.0.weight True\n",
      "encoder.layers.encoder_layer_0.mlp.0.bias True\n",
      "encoder.layers.encoder_layer_0.mlp.3.weight True\n",
      "encoder.layers.encoder_layer_0.mlp.3.bias True\n",
      "encoder.layers.encoder_layer_1.ln_1.weight True\n",
      "encoder.layers.encoder_layer_1.ln_1.bias True\n",
      "encoder.layers.encoder_layer_1.self_attention.in_proj_weight True\n",
      "encoder.layers.encoder_layer_1.self_attention.in_proj_bias True\n",
      "encoder.layers.encoder_layer_1.self_attention.out_proj.weight True\n",
      "encoder.layers.encoder_layer_1.self_attention.out_proj.bias True\n",
      "encoder.layers.encoder_layer_1.ln_2.weight True\n",
      "encoder.layers.encoder_layer_1.ln_2.bias True\n",
      "encoder.layers.encoder_layer_1.mlp.0.weight True\n",
      "encoder.layers.encoder_layer_1.mlp.0.bias True\n",
      "encoder.layers.encoder_layer_1.mlp.3.weight True\n",
      "encoder.layers.encoder_layer_1.mlp.3.bias True\n",
      "encoder.layers.encoder_layer_2.ln_1.weight True\n",
      "encoder.layers.encoder_layer_2.ln_1.bias True\n",
      "encoder.layers.encoder_layer_2.self_attention.in_proj_weight True\n",
      "encoder.layers.encoder_layer_2.self_attention.in_proj_bias True\n",
      "encoder.layers.encoder_layer_2.self_attention.out_proj.weight True\n",
      "encoder.layers.encoder_layer_2.self_attention.out_proj.bias True\n",
      "encoder.layers.encoder_layer_2.ln_2.weight True\n",
      "encoder.layers.encoder_layer_2.ln_2.bias True\n",
      "encoder.layers.encoder_layer_2.mlp.0.weight True\n",
      "encoder.layers.encoder_layer_2.mlp.0.bias True\n",
      "encoder.layers.encoder_layer_2.mlp.3.weight True\n",
      "encoder.layers.encoder_layer_2.mlp.3.bias True\n",
      "encoder.layers.encoder_layer_3.ln_1.weight True\n",
      "encoder.layers.encoder_layer_3.ln_1.bias True\n",
      "encoder.layers.encoder_layer_3.self_attention.in_proj_weight True\n",
      "encoder.layers.encoder_layer_3.self_attention.in_proj_bias True\n",
      "encoder.layers.encoder_layer_3.self_attention.out_proj.weight True\n",
      "encoder.layers.encoder_layer_3.self_attention.out_proj.bias True\n",
      "encoder.layers.encoder_layer_3.ln_2.weight True\n",
      "encoder.layers.encoder_layer_3.ln_2.bias True\n",
      "encoder.layers.encoder_layer_3.mlp.0.weight True\n",
      "encoder.layers.encoder_layer_3.mlp.0.bias True\n",
      "encoder.layers.encoder_layer_3.mlp.3.weight True\n",
      "encoder.layers.encoder_layer_3.mlp.3.bias True\n",
      "encoder.layers.encoder_layer_4.ln_1.weight True\n",
      "encoder.layers.encoder_layer_4.ln_1.bias True\n",
      "encoder.layers.encoder_layer_4.self_attention.in_proj_weight True\n",
      "encoder.layers.encoder_layer_4.self_attention.in_proj_bias True\n",
      "encoder.layers.encoder_layer_4.self_attention.out_proj.weight True\n",
      "encoder.layers.encoder_layer_4.self_attention.out_proj.bias True\n",
      "encoder.layers.encoder_layer_4.ln_2.weight True\n",
      "encoder.layers.encoder_layer_4.ln_2.bias True\n",
      "encoder.layers.encoder_layer_4.mlp.0.weight True\n",
      "encoder.layers.encoder_layer_4.mlp.0.bias True\n",
      "encoder.layers.encoder_layer_4.mlp.3.weight True\n",
      "encoder.layers.encoder_layer_4.mlp.3.bias True\n",
      "encoder.layers.encoder_layer_5.ln_1.weight True\n",
      "encoder.layers.encoder_layer_5.ln_1.bias True\n",
      "encoder.layers.encoder_layer_5.self_attention.in_proj_weight True\n",
      "encoder.layers.encoder_layer_5.self_attention.in_proj_bias True\n",
      "encoder.layers.encoder_layer_5.self_attention.out_proj.weight True\n",
      "encoder.layers.encoder_layer_5.self_attention.out_proj.bias True\n",
      "encoder.layers.encoder_layer_5.ln_2.weight True\n",
      "encoder.layers.encoder_layer_5.ln_2.bias True\n",
      "encoder.layers.encoder_layer_5.mlp.0.weight True\n",
      "encoder.layers.encoder_layer_5.mlp.0.bias True\n",
      "encoder.layers.encoder_layer_5.mlp.3.weight True\n",
      "encoder.layers.encoder_layer_5.mlp.3.bias True\n",
      "encoder.layers.encoder_layer_6.ln_1.weight True\n",
      "encoder.layers.encoder_layer_6.ln_1.bias True\n",
      "encoder.layers.encoder_layer_6.self_attention.in_proj_weight True\n",
      "encoder.layers.encoder_layer_6.self_attention.in_proj_bias True\n",
      "encoder.layers.encoder_layer_6.self_attention.out_proj.weight True\n",
      "encoder.layers.encoder_layer_6.self_attention.out_proj.bias True\n",
      "encoder.layers.encoder_layer_6.ln_2.weight True\n",
      "encoder.layers.encoder_layer_6.ln_2.bias True\n",
      "encoder.layers.encoder_layer_6.mlp.0.weight True\n",
      "encoder.layers.encoder_layer_6.mlp.0.bias True\n",
      "encoder.layers.encoder_layer_6.mlp.3.weight True\n",
      "encoder.layers.encoder_layer_6.mlp.3.bias True\n",
      "encoder.layers.encoder_layer_7.ln_1.weight True\n",
      "encoder.layers.encoder_layer_7.ln_1.bias True\n",
      "encoder.layers.encoder_layer_7.self_attention.in_proj_weight True\n",
      "encoder.layers.encoder_layer_7.self_attention.in_proj_bias True\n",
      "encoder.layers.encoder_layer_7.self_attention.out_proj.weight True\n",
      "encoder.layers.encoder_layer_7.self_attention.out_proj.bias True\n",
      "encoder.layers.encoder_layer_7.ln_2.weight True\n",
      "encoder.layers.encoder_layer_7.ln_2.bias True\n",
      "encoder.layers.encoder_layer_7.mlp.0.weight True\n",
      "encoder.layers.encoder_layer_7.mlp.0.bias True\n",
      "encoder.layers.encoder_layer_7.mlp.3.weight True\n",
      "encoder.layers.encoder_layer_7.mlp.3.bias True\n",
      "encoder.layers.encoder_layer_8.ln_1.weight True\n",
      "encoder.layers.encoder_layer_8.ln_1.bias True\n",
      "encoder.layers.encoder_layer_8.self_attention.in_proj_weight True\n",
      "encoder.layers.encoder_layer_8.self_attention.in_proj_bias True\n",
      "encoder.layers.encoder_layer_8.self_attention.out_proj.weight True\n",
      "encoder.layers.encoder_layer_8.self_attention.out_proj.bias True\n",
      "encoder.layers.encoder_layer_8.ln_2.weight True\n",
      "encoder.layers.encoder_layer_8.ln_2.bias True\n",
      "encoder.layers.encoder_layer_8.mlp.0.weight True\n",
      "encoder.layers.encoder_layer_8.mlp.0.bias True\n",
      "encoder.layers.encoder_layer_8.mlp.3.weight True\n",
      "encoder.layers.encoder_layer_8.mlp.3.bias True\n",
      "encoder.layers.encoder_layer_9.ln_1.weight True\n",
      "encoder.layers.encoder_layer_9.ln_1.bias True\n",
      "encoder.layers.encoder_layer_9.self_attention.in_proj_weight True\n",
      "encoder.layers.encoder_layer_9.self_attention.in_proj_bias True\n",
      "encoder.layers.encoder_layer_9.self_attention.out_proj.weight True\n",
      "encoder.layers.encoder_layer_9.self_attention.out_proj.bias True\n",
      "encoder.layers.encoder_layer_9.ln_2.weight True\n",
      "encoder.layers.encoder_layer_9.ln_2.bias True\n",
      "encoder.layers.encoder_layer_9.mlp.0.weight True\n",
      "encoder.layers.encoder_layer_9.mlp.0.bias True\n",
      "encoder.layers.encoder_layer_9.mlp.3.weight True\n",
      "encoder.layers.encoder_layer_9.mlp.3.bias True\n",
      "encoder.layers.encoder_layer_10.ln_1.weight True\n",
      "encoder.layers.encoder_layer_10.ln_1.bias True\n",
      "encoder.layers.encoder_layer_10.self_attention.in_proj_weight True\n",
      "encoder.layers.encoder_layer_10.self_attention.in_proj_bias True\n",
      "encoder.layers.encoder_layer_10.self_attention.out_proj.weight True\n",
      "encoder.layers.encoder_layer_10.self_attention.out_proj.bias True\n",
      "encoder.layers.encoder_layer_10.ln_2.weight True\n",
      "encoder.layers.encoder_layer_10.ln_2.bias True\n",
      "encoder.layers.encoder_layer_10.mlp.0.weight True\n",
      "encoder.layers.encoder_layer_10.mlp.0.bias True\n",
      "encoder.layers.encoder_layer_10.mlp.3.weight True\n",
      "encoder.layers.encoder_layer_10.mlp.3.bias True\n",
      "encoder.layers.encoder_layer_11.ln_1.weight True\n",
      "encoder.layers.encoder_layer_11.ln_1.bias True\n",
      "encoder.layers.encoder_layer_11.self_attention.in_proj_weight True\n",
      "encoder.layers.encoder_layer_11.self_attention.in_proj_bias True\n",
      "encoder.layers.encoder_layer_11.self_attention.out_proj.weight True\n",
      "encoder.layers.encoder_layer_11.self_attention.out_proj.bias True\n",
      "encoder.layers.encoder_layer_11.ln_2.weight True\n",
      "encoder.layers.encoder_layer_11.ln_2.bias True\n",
      "encoder.layers.encoder_layer_11.mlp.0.weight True\n",
      "encoder.layers.encoder_layer_11.mlp.0.bias True\n",
      "encoder.layers.encoder_layer_11.mlp.3.weight True\n",
      "encoder.layers.encoder_layer_11.mlp.3.bias True\n",
      "encoder.ln.weight True\n",
      "encoder.ln.bias True\n",
      "heads.head.weight True\n",
      "heads.head.bias True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-27T08:01:59.013389800Z",
     "start_time": "2024-08-27T08:01:58.977254200Z"
    }
   },
   "id": "72c19055aade3a8c",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.layers.encoder_layer_0.mlp.0.weight: True\n",
      "encoder.layers.encoder_layer_0.mlp.0.bias: True\n",
      "encoder.layers.encoder_layer_0.mlp.3.weight: True\n",
      "encoder.layers.encoder_layer_0.mlp.3.bias: True\n",
      "encoder.layers.encoder_layer_1.mlp.0.weight: True\n",
      "encoder.layers.encoder_layer_1.mlp.0.bias: True\n",
      "encoder.layers.encoder_layer_1.mlp.3.weight: True\n",
      "encoder.layers.encoder_layer_1.mlp.3.bias: True\n",
      "encoder.layers.encoder_layer_2.mlp.0.weight: True\n",
      "encoder.layers.encoder_layer_2.mlp.0.bias: True\n",
      "encoder.layers.encoder_layer_2.mlp.3.weight: True\n",
      "encoder.layers.encoder_layer_2.mlp.3.bias: True\n",
      "encoder.layers.encoder_layer_3.mlp.0.weight: True\n",
      "encoder.layers.encoder_layer_3.mlp.0.bias: True\n",
      "encoder.layers.encoder_layer_3.mlp.3.weight: True\n",
      "encoder.layers.encoder_layer_3.mlp.3.bias: True\n",
      "encoder.layers.encoder_layer_4.mlp.0.weight: True\n",
      "encoder.layers.encoder_layer_4.mlp.0.bias: True\n",
      "encoder.layers.encoder_layer_4.mlp.3.weight: True\n",
      "encoder.layers.encoder_layer_4.mlp.3.bias: True\n",
      "encoder.layers.encoder_layer_5.mlp.0.weight: True\n",
      "encoder.layers.encoder_layer_5.mlp.0.bias: True\n",
      "encoder.layers.encoder_layer_5.mlp.3.weight: True\n",
      "encoder.layers.encoder_layer_5.mlp.3.bias: True\n",
      "encoder.layers.encoder_layer_6.mlp.0.weight: True\n",
      "encoder.layers.encoder_layer_6.mlp.0.bias: True\n",
      "encoder.layers.encoder_layer_6.mlp.3.weight: True\n",
      "encoder.layers.encoder_layer_6.mlp.3.bias: True\n",
      "encoder.layers.encoder_layer_7.mlp.0.weight: True\n",
      "encoder.layers.encoder_layer_7.mlp.0.bias: True\n",
      "encoder.layers.encoder_layer_7.mlp.3.weight: True\n",
      "encoder.layers.encoder_layer_7.mlp.3.bias: True\n",
      "encoder.layers.encoder_layer_8.mlp.0.weight: True\n",
      "encoder.layers.encoder_layer_8.mlp.0.bias: True\n",
      "encoder.layers.encoder_layer_8.mlp.3.weight: True\n",
      "encoder.layers.encoder_layer_8.mlp.3.bias: True\n",
      "encoder.layers.encoder_layer_9.mlp.0.weight: True\n",
      "encoder.layers.encoder_layer_9.mlp.0.bias: True\n",
      "encoder.layers.encoder_layer_9.mlp.3.weight: True\n",
      "encoder.layers.encoder_layer_9.mlp.3.bias: True\n",
      "encoder.layers.encoder_layer_10.mlp.0.weight: True\n",
      "encoder.layers.encoder_layer_10.mlp.0.bias: True\n",
      "encoder.layers.encoder_layer_10.mlp.3.weight: True\n",
      "encoder.layers.encoder_layer_10.mlp.3.bias: True\n",
      "encoder.layers.encoder_layer_11.mlp.0.weight: True\n",
      "encoder.layers.encoder_layer_11.mlp.0.bias: True\n",
      "encoder.layers.encoder_layer_11.mlp.3.weight: True\n",
      "encoder.layers.encoder_layer_11.mlp.3.bias: True\n",
      "encoder.ln.weight: True\n",
      "encoder.ln.bias: True\n",
      "heads.head.weight: True\n",
      "heads.head.bias: True\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.heads.head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if \"mlp\" in name and isinstance(module, torch.nn.Linear):\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    " \n",
    "for name, module in model.named_modules():\n",
    "    if \"encoder.ln\" in name and isinstance(module, torch.nn.LayerNorm):\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        \n",
    "            print(f\"{name}: {param.requires_grad}\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-27T08:05:02.715219500Z",
     "start_time": "2024-08-27T08:05:02.703216600Z"
    }
   },
   "id": "c50e85dd6adc885e",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-27T08:04:04.919077600Z",
     "start_time": "2024-08-27T08:04:04.891080300Z"
    }
   },
   "id": "92ef9756d602692a",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-26T09:38:46.191110600Z",
     "start_time": "2024-08-26T09:38:46.119128Z"
    }
   },
   "id": "128601bbf4396ffb",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bc206b5d0ccdee91"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7dc97747bcea994e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
