{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8eabcaa-2968-4cb9-abdc-1235ce3b5ce0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.models import vit_b_16\n",
    "from external.wdpruning.vit_wdpruning import VisionTransformerWithWDPruning\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\"ide\"), '../..')))\n",
    "#sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\"deep_feature_reweighting\"), '../..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83ac12b5-a4c1-41a4-a301-c96b903003f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e26bb0f-380c-477d-86bf-67d627be18a9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using stride: 16, and patch number is num_y14 * num_x14\n",
      "using drop_out rate is : 0.0\n",
      "using attn_drop_out rate is : 0.0\n",
      "using drop_path rate is : 0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Sequential(\n",
       "    (head): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "ckpt_pruned = torch.load(\"/home/primmere/ide/external/wdpruning/logs/wb/r90e50val/checkpoint-best.pth\", map_location=device,weights_only=False)['model']\n",
    "ckpt_orig = torch.load(\"/home/primmere/ide/dfr/logs/vit_waterbirds.pth\", map_location=device,weights_only=False)\n",
    "\n",
    "model_pruned = VisionTransformerWithWDPruning(num_classes=2,\n",
    "                                       patch_size=16, embed_dim=768,\n",
    "                                       depth=12, num_heads=12, mlp_ratio=4,\n",
    "                                       head_pruning=True, fc_pruning=True)\n",
    "model_pruned._make_structural_pruning()\n",
    "model_pruned.to(device)\n",
    "\n",
    "model_orig = vit_b_16(weights=None)\n",
    "model_orig.heads.head = nn.Linear(model_orig.heads.head.in_features, 2)\n",
    "model_orig.load_state_dict(ckpt_orig)\n",
    "model_orig.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6dc3adcf-aca8-4b28-9493-483c2e44d60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.layers.encoder_layer_0.ln_1.weight, blocks.0.norm1.weight differs!\n",
      "encoder.layers.encoder_layer_0.ln_1.bias, blocks.0.norm1.bias differs!\n",
      "encoder.layers.encoder_layer_0.self_attention.in_proj_weight, blocks.0.attn.qkv.weight differs!\n",
      "encoder.layers.encoder_layer_0.self_attention.in_proj_bias, blocks.0.attn.qkv.bias differs!\n",
      "encoder.layers.encoder_layer_0.self_attention.out_proj.weight, blocks.0.attn.proj.weight differs!\n",
      "encoder.layers.encoder_layer_0.self_attention.out_proj.bias, blocks.0.attn.proj.bias differs!\n",
      "encoder.layers.encoder_layer_0.ln_2.weight, blocks.0.norm2.weight differs!\n",
      "encoder.layers.encoder_layer_0.ln_2.bias, blocks.0.norm2.bias differs!\n",
      "encoder.layers.encoder_layer_0.mlp.0.weight, blocks.0.mlp.fc1.weight differs!\n",
      "encoder.layers.encoder_layer_0.mlp.0.bias, blocks.0.mlp.fc1.bias differs!\n",
      "encoder.layers.encoder_layer_0.mlp.3.weight, blocks.0.mlp.fc2.weight differs!\n",
      "encoder.layers.encoder_layer_0.mlp.3.bias, blocks.0.mlp.fc2.bias differs!\n",
      "encoder.layers.encoder_layer_1.ln_1.weight, blocks.1.norm1.weight differs!\n",
      "encoder.layers.encoder_layer_1.ln_1.bias, blocks.1.norm1.bias differs!\n",
      "encoder.layers.encoder_layer_1.self_attention.in_proj_weight, blocks.1.attn.qkv.weight differs!\n",
      "encoder.layers.encoder_layer_1.self_attention.in_proj_bias, blocks.1.attn.qkv.bias differs!\n",
      "encoder.layers.encoder_layer_1.self_attention.out_proj.weight, blocks.1.attn.proj.weight differs!\n",
      "encoder.layers.encoder_layer_1.self_attention.out_proj.bias, blocks.1.attn.proj.bias differs!\n",
      "encoder.layers.encoder_layer_1.ln_2.weight, blocks.1.norm2.weight differs!\n",
      "encoder.layers.encoder_layer_1.ln_2.bias, blocks.1.norm2.bias differs!\n",
      "encoder.layers.encoder_layer_1.mlp.0.weight, blocks.1.mlp.fc1.weight differs!\n",
      "encoder.layers.encoder_layer_1.mlp.0.bias, blocks.1.mlp.fc1.bias differs!\n",
      "encoder.layers.encoder_layer_1.mlp.3.weight, blocks.1.mlp.fc2.weight differs!\n",
      "encoder.layers.encoder_layer_1.mlp.3.bias, blocks.1.mlp.fc2.bias differs!\n",
      "encoder.layers.encoder_layer_2.ln_1.weight, blocks.2.norm1.weight differs!\n",
      "encoder.layers.encoder_layer_2.ln_1.bias, blocks.2.norm1.bias differs!\n",
      "encoder.layers.encoder_layer_2.self_attention.in_proj_weight, blocks.2.attn.qkv.weight differs!\n",
      "encoder.layers.encoder_layer_2.self_attention.in_proj_bias, blocks.2.attn.qkv.bias differs!\n",
      "encoder.layers.encoder_layer_2.self_attention.out_proj.weight, blocks.2.attn.proj.weight differs!\n",
      "encoder.layers.encoder_layer_2.self_attention.out_proj.bias, blocks.2.attn.proj.bias differs!\n",
      "encoder.layers.encoder_layer_2.ln_2.weight, blocks.2.norm2.weight differs!\n",
      "encoder.layers.encoder_layer_2.ln_2.bias, blocks.2.norm2.bias differs!\n",
      "encoder.layers.encoder_layer_2.mlp.0.weight, blocks.2.mlp.fc1.weight differs!\n",
      "encoder.layers.encoder_layer_2.mlp.0.bias, blocks.2.mlp.fc1.bias differs!\n",
      "encoder.layers.encoder_layer_2.mlp.3.weight, blocks.2.mlp.fc2.weight differs!\n",
      "encoder.layers.encoder_layer_2.mlp.3.bias, blocks.2.mlp.fc2.bias differs!\n",
      "encoder.layers.encoder_layer_3.ln_1.weight, blocks.3.norm1.weight differs!\n",
      "encoder.layers.encoder_layer_3.ln_1.bias, blocks.3.norm1.bias differs!\n",
      "encoder.layers.encoder_layer_3.self_attention.in_proj_weight, blocks.3.attn.qkv.weight differs!\n",
      "encoder.layers.encoder_layer_3.self_attention.in_proj_bias, blocks.3.attn.qkv.bias differs!\n",
      "encoder.layers.encoder_layer_3.self_attention.out_proj.weight, blocks.3.attn.proj.weight differs!\n",
      "encoder.layers.encoder_layer_3.self_attention.out_proj.bias, blocks.3.attn.proj.bias differs!\n",
      "encoder.layers.encoder_layer_3.ln_2.weight, blocks.3.norm2.weight differs!\n",
      "encoder.layers.encoder_layer_3.ln_2.bias, blocks.3.norm2.bias differs!\n",
      "encoder.layers.encoder_layer_3.mlp.0.weight, blocks.3.mlp.fc1.weight differs!\n",
      "encoder.layers.encoder_layer_3.mlp.0.bias, blocks.3.mlp.fc1.bias differs!\n",
      "encoder.layers.encoder_layer_3.mlp.3.weight, blocks.3.mlp.fc2.weight differs!\n",
      "encoder.layers.encoder_layer_3.mlp.3.bias, blocks.3.mlp.fc2.bias differs!\n",
      "encoder.layers.encoder_layer_4.ln_1.weight, blocks.4.norm1.weight differs!\n",
      "encoder.layers.encoder_layer_4.ln_1.bias, blocks.4.norm1.bias differs!\n",
      "encoder.layers.encoder_layer_4.self_attention.in_proj_weight, blocks.4.attn.qkv.weight differs!\n",
      "encoder.layers.encoder_layer_4.self_attention.in_proj_bias, blocks.4.attn.qkv.bias differs!\n",
      "encoder.layers.encoder_layer_4.self_attention.out_proj.weight, blocks.4.attn.proj.weight differs!\n",
      "encoder.layers.encoder_layer_4.self_attention.out_proj.bias, blocks.4.attn.proj.bias differs!\n",
      "encoder.layers.encoder_layer_4.ln_2.weight, blocks.4.norm2.weight differs!\n",
      "encoder.layers.encoder_layer_4.ln_2.bias, blocks.4.norm2.bias differs!\n",
      "encoder.layers.encoder_layer_4.mlp.0.weight, blocks.4.mlp.fc1.weight differs!\n",
      "encoder.layers.encoder_layer_4.mlp.0.bias, blocks.4.mlp.fc1.bias differs!\n",
      "encoder.layers.encoder_layer_4.mlp.3.weight, blocks.4.mlp.fc2.weight differs!\n",
      "encoder.layers.encoder_layer_4.mlp.3.bias, blocks.4.mlp.fc2.bias differs!\n",
      "encoder.layers.encoder_layer_5.ln_1.weight, blocks.5.norm1.weight differs!\n",
      "encoder.layers.encoder_layer_5.ln_1.bias, blocks.5.norm1.bias differs!\n",
      "encoder.layers.encoder_layer_5.self_attention.in_proj_weight, blocks.5.attn.qkv.weight differs!\n",
      "encoder.layers.encoder_layer_5.self_attention.in_proj_bias, blocks.5.attn.qkv.bias differs!\n",
      "encoder.layers.encoder_layer_5.self_attention.out_proj.weight, blocks.5.attn.proj.weight differs!\n",
      "encoder.layers.encoder_layer_5.self_attention.out_proj.bias, blocks.5.attn.proj.bias differs!\n",
      "encoder.layers.encoder_layer_5.ln_2.weight, blocks.5.norm2.weight differs!\n",
      "encoder.layers.encoder_layer_5.ln_2.bias, blocks.5.norm2.bias differs!\n",
      "encoder.layers.encoder_layer_5.mlp.0.weight, blocks.5.mlp.fc1.weight differs!\n",
      "encoder.layers.encoder_layer_5.mlp.0.bias, blocks.5.mlp.fc1.bias differs!\n",
      "encoder.layers.encoder_layer_5.mlp.3.weight, blocks.5.mlp.fc2.weight differs!\n",
      "encoder.layers.encoder_layer_5.mlp.3.bias, blocks.5.mlp.fc2.bias differs!\n",
      "encoder.layers.encoder_layer_6.ln_1.weight, blocks.6.norm1.weight differs!\n",
      "encoder.layers.encoder_layer_6.ln_1.bias, blocks.6.norm1.bias differs!\n",
      "encoder.layers.encoder_layer_6.self_attention.in_proj_weight, blocks.6.attn.qkv.weight differs!\n",
      "encoder.layers.encoder_layer_6.self_attention.in_proj_bias, blocks.6.attn.qkv.bias differs!\n",
      "encoder.layers.encoder_layer_6.self_attention.out_proj.weight, blocks.6.attn.proj.weight differs!\n",
      "encoder.layers.encoder_layer_6.self_attention.out_proj.bias, blocks.6.attn.proj.bias differs!\n",
      "encoder.layers.encoder_layer_6.ln_2.weight, blocks.6.norm2.weight differs!\n",
      "encoder.layers.encoder_layer_6.ln_2.bias, blocks.6.norm2.bias differs!\n",
      "encoder.layers.encoder_layer_6.mlp.0.weight, blocks.6.mlp.fc1.weight differs!\n",
      "encoder.layers.encoder_layer_6.mlp.0.bias, blocks.6.mlp.fc1.bias differs!\n",
      "encoder.layers.encoder_layer_6.mlp.3.weight, blocks.6.mlp.fc2.weight differs!\n",
      "encoder.layers.encoder_layer_6.mlp.3.bias, blocks.6.mlp.fc2.bias differs!\n",
      "encoder.layers.encoder_layer_7.ln_1.weight, blocks.7.norm1.weight differs!\n",
      "encoder.layers.encoder_layer_7.ln_1.bias, blocks.7.norm1.bias differs!\n",
      "encoder.layers.encoder_layer_7.self_attention.in_proj_weight, blocks.7.attn.qkv.weight differs!\n",
      "encoder.layers.encoder_layer_7.self_attention.in_proj_bias, blocks.7.attn.qkv.bias differs!\n",
      "encoder.layers.encoder_layer_7.self_attention.out_proj.weight, blocks.7.attn.proj.weight differs!\n",
      "encoder.layers.encoder_layer_7.self_attention.out_proj.bias, blocks.7.attn.proj.bias differs!\n",
      "encoder.layers.encoder_layer_7.ln_2.weight, blocks.7.norm2.weight differs!\n",
      "encoder.layers.encoder_layer_7.ln_2.bias, blocks.7.norm2.bias differs!\n",
      "encoder.layers.encoder_layer_7.mlp.0.weight, blocks.7.mlp.fc1.weight differs!\n",
      "encoder.layers.encoder_layer_7.mlp.0.bias, blocks.7.mlp.fc1.bias differs!\n",
      "encoder.layers.encoder_layer_7.mlp.3.weight, blocks.7.mlp.fc2.weight differs!\n",
      "encoder.layers.encoder_layer_7.mlp.3.bias, blocks.7.mlp.fc2.bias differs!\n",
      "encoder.layers.encoder_layer_8.ln_1.weight, blocks.8.norm1.weight differs!\n",
      "encoder.layers.encoder_layer_8.ln_1.bias, blocks.8.norm1.bias differs!\n",
      "encoder.layers.encoder_layer_8.self_attention.in_proj_weight, blocks.8.attn.qkv.weight differs!\n",
      "encoder.layers.encoder_layer_8.self_attention.in_proj_bias, blocks.8.attn.qkv.bias differs!\n",
      "encoder.layers.encoder_layer_8.self_attention.out_proj.weight, blocks.8.attn.proj.weight differs!\n",
      "encoder.layers.encoder_layer_8.self_attention.out_proj.bias, blocks.8.attn.proj.bias differs!\n",
      "encoder.layers.encoder_layer_8.ln_2.weight, blocks.8.norm2.weight differs!\n",
      "encoder.layers.encoder_layer_8.ln_2.bias, blocks.8.norm2.bias differs!\n",
      "encoder.layers.encoder_layer_8.mlp.0.weight, blocks.8.mlp.fc1.weight differs!\n",
      "encoder.layers.encoder_layer_8.mlp.0.bias, blocks.8.mlp.fc1.bias differs!\n",
      "encoder.layers.encoder_layer_8.mlp.3.weight, blocks.8.mlp.fc2.weight differs!\n",
      "encoder.layers.encoder_layer_8.mlp.3.bias, blocks.8.mlp.fc2.bias differs!\n",
      "encoder.layers.encoder_layer_9.ln_1.weight, blocks.9.norm1.weight differs!\n",
      "encoder.layers.encoder_layer_9.ln_1.bias, blocks.9.norm1.bias differs!\n",
      "encoder.layers.encoder_layer_9.self_attention.in_proj_weight, blocks.9.attn.qkv.weight differs!\n",
      "encoder.layers.encoder_layer_9.self_attention.in_proj_bias, blocks.9.attn.qkv.bias differs!\n",
      "encoder.layers.encoder_layer_9.self_attention.out_proj.weight, blocks.9.attn.proj.weight differs!\n",
      "encoder.layers.encoder_layer_9.self_attention.out_proj.bias, blocks.9.attn.proj.bias differs!\n",
      "encoder.layers.encoder_layer_9.ln_2.weight, blocks.9.norm2.weight differs!\n",
      "encoder.layers.encoder_layer_9.ln_2.bias, blocks.9.norm2.bias differs!\n",
      "encoder.layers.encoder_layer_9.mlp.0.weight, blocks.9.mlp.fc1.weight differs!\n",
      "encoder.layers.encoder_layer_9.mlp.0.bias, blocks.9.mlp.fc1.bias differs!\n",
      "encoder.layers.encoder_layer_9.mlp.3.weight, blocks.9.mlp.fc2.weight differs!\n",
      "encoder.layers.encoder_layer_9.mlp.3.bias, blocks.9.mlp.fc2.bias differs!\n",
      "encoder.layers.encoder_layer_10.ln_1.weight, blocks.10.norm1.weight differs!\n",
      "encoder.layers.encoder_layer_10.ln_1.bias, blocks.10.norm1.bias differs!\n",
      "encoder.layers.encoder_layer_10.self_attention.in_proj_weight, blocks.10.attn.qkv.weight differs!\n",
      "encoder.layers.encoder_layer_10.self_attention.in_proj_bias, blocks.10.attn.qkv.bias differs!\n",
      "encoder.layers.encoder_layer_10.self_attention.out_proj.weight, blocks.10.attn.proj.weight differs!\n",
      "encoder.layers.encoder_layer_10.self_attention.out_proj.bias, blocks.10.attn.proj.bias differs!\n",
      "encoder.layers.encoder_layer_10.ln_2.weight, blocks.10.norm2.weight differs!\n",
      "encoder.layers.encoder_layer_10.ln_2.bias, blocks.10.norm2.bias differs!\n",
      "encoder.layers.encoder_layer_10.mlp.0.weight, blocks.10.mlp.fc1.weight differs!\n",
      "encoder.layers.encoder_layer_10.mlp.0.bias, blocks.10.mlp.fc1.bias differs!\n",
      "encoder.layers.encoder_layer_10.mlp.3.weight, blocks.10.mlp.fc2.weight differs!\n",
      "encoder.layers.encoder_layer_10.mlp.3.bias, blocks.10.mlp.fc2.bias differs!\n",
      "encoder.layers.encoder_layer_11.ln_1.weight, blocks.11.norm1.weight differs!\n",
      "encoder.layers.encoder_layer_11.ln_1.bias, blocks.11.norm1.bias differs!\n",
      "encoder.layers.encoder_layer_11.self_attention.in_proj_weight, blocks.11.attn.qkv.weight differs!\n",
      "encoder.layers.encoder_layer_11.self_attention.in_proj_bias, blocks.11.attn.qkv.bias differs!\n",
      "encoder.layers.encoder_layer_11.self_attention.out_proj.weight, blocks.11.attn.proj.weight differs!\n",
      "encoder.layers.encoder_layer_11.self_attention.out_proj.bias, blocks.11.attn.proj.bias differs!\n",
      "encoder.layers.encoder_layer_11.ln_2.weight, blocks.11.norm2.weight differs!\n",
      "encoder.layers.encoder_layer_11.ln_2.bias, blocks.11.norm2.bias differs!\n",
      "encoder.layers.encoder_layer_11.mlp.0.weight, blocks.11.mlp.fc1.weight differs!\n",
      "encoder.layers.encoder_layer_11.mlp.0.bias, blocks.11.mlp.fc1.bias differs!\n",
      "encoder.layers.encoder_layer_11.mlp.3.weight, blocks.11.mlp.fc2.weight differs!\n",
      "encoder.layers.encoder_layer_11.mlp.3.bias, blocks.11.mlp.fc2.bias differs!\n"
     ]
    }
   ],
   "source": [
    "for (name_a, p_a), (name_b, p_b) in zip(model_orig.named_parameters(),\n",
    "                                        model_pruned.named_parameters()):\n",
    "    # --- NEW filter ---------------------------------------------------\n",
    "    if \"blocks\" not in name_b:            # only look at blocks tensors\n",
    "        continue\n",
    "    if \"saliency\" in name_b or \"threshold\" in name_b:\n",
    "        raise ValueError(  f\"shouldnt exist: {name_b}\" )                       # skip the unwanted ones\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    if not torch.allclose(p_a, p_b, rtol=1e-5, atol=1e-8):\n",
    "        print(f\"{name_a}, {name_b} differs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67294069-bf47-4627-81df-7139b58678b1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxx\n",
      "encoder.layers.encoder_layer_0.ln_1.weight\n",
      "blocks.0.norm1.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_0.ln_1.bias\n",
      "blocks.0.norm1.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_0.self_attention.in_proj_weight\n",
      "blocks.0.attn.qkv.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_0.self_attention.in_proj_bias\n",
      "blocks.0.attn.qkv.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_0.self_attention.out_proj.weight\n",
      "blocks.0.attn.proj.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_0.self_attention.out_proj.bias\n",
      "blocks.0.attn.proj.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_0.ln_2.weight\n",
      "blocks.0.norm2.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_0.ln_2.bias\n",
      "blocks.0.norm2.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_0.mlp.0.weight\n",
      "blocks.0.mlp.fc1.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_0.mlp.0.bias\n",
      "blocks.0.mlp.fc1.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_0.mlp.3.weight\n",
      "blocks.0.mlp.fc2.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_0.mlp.3.bias\n",
      "blocks.0.mlp.fc2.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_1.ln_1.weight\n",
      "blocks.1.norm1.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_1.ln_1.bias\n",
      "blocks.1.norm1.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_1.self_attention.in_proj_weight\n",
      "blocks.1.attn.qkv.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_1.self_attention.in_proj_bias\n",
      "blocks.1.attn.qkv.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_1.self_attention.out_proj.weight\n",
      "blocks.1.attn.proj.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_1.self_attention.out_proj.bias\n",
      "blocks.1.attn.proj.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_1.ln_2.weight\n",
      "blocks.1.norm2.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_1.ln_2.bias\n",
      "blocks.1.norm2.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_1.mlp.0.weight\n",
      "blocks.1.mlp.fc1.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_1.mlp.0.bias\n",
      "blocks.1.mlp.fc1.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_1.mlp.3.weight\n",
      "blocks.1.mlp.fc2.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_1.mlp.3.bias\n",
      "blocks.1.mlp.fc2.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_2.ln_1.weight\n",
      "blocks.2.norm1.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_2.ln_1.bias\n",
      "blocks.2.norm1.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_2.self_attention.in_proj_weight\n",
      "blocks.2.attn.qkv.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_2.self_attention.in_proj_bias\n",
      "blocks.2.attn.qkv.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_2.self_attention.out_proj.weight\n",
      "blocks.2.attn.proj.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_2.self_attention.out_proj.bias\n",
      "blocks.2.attn.proj.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_2.ln_2.weight\n",
      "blocks.2.norm2.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_2.ln_2.bias\n",
      "blocks.2.norm2.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_2.mlp.0.weight\n",
      "blocks.2.mlp.fc1.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_2.mlp.0.bias\n",
      "blocks.2.mlp.fc1.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_2.mlp.3.weight\n",
      "blocks.2.mlp.fc2.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_2.mlp.3.bias\n",
      "blocks.2.mlp.fc2.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_3.ln_1.weight\n",
      "blocks.3.norm1.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_3.ln_1.bias\n",
      "blocks.3.norm1.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_3.self_attention.in_proj_weight\n",
      "blocks.3.attn.qkv.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_3.self_attention.in_proj_bias\n",
      "blocks.3.attn.qkv.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_3.self_attention.out_proj.weight\n",
      "blocks.3.attn.proj.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_3.self_attention.out_proj.bias\n",
      "blocks.3.attn.proj.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_3.ln_2.weight\n",
      "blocks.3.norm2.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_3.ln_2.bias\n",
      "blocks.3.norm2.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_3.mlp.0.weight\n",
      "blocks.3.mlp.fc1.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_3.mlp.0.bias\n",
      "blocks.3.mlp.fc1.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_3.mlp.3.weight\n",
      "blocks.3.mlp.fc2.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_3.mlp.3.bias\n",
      "blocks.3.mlp.fc2.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_4.ln_1.weight\n",
      "blocks.4.norm1.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_4.ln_1.bias\n",
      "blocks.4.norm1.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_4.self_attention.in_proj_weight\n",
      "blocks.4.attn.qkv.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_4.self_attention.in_proj_bias\n",
      "blocks.4.attn.qkv.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_4.self_attention.out_proj.weight\n",
      "blocks.4.attn.proj.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_4.self_attention.out_proj.bias\n",
      "blocks.4.attn.proj.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_4.ln_2.weight\n",
      "blocks.4.norm2.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_4.ln_2.bias\n",
      "blocks.4.norm2.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_4.mlp.0.weight\n",
      "blocks.4.mlp.fc1.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_4.mlp.0.bias\n",
      "blocks.4.mlp.fc1.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_4.mlp.3.weight\n",
      "blocks.4.mlp.fc2.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_4.mlp.3.bias\n",
      "blocks.4.mlp.fc2.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_5.ln_1.weight\n",
      "blocks.5.norm1.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_5.ln_1.bias\n",
      "blocks.5.norm1.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_5.self_attention.in_proj_weight\n",
      "blocks.5.attn.qkv.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_5.self_attention.in_proj_bias\n",
      "blocks.5.attn.qkv.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_5.self_attention.out_proj.weight\n",
      "blocks.5.attn.proj.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_5.self_attention.out_proj.bias\n",
      "blocks.5.attn.proj.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_5.ln_2.weight\n",
      "blocks.5.norm2.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_5.ln_2.bias\n",
      "blocks.5.norm2.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_5.mlp.0.weight\n",
      "blocks.5.mlp.fc1.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_5.mlp.0.bias\n",
      "blocks.5.mlp.fc1.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_5.mlp.3.weight\n",
      "blocks.5.mlp.fc2.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_5.mlp.3.bias\n",
      "blocks.5.mlp.fc2.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_6.ln_1.weight\n",
      "blocks.6.norm1.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_6.ln_1.bias\n",
      "blocks.6.norm1.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_6.self_attention.in_proj_weight\n",
      "blocks.6.attn.qkv.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_6.self_attention.in_proj_bias\n",
      "blocks.6.attn.qkv.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_6.self_attention.out_proj.weight\n",
      "blocks.6.attn.proj.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_6.self_attention.out_proj.bias\n",
      "blocks.6.attn.proj.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_6.ln_2.weight\n",
      "blocks.6.norm2.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_6.ln_2.bias\n",
      "blocks.6.norm2.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_6.mlp.0.weight\n",
      "blocks.6.mlp.fc1.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_6.mlp.0.bias\n",
      "blocks.6.mlp.fc1.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_6.mlp.3.weight\n",
      "blocks.6.mlp.fc2.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_6.mlp.3.bias\n",
      "blocks.6.mlp.fc2.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_7.ln_1.weight\n",
      "blocks.7.norm1.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_7.ln_1.bias\n",
      "blocks.7.norm1.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_7.self_attention.in_proj_weight\n",
      "blocks.7.attn.qkv.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_7.self_attention.in_proj_bias\n",
      "blocks.7.attn.qkv.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_7.self_attention.out_proj.weight\n",
      "blocks.7.attn.proj.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_7.self_attention.out_proj.bias\n",
      "blocks.7.attn.proj.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_7.ln_2.weight\n",
      "blocks.7.norm2.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_7.ln_2.bias\n",
      "blocks.7.norm2.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_7.mlp.0.weight\n",
      "blocks.7.mlp.fc1.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_7.mlp.0.bias\n",
      "blocks.7.mlp.fc1.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_7.mlp.3.weight\n",
      "blocks.7.mlp.fc2.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_7.mlp.3.bias\n",
      "blocks.7.mlp.fc2.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_8.ln_1.weight\n",
      "blocks.8.norm1.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_8.ln_1.bias\n",
      "blocks.8.norm1.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_8.self_attention.in_proj_weight\n",
      "blocks.8.attn.qkv.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_8.self_attention.in_proj_bias\n",
      "blocks.8.attn.qkv.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_8.self_attention.out_proj.weight\n",
      "blocks.8.attn.proj.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_8.self_attention.out_proj.bias\n",
      "blocks.8.attn.proj.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_8.ln_2.weight\n",
      "blocks.8.norm2.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_8.ln_2.bias\n",
      "blocks.8.norm2.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_8.mlp.0.weight\n",
      "blocks.8.mlp.fc1.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_8.mlp.0.bias\n",
      "blocks.8.mlp.fc1.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_8.mlp.3.weight\n",
      "blocks.8.mlp.fc2.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_8.mlp.3.bias\n",
      "blocks.8.mlp.fc2.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_9.ln_1.weight\n",
      "blocks.9.norm1.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_9.ln_1.bias\n",
      "blocks.9.norm1.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_9.self_attention.in_proj_weight\n",
      "blocks.9.attn.qkv.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_9.self_attention.in_proj_bias\n",
      "blocks.9.attn.qkv.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_9.self_attention.out_proj.weight\n",
      "blocks.9.attn.proj.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_9.self_attention.out_proj.bias\n",
      "blocks.9.attn.proj.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_9.ln_2.weight\n",
      "blocks.9.norm2.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_9.ln_2.bias\n",
      "blocks.9.norm2.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_9.mlp.0.weight\n",
      "blocks.9.mlp.fc1.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_9.mlp.0.bias\n",
      "blocks.9.mlp.fc1.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_9.mlp.3.weight\n",
      "blocks.9.mlp.fc2.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_9.mlp.3.bias\n",
      "blocks.9.mlp.fc2.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_10.ln_1.weight\n",
      "blocks.10.norm1.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_10.ln_1.bias\n",
      "blocks.10.norm1.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_10.self_attention.in_proj_weight\n",
      "blocks.10.attn.qkv.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_10.self_attention.in_proj_bias\n",
      "blocks.10.attn.qkv.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_10.self_attention.out_proj.weight\n",
      "blocks.10.attn.proj.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_10.self_attention.out_proj.bias\n",
      "blocks.10.attn.proj.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_10.ln_2.weight\n",
      "blocks.10.norm2.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_10.ln_2.bias\n",
      "blocks.10.norm2.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_10.mlp.0.weight\n",
      "blocks.10.mlp.fc1.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_10.mlp.0.bias\n",
      "blocks.10.mlp.fc1.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_10.mlp.3.weight\n",
      "blocks.10.mlp.fc2.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_10.mlp.3.bias\n",
      "blocks.10.mlp.fc2.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_11.ln_1.weight\n",
      "blocks.11.norm1.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_11.ln_1.bias\n",
      "blocks.11.norm1.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_11.self_attention.in_proj_weight\n",
      "blocks.11.attn.qkv.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_11.self_attention.in_proj_bias\n",
      "blocks.11.attn.qkv.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_11.self_attention.out_proj.weight\n",
      "blocks.11.attn.proj.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_11.self_attention.out_proj.bias\n",
      "blocks.11.attn.proj.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_11.ln_2.weight\n",
      "blocks.11.norm2.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_11.ln_2.bias\n",
      "blocks.11.norm2.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_11.mlp.0.weight\n",
      "blocks.11.mlp.fc1.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_11.mlp.0.bias\n",
      "blocks.11.mlp.fc1.bias\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_11.mlp.3.weight\n",
      "blocks.11.mlp.fc2.weight\n",
      "xxx\n",
      "xxx\n",
      "encoder.layers.encoder_layer_11.mlp.3.bias\n",
      "blocks.11.mlp.fc2.bias\n",
      "xxx\n"
     ]
    }
   ],
   "source": [
    "for (name_a, p_a), (name_b, p_b) in zip(model_orig.named_parameters(),\n",
    "                                        model_pruned.named_parameters()):\n",
    "    if \"blocks\" not in name_b:         \n",
    "        continue\n",
    "    if \"saliency\" in name_b or \"threshold\" in name_b:\n",
    "        continue \n",
    "    print(\"xxx\")\n",
    "    print(name_a)\n",
    "    print(name_b)\n",
    "    print(\"xxx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfr2",
   "language": "python",
   "name": "dfr2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
