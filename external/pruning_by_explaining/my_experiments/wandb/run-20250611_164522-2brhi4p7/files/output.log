load the dataset
waterbirds
get train set
[0 1 2 3]
Number of unique labels: 2, Number of unique places: 2, Total groups: 4
group 0: 3518
group 1: 185
group 2: 55
group 3: 1037
get valid set
[0 1 2 3]
Number of unique labels: 2, Number of unique places: 2, Total groups: 4
group 0: 456
group 1: 456
group 2: 143
group 3: 144
no / wrong dataset
0
1
2
3
pruning indices  160
0
1
2
3
pruning indices (val) 1199
Arch:vit_b_16
/home/primmere/.conda/envs/dfr2/lib/python3.10/site-packages/torch/autograd/graph.py:768: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Initial accuracy: top1=0.7614678899082569
Initial worst accuracy=0.4405594405594406
Initial group accuracies={3: 0.5416666666666666, 2: 0.4405594405594406, 0: 0.875, 1: 0.8179824561403509}
Processing 20% Pruning:  20%|██        | 4/20 [01:36<06:28, 24.27s/it]                      
acc top1 train 0.7
acc worst train 0.525
worst groups train {0: 0.925, 1: 0.8, 2: 0.55, 3: 0.525}
Accuracy-Flow list: [0.7614678899082569]
Worst Accuracy-Flow list: [0.4405594405594406]
Group accuracy: {3: 0.5416666666666666, 2: 0.4405594405594406, 0: 0.875, 1: 0.8179824561403509}
Logged the results of 0% Pruning Rate to wandb!
/home/primmere/logs/pxp/results5.0.pth
acc top1 train 0.69375
acc worst train 0.5
worst groups train {1: 0.8, 2: 0.55, 0: 0.925, 3: 0.5}
Accuracy-Flow list: [0.7614678899082569, 0.7673060884070059]
Worst Accuracy-Flow list: [0.4405594405594406, 0.46853146853146854]
Group accuracy: {3: 0.4930555555555556, 2: 0.46853146853146854, 0: 0.8771929824561403, 1: 0.8377192982456141}
Logged the results of 0.05% Pruning Rate to wandb!
/home/primmere/logs/pxp/results10.0.pth
acc top1 train 0.65
acc worst train 0.4
worst groups train {1: 0.825, 2: 0.45, 0: 0.925, 3: 0.4}
Accuracy-Flow list: [0.7614678899082569, 0.7673060884070059, 0.7597998331943286]
Worst Accuracy-Flow list: [0.4405594405594406, 0.46853146853146854, 0.3776223776223776]
Group accuracy: {3: 0.4791666666666667, 2: 0.3776223776223776, 0: 0.8903508771929824, 1: 0.8377192982456141}
Logged the results of 0.1% Pruning Rate to wandb!
/home/primmere/logs/pxp/results15.0.pth
acc top1 train 0.66875
acc worst train 0.375
worst groups train {0: 0.95, 1: 0.9, 2: 0.45, 3: 0.375}
Accuracy-Flow list: [0.7614678899082569, 0.7673060884070059, 0.7597998331943286, 0.7606338615512928]
Worst Accuracy-Flow list: [0.4405594405594406, 0.46853146853146854, 0.3776223776223776, 0.34265734265734266]
Group accuracy: {3: 0.4444444444444444, 2: 0.34265734265734266, 0: 0.8969298245614035, 1: 0.8552631578947368}
Logged the results of 0.15% Pruning Rate to wandb!
/home/primmere/logs/pxp/results20.0.pth
acc top1 train 0.6625
acc worst train 0.375
worst groups train {3: 0.375, 0: 0.95, 2: 0.45, 1: 0.875}
Accuracy-Flow list: [0.7614678899082569, 0.7673060884070059, 0.7597998331943286, 0.7606338615512928, 0.7606338615512928]
Worst Accuracy-Flow list: [0.4405594405594406, 0.46853146853146854, 0.3776223776223776, 0.34265734265734266, 0.32167832167832167]
Group accuracy: {3: 0.4444444444444444, 2: 0.32167832167832167, 0: 0.8969298245614035, 1: 0.8618421052631579}
Logged the results of 0.2% Pruning Rate to wandb!
/home/primmere/logs/pxp/results25.0.pth
acc top1 train 0.6125
acc worst train 0.35
worst groups train {0: 0.875, 3: 0.35, 1: 0.825, 2: 0.4}
Accuracy-Flow list: [0.7614678899082569, 0.7673060884070059, 0.7597998331943286, 0.7606338615512928, 0.7606338615512928, 0.7264386989157632]
Worst Accuracy-Flow list: [0.4405594405594406, 0.46853146853146854, 0.3776223776223776, 0.34265734265734266, 0.32167832167832167, 0.27972027972027974]
Group accuracy: {3: 0.4166666666666667, 2: 0.27972027972027974, 0: 0.8618421052631579, 1: 0.8289473684210527}
Logged the results of 0.25% Pruning Rate to wandb!
/home/primmere/logs/pxp/results30.0.pth
acc top1 train 0.6
acc worst train 0.4
worst groups train {0: 0.825, 3: 0.4, 2: 0.4, 1: 0.775}
Accuracy-Flow list: [0.7614678899082569, 0.7673060884070059, 0.7597998331943286, 0.7606338615512928, 0.7606338615512928, 0.7264386989157632, 0.6797331109257715]
Worst Accuracy-Flow list: [0.4405594405594406, 0.46853146853146854, 0.3776223776223776, 0.34265734265734266, 0.32167832167832167, 0.27972027972027974, 0.26573426573426573]
Group accuracy: {3: 0.4166666666666667, 2: 0.26573426573426573, 0: 0.7960526315789473, 1: 0.7763157894736842}
Logged the results of 0.3% Pruning Rate to wandb!
/home/primmere/logs/pxp/results35.0.pth
acc top1 train 0.6
acc worst train 0.35
worst groups train {3: 0.35, 0: 0.8, 2: 0.425, 1: 0.825}
Accuracy-Flow list: [0.7614678899082569, 0.7673060884070059, 0.7597998331943286, 0.7606338615512928, 0.7606338615512928, 0.7264386989157632, 0.6797331109257715, 0.6872393661384487]
Worst Accuracy-Flow list: [0.4405594405594406, 0.46853146853146854, 0.3776223776223776, 0.34265734265734266, 0.32167832167832167, 0.27972027972027974, 0.26573426573426573, 0.23076923076923078]
Group accuracy: {3: 0.3680555555555556, 2: 0.23076923076923078, 0: 0.8070175438596491, 1: 0.8114035087719298}
Logged the results of 0.35% Pruning Rate to wandb!
/home/primmere/logs/pxp/results40.0.pth
acc top1 train 0.575
acc worst train 0.25
worst groups train {3: 0.25, 2: 0.375, 0: 0.85, 1: 0.825}
Accuracy-Flow list: [0.7614678899082569, 0.7673060884070059, 0.7597998331943286, 0.7606338615512928, 0.7606338615512928, 0.7264386989157632, 0.6797331109257715, 0.6872393661384487, 0.7055879899916597]
Worst Accuracy-Flow list: [0.4405594405594406, 0.46853146853146854, 0.3776223776223776, 0.34265734265734266, 0.32167832167832167, 0.27972027972027974, 0.26573426573426573, 0.23076923076923078, 0.21678321678321677]
Group accuracy: {3: 0.3055555555555556, 2: 0.21678321678321677, 0: 0.8508771929824561, 1: 0.8399122807017544}
Logged the results of 0.4% Pruning Rate to wandb!
/home/primmere/logs/pxp/results45.0.pth
acc top1 train 0.59375
acc worst train 0.35
worst groups train {0: 0.675, 1: 0.75, 2: 0.6, 3: 0.35}
Accuracy-Flow list: [0.7614678899082569, 0.7673060884070059, 0.7597998331943286, 0.7606338615512928, 0.7606338615512928, 0.7264386989157632, 0.6797331109257715, 0.6872393661384487, 0.7055879899916597, 0.646371976647206]
Worst Accuracy-Flow list: [0.4405594405594406, 0.46853146853146854, 0.3776223776223776, 0.34265734265734266, 0.32167832167832167, 0.27972027972027974, 0.26573426573426573, 0.23076923076923078, 0.21678321678321677, 0.40559440559440557]
Group accuracy: {3: 0.4375, 2: 0.40559440559440557, 0: 0.6995614035087719, 1: 0.7346491228070176}
Logged the results of 0.45% Pruning Rate to wandb!
/home/primmere/logs/pxp/results50.0.pth
acc top1 train 0.5625
acc worst train 0.375
worst groups train {1: 0.725, 3: 0.375, 0: 0.625, 2: 0.525}
Accuracy-Flow list: [0.7614678899082569, 0.7673060884070059, 0.7597998331943286, 0.7606338615512928, 0.7606338615512928, 0.7264386989157632, 0.6797331109257715, 0.6872393661384487, 0.7055879899916597, 0.646371976647206, 0.6271893244370309]
Worst Accuracy-Flow list: [0.4405594405594406, 0.46853146853146854, 0.3776223776223776, 0.34265734265734266, 0.32167832167832167, 0.27972027972027974, 0.26573426573426573, 0.23076923076923078, 0.21678321678321677, 0.40559440559440557, 0.375]
Group accuracy: {3: 0.375, 2: 0.46853146853146854, 0: 0.6469298245614035, 1: 0.7368421052631579}
Logged the results of 0.5% Pruning Rate to wandb!
/home/primmere/logs/pxp/results55.00000000000001.pth
acc top1 train 0.50625
acc worst train 0.35
worst groups train {1: 0.575, 0: 0.575, 2: 0.525, 3: 0.35}
Accuracy-Flow list: [0.7614678899082569, 0.7673060884070059, 0.7597998331943286, 0.7606338615512928, 0.7606338615512928, 0.7264386989157632, 0.6797331109257715, 0.6872393661384487, 0.7055879899916597, 0.646371976647206, 0.6271893244370309, 0.5496246872393661]
Worst Accuracy-Flow list: [0.4405594405594406, 0.46853146853146854, 0.3776223776223776, 0.34265734265734266, 0.32167832167832167, 0.27972027972027974, 0.26573426573426573, 0.23076923076923078, 0.21678321678321677, 0.40559440559440557, 0.375, 0.4444444444444444]
Group accuracy: {3: 0.4444444444444444, 2: 0.44755244755244755, 0: 0.5570175438596491, 1: 0.6074561403508771}
Logged the results of 0.55% Pruning Rate to wandb!
/home/primmere/logs/pxp/results60.0.pth
acc top1 train 0.53125
acc worst train 0.425
worst groups train {0: 0.5, 1: 0.675, 3: 0.425, 2: 0.525}
Accuracy-Flow list: [0.7614678899082569, 0.7673060884070059, 0.7597998331943286, 0.7606338615512928, 0.7606338615512928, 0.7264386989157632, 0.6797331109257715, 0.6872393661384487, 0.7055879899916597, 0.646371976647206, 0.6271893244370309, 0.5496246872393661, 0.5346121768140116]
Worst Accuracy-Flow list: [0.4405594405594406, 0.46853146853146854, 0.3776223776223776, 0.34265734265734266, 0.32167832167832167, 0.27972027972027974, 0.26573426573426573, 0.23076923076923078, 0.21678321678321677, 0.40559440559440557, 0.375, 0.4444444444444444, 0.3819444444444444]
Group accuracy: {3: 0.3819444444444444, 2: 0.44755244755244755, 0: 0.5657894736842105, 1: 0.5789473684210527}
Logged the results of 0.6% Pruning Rate to wandb!
/home/primmere/logs/pxp/results65.0.pth
acc top1 train 0.53125
acc worst train 0.4
worst groups train {1: 0.625, 2: 0.4, 0: 0.65, 3: 0.45}
Accuracy-Flow list: [0.7614678899082569, 0.7673060884070059, 0.7597998331943286, 0.7606338615512928, 0.7606338615512928, 0.7264386989157632, 0.6797331109257715, 0.6872393661384487, 0.7055879899916597, 0.646371976647206, 0.6271893244370309, 0.5496246872393661, 0.5346121768140116, 0.5713094245204337]
Worst Accuracy-Flow list: [0.4405594405594406, 0.46853146853146854, 0.3776223776223776, 0.34265734265734266, 0.32167832167832167, 0.27972027972027974, 0.26573426573426573, 0.23076923076923078, 0.21678321678321677, 0.40559440559440557, 0.375, 0.4444444444444444, 0.3819444444444444, 0.3888888888888889]
Group accuracy: {3: 0.3888888888888889, 2: 0.44755244755244755, 0: 0.6074561403508771, 1: 0.631578947368421}
Logged the results of 0.65% Pruning Rate to wandb!
/home/primmere/logs/pxp/results70.0.pth
acc top1 train 0.56875
acc worst train 0.45
worst groups train {0: 0.625, 3: 0.45, 2: 0.475, 1: 0.725}
Accuracy-Flow list: [0.7614678899082569, 0.7673060884070059, 0.7597998331943286, 0.7606338615512928, 0.7606338615512928, 0.7264386989157632, 0.6797331109257715, 0.6872393661384487, 0.7055879899916597, 0.646371976647206, 0.6271893244370309, 0.5496246872393661, 0.5346121768140116, 0.5713094245204337, 0.603836530442035]
Worst Accuracy-Flow list: [0.4405594405594406, 0.46853146853146854, 0.3776223776223776, 0.34265734265734266, 0.32167832167832167, 0.27972027972027974, 0.26573426573426573, 0.23076923076923078, 0.21678321678321677, 0.40559440559440557, 0.375, 0.4444444444444444, 0.3819444444444444, 0.3888888888888889, 0.3680555555555556]
Group accuracy: {3: 0.3680555555555556, 2: 0.5664335664335665, 0: 0.581140350877193, 1: 0.7127192982456141}
Logged the results of 0.7% Pruning Rate to wandb!
/home/primmere/logs/pxp/results75.0.pth
acc top1 train 0.5125
acc worst train 0.125
worst groups train {0: 0.75, 2: 0.225, 3: 0.125, 1: 0.95}
Accuracy-Flow list: [0.7614678899082569, 0.7673060884070059, 0.7597998331943286, 0.7606338615512928, 0.7606338615512928, 0.7264386989157632, 0.6797331109257715, 0.6872393661384487, 0.7055879899916597, 0.646371976647206, 0.6271893244370309, 0.5496246872393661, 0.5346121768140116, 0.5713094245204337, 0.603836530442035, 0.7022518765638032]
Worst Accuracy-Flow list: [0.4405594405594406, 0.46853146853146854, 0.3776223776223776, 0.34265734265734266, 0.32167832167832167, 0.27972027972027974, 0.26573426573426573, 0.23076923076923078, 0.21678321678321677, 0.40559440559440557, 0.375, 0.4444444444444444, 0.3819444444444444, 0.3888888888888889, 0.3680555555555556, 0.13194444444444445]
Group accuracy: {3: 0.13194444444444445, 2: 0.23076923076923078, 0: 0.8048245614035088, 1: 0.9276315789473685}
Logged the results of 0.75% Pruning Rate to wandb!
/home/primmere/logs/pxp/results80.0.pth
acc top1 train 0.48125
acc worst train 0.2
worst groups train {3: 0.2, 1: 0.725, 0: 0.7, 2: 0.3}
Accuracy-Flow list: [0.7614678899082569, 0.7673060884070059, 0.7597998331943286, 0.7606338615512928, 0.7606338615512928, 0.7264386989157632, 0.6797331109257715, 0.6872393661384487, 0.7055879899916597, 0.646371976647206, 0.6271893244370309, 0.5496246872393661, 0.5346121768140116, 0.5713094245204337, 0.603836530442035, 0.7022518765638032, 0.5996663886572143]
Worst Accuracy-Flow list: [0.4405594405594406, 0.46853146853146854, 0.3776223776223776, 0.34265734265734266, 0.32167832167832167, 0.27972027972027974, 0.26573426573426573, 0.23076923076923078, 0.21678321678321677, 0.40559440559440557, 0.375, 0.4444444444444444, 0.3819444444444444, 0.3888888888888889, 0.3680555555555556, 0.13194444444444445, 0.22916666666666666]
Group accuracy: {3: 0.22916666666666666, 2: 0.2867132867132867, 0: 0.6842105263157895, 1: 0.7302631578947368}
Logged the results of 0.8% Pruning Rate to wandb!
/home/primmere/logs/pxp/results85.0.pth
acc top1 train 0.475
acc worst train 0.125
worst groups train {0: 0.125, 2: 0.825, 1: 0.525, 3: 0.425}
Accuracy-Flow list: [0.7614678899082569, 0.7673060884070059, 0.7597998331943286, 0.7606338615512928, 0.7606338615512928, 0.7264386989157632, 0.6797331109257715, 0.6872393661384487, 0.7055879899916597, 0.646371976647206, 0.6271893244370309, 0.5496246872393661, 0.5346121768140116, 0.5713094245204337, 0.603836530442035, 0.7022518765638032, 0.5996663886572143, 0.34361968306922436]
Worst Accuracy-Flow list: [0.4405594405594406, 0.46853146853146854, 0.3776223776223776, 0.34265734265734266, 0.32167832167832167, 0.27972027972027974, 0.26573426573426573, 0.23076923076923078, 0.21678321678321677, 0.40559440559440557, 0.375, 0.4444444444444444, 0.3819444444444444, 0.3888888888888889, 0.3680555555555556, 0.13194444444444445, 0.22916666666666666, 0.1074561403508772]
Group accuracy: {3: 0.5208333333333334, 2: 0.8321678321678322, 0: 0.1074561403508772, 1: 0.3706140350877193}
Logged the results of 0.85% Pruning Rate to wandb!
/home/primmere/logs/pxp/results90.0.pth
acc top1 train 0.5
acc worst train 0.075
worst groups train {0: 0.075, 2: 0.925, 3: 0.625, 1: 0.375}
Accuracy-Flow list: [0.7614678899082569, 0.7673060884070059, 0.7597998331943286, 0.7606338615512928, 0.7606338615512928, 0.7264386989157632, 0.6797331109257715, 0.6872393661384487, 0.7055879899916597, 0.646371976647206, 0.6271893244370309, 0.5496246872393661, 0.5346121768140116, 0.5713094245204337, 0.603836530442035, 0.7022518765638032, 0.5996663886572143, 0.34361968306922436, 0.32360300250208507]
Worst Accuracy-Flow list: [0.4405594405594406, 0.46853146853146854, 0.3776223776223776, 0.34265734265734266, 0.32167832167832167, 0.27972027972027974, 0.26573426573426573, 0.23076923076923078, 0.21678321678321677, 0.40559440559440557, 0.375, 0.4444444444444444, 0.3819444444444444, 0.3888888888888889, 0.3680555555555556, 0.13194444444444445, 0.22916666666666666, 0.1074561403508772, 0.07675438596491228]
Group accuracy: {3: 0.6666666666666666, 2: 0.8811188811188811, 0: 0.07675438596491228, 1: 0.28728070175438597}
Logged the results of 0.9% Pruning Rate to wandb!
/home/primmere/logs/pxp/results95.0.pth
acc top1 train 0.4875
acc worst train 0.0
worst groups train {3: 0.925, 1: 0.025, 2: 1.0, 0: 0.0}
Accuracy-Flow list: [0.7614678899082569, 0.7673060884070059, 0.7597998331943286, 0.7606338615512928, 0.7606338615512928, 0.7264386989157632, 0.6797331109257715, 0.6872393661384487, 0.7055879899916597, 0.646371976647206, 0.6271893244370309, 0.5496246872393661, 0.5346121768140116, 0.5713094245204337, 0.603836530442035, 0.7022518765638032, 0.5996663886572143, 0.34361968306922436, 0.32360300250208507, 0.2535446205170976]
Worst Accuracy-Flow list: [0.4405594405594406, 0.46853146853146854, 0.3776223776223776, 0.34265734265734266, 0.32167832167832167, 0.27972027972027974, 0.26573426573426573, 0.23076923076923078, 0.21678321678321677, 0.40559440559440557, 0.375, 0.4444444444444444, 0.3819444444444444, 0.3888888888888889, 0.3680555555555556, 0.13194444444444445, 0.22916666666666666, 0.1074561403508772, 0.07675438596491228, 0.006578947368421052]
Group accuracy: {3: 0.9444444444444444, 2: 0.9790209790209791, 0: 0.006578947368421052, 1: 0.05482456140350877}
Logged the results of 0.95% Pruning Rate to wandb!
/home/primmere/ide/external/pruning_by_explaining/my_experiments/my_global_pruning.py:301: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.
  top1_auc = np.trapz(top1_acc_list, pruning_rates)
Top1 AUC: 0.5928482068390326
Logged the AUC of the Top1 Accuracy to wandb!
Top1 AUC: 0.5928482068390326
Top1 AUC: 0.2818157536907537
Worst AUC: 0.2818157536907537
