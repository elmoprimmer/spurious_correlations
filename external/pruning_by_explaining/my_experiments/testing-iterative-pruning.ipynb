{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c79d284bb264d1f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T21:39:59.236365Z",
     "start_time": "2025-07-02T21:39:45.451882600Z"
    },
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import click\n",
    "import torch\n",
    "import tqdm.auto\n",
    "import numpy as np\n",
    "from torchvision.models import vit_b_16\n",
    " \n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "#project_root = \"C:/Users/elmop/deep_feature_reweighting/deep_feature_reweighting/external/pruning_by_explaining\"\n",
    "project_root = \"/home/primmere/ide/external/pruning_by_explaining\"\n",
    "sys.path.insert(0, project_root)                 \n",
    "sys.path.insert(0, os.path.dirname(project_root))\n",
    "\n",
    "from pruning_by_explaining.models import ModelLoader\n",
    "from pruning_by_explaining.metrics import compute_accuracy\n",
    "from pruning_by_explaining.my_metrics import compute_worst_accuracy\n",
    "from pruning_by_explaining.my_datasets import WaterBirds, get_sample_indices_for_group, WaterBirdSubset, ISIC, ISICSubset\n",
    "from pruning_by_explaining.utils import (\n",
    "    initialize_random_seed,\n",
    "    initialize_wandb_logger,\n",
    ")\n",
    "\n",
    "from pruning_by_explaining.pxp import (\n",
    "    ModelLayerUtils,\n",
    "    get_cnn_composite,\n",
    "    get_vit_composite,\n",
    ")\n",
    "\n",
    "from pruning_by_explaining.pxp import GlobalPruningOperations\n",
    "from pruning_by_explaining.pxp import ComponentAttibution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b41d0ce48cf8cb4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T08:24:20.023885700Z",
     "start_time": "2025-07-02T08:24:19.673321500Z"
    },
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n",
      "Number of unique labels: 2, Number of unique places: 2, Total groups: 4\n",
      "group 0: 3518\n",
      "group 1: 185\n",
      "group 2: 55\n",
      "group 3: 1037\n",
      "[0 1 2 3]\n",
      "Number of unique labels: 2, Number of unique places: 2, Total groups: 4\n",
      "group 0: 456\n",
      "group 1: 456\n",
      "group 2: 143\n",
      "group 3: 144\n",
      "[0 1 2 3]\n",
      "Number of unique labels: 2, Number of unique places: 2, Total groups: 4\n",
      "group 0: 2255\n",
      "group 1: 2255\n",
      "group 2: 642\n",
      "group 3: 642\n",
      "target groups: [1]\n",
      "pruning indices: 30\n"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "initialize_random_seed(seed)\n",
    "num_workers = 12\n",
    "device_string = \"cuda\"\n",
    "device = torch.device(device_string)\n",
    "waterbirds = WaterBirds('/scratch_shared/primmere/waterbird', seed = 1, num_workers = num_workers)\n",
    "\n",
    "least_rel_first = True\n",
    "abs_flag = True\n",
    "#least_rel_first2 = False\n",
    "#abs_flag2 = False\n",
    "#Zplus_flag = True\n",
    "\n",
    "#scale_bool = True\n",
    "\n",
    "prune_r = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "\n",
    "           \n",
    "layer_type = 'Softmax'\n",
    "\n",
    "train_set = waterbirds.get_train_set()\n",
    "val_set = waterbirds.get_valid_set()\n",
    "test_set = waterbirds.get_test_set()\n",
    "\n",
    "pruning_indices = get_sample_indices_for_group(val_set, 30, device_string, [1])\n",
    "print(\"pruning indices:\" , len(pruning_indices))\n",
    "\n",
    "custom_pruning_set = ISICSubset(val_set, pruning_indices)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=num_workers)\n",
    "prune_dataloader = torch.utils.data.DataLoader(custom_pruning_set, batch_size=1, shuffle=True, num_workers=num_workers)\n",
    "#prune_dataloader2 = torch.utils.data.DataLoader(custom_pruning_set2, batch_size=8, shuffle=True, num_workers=num_workers)\n",
    "#prune_dataloader3 = torch.utils.data.DataLoader(custom_pruning_set3, batch_size=8, shuffle=True, num_workers=num_workers)\n",
    "val_dataloader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "672071d5bfd8ce01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T21:40:07.494175Z",
     "start_time": "2025-07-02T21:39:59.246373Z"
    },
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "suggested_composite = {\n",
    "        \"low_level_hidden_layer_rule\": \"Epsilon\",\n",
    "        \"mid_level_hidden_layer_rule\":\"Epsilon\",\n",
    "        \"high_level_hidden_layer_rule\": \"Epsilon\",\n",
    "        \"fully_connected_layers_rule\": \"Epsilon\",\n",
    "        \"softmax_rule\": \"Epsilon\",\n",
    "    }\n",
    "composite = get_vit_composite(\"vit_b_16\", suggested_composite)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de2053c4-6614-46aa-be9f-b59ef6189006",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arch:vit_b_16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/primmere/ide/external/pruning_by_explaining/models/utils.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "model = ModelLoader.get_basic_model(\"vit_b_16\", \"/home/primmere/ide/dfr/logs/vit_waterbirds.pth\", device, num_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ea11d17-050c-472d-8ab6-8d2523ac3bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Sequential(\n",
       "    (head): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "218d21ab3e4aedec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T08:24:27.257200600Z",
     "start_time": "2025-07-02T08:24:27.243210Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "layer_types = {\n",
    "        \"Softmax\": torch.nn.Softmax,\n",
    "        \"Linear\": torch.nn.Linear,\n",
    "        \"Conv2d\": torch.nn.Conv2d,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20258526-a237-412b-957e-986133fc5f23",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n0: 0.9937915742793791\\n1: 0.7835920177383592\\n2: 0.7461059190031153\\n3: 0.956386292834891\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "acc_worst, acc_groups = compute_worst_accuracy(\n",
    "        model,\n",
    "        val_dataloader,\n",
    "        device,\n",
    "    )\n",
    "for i in range(4):\n",
    "    print(f'{i}: {acc_groups[i]}')\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "0: 0.9937915742793791\n",
    "1: 0.7835920177383592\n",
    "2: 0.7461059190031153\n",
    "3: 0.956386292834891\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ef84cca0140e419",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T08:41:21.019829100Z",
     "start_time": "2025-07-02T08:41:20.959062Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/primmere/.conda/envs/dfr2/lib/python3.10/site-packages/torch/autograd/graph.py:768: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n",
      "[<torch.utils.hooks.RemovableHandle object at 0x151ed103f100>]\n"
     ]
    }
   ],
   "source": [
    "component_attributor = ComponentAttibution(\n",
    "        \"Relevance\",\n",
    "        \"ViT\",\n",
    "        layer_types[layer_type],\n",
    "        True\n",
    "    )\n",
    "print(\"done\")\n",
    "\n",
    "components_relevances = component_attributor.attribute(\n",
    "        model,\n",
    "        prune_dataloader,\n",
    "        composite,\n",
    "        abs_flag=True,\n",
    "        device=device,\n",
    "    )\n",
    "print(\"done!\")\n",
    "\n",
    "layer_names = component_attributor.layer_names\n",
    "pruner = GlobalPruningOperations(\n",
    "        layer_types[layer_type],\n",
    "        layer_names,\n",
    "    )\n",
    "\n",
    "global_pruning_mask = pruner.generate_global_pruning_mask(\n",
    "                model,\n",
    "                components_relevances,\n",
    "                0.05,\n",
    "                subsequent_layer_pruning=\"Both\",\n",
    "                least_relevant_first=True,\n",
    "                device=device,\n",
    "            )\n",
    "\n",
    "hook_handles = pruner.fit_pruning_mask(model, global_pruning_mask,)\n",
    "print(hook_handles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f1f9cca-a86a-4e87-b357-1298e0341e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encoder.layers.encoder_layer_0.self_attention.softmax',\n",
       "              tensor([11,  4])),\n",
       "             ('encoder.layers.encoder_layer_1.self_attention.softmax',\n",
       "              tensor([5, 3])),\n",
       "             ('encoder.layers.encoder_layer_2.self_attention.softmax',\n",
       "              tensor([2, 7, 0])),\n",
       "             ('encoder.layers.encoder_layer_3.self_attention.softmax',\n",
       "              tensor([], dtype=torch.int64)),\n",
       "             ('encoder.layers.encoder_layer_4.self_attention.softmax',\n",
       "              tensor([], dtype=torch.int64)),\n",
       "             ('encoder.layers.encoder_layer_5.self_attention.softmax',\n",
       "              tensor([], dtype=torch.int64)),\n",
       "             ('encoder.layers.encoder_layer_6.self_attention.softmax',\n",
       "              tensor([], dtype=torch.int64)),\n",
       "             ('encoder.layers.encoder_layer_7.self_attention.softmax',\n",
       "              tensor([], dtype=torch.int64)),\n",
       "             ('encoder.layers.encoder_layer_8.self_attention.softmax',\n",
       "              tensor([], dtype=torch.int64)),\n",
       "             ('encoder.layers.encoder_layer_9.self_attention.softmax',\n",
       "              tensor([], dtype=torch.int64)),\n",
       "             ('encoder.layers.encoder_layer_10.self_attention.softmax',\n",
       "              tensor([], dtype=torch.int64)),\n",
       "             ('encoder.layers.encoder_layer_11.self_attention.softmax',\n",
       "              tensor([], dtype=torch.int64))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_pruning_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea6a59ca-654a-4b9d-bdeb-6d3409a82d7d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arch:vit_b_16\n"
     ]
    }
   ],
   "source": [
    "model2 = ModelLoader.get_basic_model(\"vit_b_16\", \"/home/primmere/ide/dfr/logs/vit_waterbirds.pth\", device, num_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "881c4b94-298c-43b7-a4ae-9bd108ed3616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11  4]\n",
      "[5 3]\n",
      "[2 7 0]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for k, v in global_pruning_mask.items():\n",
    "    print(v.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7df52e4-7047-4424-b14e-ec69742beff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for layer in model2.encoder.layers:\n",
    "    print(layer.self_attention.out_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edf55f16-a489-4561-b788-69d74c399d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderBlock(\n",
       "  (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (self_attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (mlp): MLPBlock(\n",
       "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Dropout(p=0.0, inplace=False)\n",
       "    (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (4): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.encoder.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6748c5d1-a01c-45fb-81b1-3fd6b41bb814",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n0: 0.9868421052631579\\n1: 0.7982456140350878\\n2: 0.7272727272727273\\n3: 0.9791666666666666\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "0: 0.9868421052631579\n",
    "1: 0.7982456140350878\n",
    "2: 0.7272727272727273\n",
    "3: 0.9791666666666666\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99464959-834e-4618-8462-95f7a049ef4c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mask_head(head):\n",
    "    def hook(module, _input, output):\n",
    "        y = output[0]       # y = attn_output (B, N, E)\n",
    "        hdim = module.head_dim # 64\n",
    "        y[..., head*hdim:(head+1)*hdim] = 0.0\n",
    "        return (y, output[1]) if len(output) == 2 else y\n",
    "    return hook\n",
    "\n",
    "\n",
    "def apply_mask_softmax(model, mask):\n",
    "    hook_handles = []\n",
    "    for layer, (k, v) in zip(model.encoder.layers, mask.items()):\n",
    "        for head in v.detach().cpu().numpy():\n",
    "            hook_handles.append(layer.self_attention.register_forward_hook(mask_head(head)))\n",
    "    return hook_handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a8dda8-229d-40b3-a70d-3b2adf53fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f48dd16-26d5-4b67-af07-4b6d6a9fdce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef debug_head(layer, head):\\n    def dbg_hook(module, _input, output):\\n        y = output[0]\\n        hdim = module.head_dim\\n        sl = y[..., head*hdim:(head+1)*hdim]\\n        print(f'{layer} head-{head}: max |value| =', sl.abs().max().item())\\n    return dbg_hook\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def debug_head(layer, head):\n",
    "    def dbg_hook(module, _input, output):\n",
    "        y = output[0]\n",
    "        hdim = module.head_dim\n",
    "        sl = y[..., head*hdim:(head+1)*hdim]\n",
    "        print(f'{layer} head-{head}: max |value| =', sl.abs().max().item())\n",
    "    return dbg_hook\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c891b15-9f35-4de0-b1d7-067bd91d3781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndbg_handles = []\\ni = 0\\nfor layer in model2.encoder.layers:\\n    for head in range(12):\\n        dbg_handles.append(layer.self_attention.register_forward_hook(debug_head(i, head)))\\n    i += 1\\n\\nprint(len(dbg_handles))\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "dbg_handles = []\n",
    "i = 0\n",
    "for layer in model2.encoder.layers:\n",
    "    for head in range(12):\n",
    "        dbg_handles.append(layer.self_attention.register_forward_hook(debug_head(i, head)))\n",
    "    i += 1\n",
    "\n",
    "print(len(dbg_handles))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "470a1b4a-b072-40f1-a2f2-8f75e9bba237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Arch:vit_b_16\n",
      "tensor([ 1.1921e-07,  5.9605e-08,  5.9605e-08,  2.3842e-07, -5.9605e-08,\n",
      "         5.9605e-08, -4.7684e-07,  2.3842e-07,  0.0000e+00,  1.1921e-07,\n",
      "         0.0000e+00, -5.9605e-08])\n",
      "tensor([ 1.1921e-07,  5.9605e-08,  0.0000e+00,  0.0000e+00,  1.1921e-07,\n",
      "        -2.9802e-08, -3.5763e-07, -5.9605e-08,  0.0000e+00, -5.9605e-08,\n",
      "        -5.9605e-08,  0.0000e+00])\n",
      "tensor([-5.9605e-08, -2.3842e-07, -5.9605e-08,  0.0000e+00,  1.1921e-07,\n",
      "         9.5367e-07,  1.1921e-07, -5.9605e-08,  0.0000e+00,  1.1921e-07,\n",
      "         0.0000e+00, -4.7684e-07])\n",
      "tensor([-2.3842e-07,  0.0000e+00, -2.3842e-07,  1.1921e-07, -2.3842e-07,\n",
      "        -9.5367e-07,  0.0000e+00,  4.7684e-07,  0.0000e+00, -2.3842e-07,\n",
      "        -2.3842e-07,  2.3842e-07])\n",
      "tensor([ 4.7684e-07,  4.7684e-07,  4.7684e-07,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  4.7684e-07, -4.7684e-07,  4.7684e-07,\n",
      "         2.3842e-07,  2.3842e-07])\n",
      "tensor([ 0.0000e+00,  0.0000e+00,  2.3842e-07, -2.3842e-07,  4.7684e-07,\n",
      "         4.7684e-07,  2.3842e-07,  0.0000e+00,  0.0000e+00,  4.7684e-07,\n",
      "         0.0000e+00,  4.7684e-07])\n",
      "tensor([ 2.3842e-07, -4.7684e-07,  0.0000e+00, -4.7684e-07,  0.0000e+00,\n",
      "         2.3842e-07, -2.3842e-07, -2.3842e-07, -2.3842e-07, -2.3842e-07,\n",
      "         0.0000e+00, -2.3842e-07])\n",
      "tensor([-4.7684e-07,  4.7684e-07,  2.3842e-07,  0.0000e+00,  0.0000e+00,\n",
      "         2.3842e-07,  0.0000e+00, -2.3842e-07,  2.3842e-07,  4.7684e-07,\n",
      "        -2.3842e-07,  0.0000e+00])\n",
      "tensor([ 2.3842e-07, -2.3842e-07, -4.7684e-07,  0.0000e+00, -4.7684e-07,\n",
      "        -2.3842e-07,  0.0000e+00,  0.0000e+00, -2.3842e-07,  1.1921e-07,\n",
      "         2.3842e-07,  0.0000e+00])\n",
      "tensor([-1.1921e-07,  0.0000e+00, -1.1921e-07,  0.0000e+00,  1.1921e-07,\n",
      "         1.1921e-07,  0.0000e+00,  7.1526e-07, -2.3842e-07,  1.1921e-07,\n",
      "         0.0000e+00,  1.7881e-07])\n",
      "tensor([-1.1921e-07,  0.0000e+00,  4.7684e-07,  1.1921e-07,  1.7881e-07,\n",
      "        -2.3842e-07,  1.1921e-07, -2.3842e-07, -5.9605e-08,  0.0000e+00,\n",
      "         4.7684e-07,  0.0000e+00])\n",
      "tensor([ 1.1921e-07, -1.1921e-07,  0.0000e+00,  2.3842e-07,  1.1921e-07,\n",
      "        -1.1921e-07, -5.9605e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -9.5367e-07,  0.0000e+00])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating group acc:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.882119433897135\n",
      "0: 0.9933481152993349\n",
      "1: 0.7906873614190687\n",
      "2: 0.7383177570093458\n",
      "3: 0.956386292834891\n",
      "0.15\n",
      "Arch:vit_b_16\n",
      "tensor([ 1.1921e-07,  5.9605e-08, -5.9605e-08,  2.3842e-07, -5.9605e-08,\n",
      "         1.7881e-07,  0.0000e+00,  2.3842e-07,  1.1921e-07,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00])\n",
      "tensor([ 1.1921e-07,  0.0000e+00,  4.7684e-07, -5.9605e-08,  0.0000e+00,\n",
      "         0.0000e+00, -2.3842e-07,  0.0000e+00,  4.7684e-07, -5.9605e-08,\n",
      "         5.9605e-08,  0.0000e+00])\n",
      "tensor([-1.1921e-07, -1.1921e-07,  2.9802e-08,  0.0000e+00,  0.0000e+00,\n",
      "         4.7684e-07,  0.0000e+00,  5.9605e-08, -2.3842e-07, -1.1921e-07,\n",
      "        -4.7684e-07,  0.0000e+00])\n",
      "tensor([-2.3842e-07,  0.0000e+00, -2.3842e-07,  2.3842e-07,  0.0000e+00,\n",
      "        -1.4305e-06, -9.5367e-07,  0.0000e+00, -1.1921e-07, -2.3842e-07,\n",
      "        -2.3842e-07,  0.0000e+00])\n",
      "tensor([4.7684e-07, 4.7684e-07, 0.0000e+00, 0.0000e+00, 2.3842e-07, 1.1921e-07,\n",
      "        9.5367e-07, 4.7684e-07, 4.7684e-07, 4.7684e-07, 2.3842e-07, 0.0000e+00])\n",
      "tensor([ 0.0000e+00,  2.3842e-07,  4.7684e-07, -2.3842e-07, -2.3842e-07,\n",
      "         0.0000e+00,  2.3842e-07,  2.3842e-07,  0.0000e+00,  0.0000e+00,\n",
      "         4.7684e-07,  4.7684e-07])\n",
      "tensor([ 2.3842e-07, -2.3842e-07,  2.3842e-07,  0.0000e+00,  0.0000e+00,\n",
      "         2.3842e-07, -2.3842e-07, -2.3842e-07, -2.3842e-07, -2.3842e-07,\n",
      "         4.7684e-07, -7.1526e-07])\n",
      "tensor([-2.3842e-07, -4.7684e-07,  7.1526e-07,  2.3842e-07,  0.0000e+00,\n",
      "        -2.3842e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.7684e-07,\n",
      "         0.0000e+00,  0.0000e+00])\n",
      "tensor([ 2.3842e-07,  2.3842e-07, -9.5367e-07,  0.0000e+00,  0.0000e+00,\n",
      "        -3.5763e-07,  0.0000e+00,  0.0000e+00, -4.7684e-07,  2.3842e-07,\n",
      "         2.3842e-07,  0.0000e+00])\n",
      "tensor([-1.1921e-07, -2.3842e-07,  0.0000e+00,  0.0000e+00, -5.9605e-08,\n",
      "         0.0000e+00,  1.1921e-07,  4.7684e-07, -4.7684e-07,  0.0000e+00,\n",
      "        -1.1921e-07,  0.0000e+00])\n",
      "tensor([-2.3842e-07, -1.1921e-07,  2.3842e-07,  1.1921e-07,  1.7881e-07,\n",
      "        -3.5763e-07,  2.3842e-07, -2.3842e-07,  5.9605e-08, -1.1921e-07,\n",
      "         0.0000e+00,  5.9605e-08])\n",
      "tensor([ 0.0000e+00,  0.0000e+00, -1.1921e-07,  2.3842e-07,  1.1921e-07,\n",
      "        -1.1921e-07,  5.9605e-08, -2.3842e-07,  0.0000e+00,  2.3842e-07,\n",
      "        -2.3842e-07,  0.0000e+00])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating group acc:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8860890576458406\n",
      "0: 0.9942350332594235\n",
      "1: 0.8088691796008869\n",
      "2: 0.7133956386292835\n",
      "3: 0.9501557632398754\n",
      "0.2\n",
      "Arch:vit_b_16\n",
      "tensor([ 2.3842e-07,  5.9605e-08,  5.9605e-08,  2.3842e-07, -5.9605e-08,\n",
      "         0.0000e+00, -4.7684e-07,  2.3842e-07,  1.1921e-07,  1.1921e-07,\n",
      "        -2.3842e-07,  0.0000e+00])\n",
      "tensor([ 0.0000e+00,  5.9605e-08,  0.0000e+00,  5.9605e-08,  1.1921e-07,\n",
      "         2.9802e-08, -2.3842e-07,  0.0000e+00,  0.0000e+00, -1.7881e-07,\n",
      "         5.9605e-08, -2.3842e-07])\n",
      "tensor([-5.9605e-08, -2.3842e-07,  0.0000e+00,  0.0000e+00,  1.1921e-07,\n",
      "         4.7684e-07, -1.1921e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -4.7684e-07, -9.5367e-07])\n",
      "tensor([-2.3842e-07,  1.1921e-07,  2.3842e-07,  1.1921e-07, -2.3842e-07,\n",
      "        -1.4305e-06,  0.0000e+00,  0.0000e+00,  1.1921e-07,  0.0000e+00,\n",
      "        -4.7684e-07,  2.3842e-07])\n",
      "tensor([ 4.7684e-07,  2.3842e-07,  2.3842e-07, -4.7684e-07,  0.0000e+00,\n",
      "         1.1921e-07,  4.7684e-07,  4.7684e-07,  0.0000e+00,  9.5367e-07,\n",
      "         0.0000e+00, -4.7684e-07])\n",
      "tensor([-4.7684e-07,  2.3842e-07,  9.5367e-07, -2.3842e-07,  4.7684e-07,\n",
      "         0.0000e+00,  4.7684e-07,  4.7684e-07,  4.7684e-07,  4.7684e-07,\n",
      "        -4.7684e-07,  0.0000e+00])\n",
      "tensor([ 0.0000e+00,  0.0000e+00,  2.3842e-07, -4.7684e-07,  4.7684e-07,\n",
      "         0.0000e+00, -4.7684e-07, -4.7684e-07,  0.0000e+00, -2.3842e-07,\n",
      "        -4.7684e-07, -4.7684e-07])\n",
      "tensor([-4.7684e-07,  4.7684e-07,  2.3842e-07,  0.0000e+00,  2.3842e-07,\n",
      "         4.7684e-07, -2.3842e-07,  0.0000e+00,  4.7684e-07,  4.7684e-07,\n",
      "         2.3842e-07,  2.3842e-07])\n",
      "tensor([ 0.0000e+00, -2.3842e-07, -4.7684e-07,  0.0000e+00,  4.7684e-07,\n",
      "        -2.3842e-07,  4.7684e-07,  2.3842e-07,  0.0000e+00, -1.1921e-07,\n",
      "         2.3842e-07,  0.0000e+00])\n",
      "tensor([-1.1921e-07, -2.3842e-07,  2.3842e-07,  4.7684e-07,  5.9605e-08,\n",
      "         2.3842e-07, -1.1921e-07,  4.7684e-07, -4.7684e-07,  0.0000e+00,\n",
      "        -1.1921e-07,  1.7881e-07])\n",
      "tensor([ 0.0000e+00,  0.0000e+00,  2.3842e-07,  2.3842e-07,  1.1921e-07,\n",
      "         0.0000e+00,  1.1921e-07,  2.3842e-07, -5.9605e-08,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00])\n",
      "tensor([ 0.0000e+00,  0.0000e+00,  2.3842e-07,  2.3842e-07,  2.3842e-07,\n",
      "         0.0000e+00,  0.0000e+00,  4.7684e-07,  0.0000e+00, -2.3842e-07,\n",
      "        -2.3842e-07, -2.3842e-07])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating group acc:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8705557473248188\n",
      "0: 0.9929046563192905\n",
      "1: 0.7600886917960089\n",
      "2: 0.7383177570093458\n",
      "3: 0.9610591900311527\n",
      "0.3\n",
      "Arch:vit_b_16\n",
      "tensor([ 1.1921e-07,  1.7881e-07,  5.9605e-08,  2.3842e-07,  0.0000e+00,\n",
      "         5.9605e-08,  0.0000e+00,  2.3842e-07,  2.3842e-07, -1.1921e-07,\n",
      "         2.3842e-07,  0.0000e+00])\n",
      "tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.1921e-07,\n",
      "         0.0000e+00, -2.3842e-07,  0.0000e+00,  4.7684e-07, -5.9605e-08,\n",
      "         5.9605e-08,  0.0000e+00])\n",
      "tensor([ 0.0000e+00,  1.1921e-07,  0.0000e+00,  0.0000e+00,  1.1921e-07,\n",
      "         0.0000e+00, -1.1921e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -4.7684e-07])\n",
      "tensor([ 0.0000e+00, -1.1921e-07,  2.3842e-07, -1.1921e-07, -2.3842e-07,\n",
      "        -9.5367e-07, -4.7684e-07,  0.0000e+00,  1.1921e-07,  0.0000e+00,\n",
      "         0.0000e+00,  4.7684e-07])\n",
      "tensor([ 4.7684e-07,  2.3842e-07,  0.0000e+00, -4.7684e-07,  0.0000e+00,\n",
      "        -1.1921e-07,  4.7684e-07,  4.7684e-07,  0.0000e+00,  4.7684e-07,\n",
      "         2.3842e-07,  0.0000e+00])\n",
      "tensor([ 0.0000e+00,  2.3842e-07,  2.3842e-07, -2.3842e-07,  4.7684e-07,\n",
      "         4.7684e-07,  2.3842e-07, -2.3842e-07,  4.7684e-07,  0.0000e+00,\n",
      "         0.0000e+00,  2.3842e-07])\n",
      "tensor([ 0.0000e+00, -2.3842e-07,  2.3842e-07,  0.0000e+00,  0.0000e+00,\n",
      "        -2.3842e-07, -7.1526e-07, -2.3842e-07, -7.1526e-07, -2.3842e-07,\n",
      "         9.5367e-07, -2.3842e-07])\n",
      "tensor([-2.3842e-07,  4.7684e-07,  2.3842e-07,  2.3842e-07,  0.0000e+00,\n",
      "        -2.3842e-07,  0.0000e+00,  0.0000e+00,  2.3842e-07,  2.3842e-07,\n",
      "         0.0000e+00,  0.0000e+00])\n",
      "tensor([ 2.3842e-07,  0.0000e+00, -4.7684e-07,  0.0000e+00,  4.7684e-07,\n",
      "        -2.3842e-07,  0.0000e+00,  2.3842e-07,  2.3842e-07,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00])\n",
      "tensor([-4.7684e-07, -1.1921e-07,  0.0000e+00,  2.3842e-07,  5.9605e-08,\n",
      "         1.1921e-07, -1.1921e-07,  9.5367e-07, -4.7684e-07,  1.1921e-07,\n",
      "         1.1921e-07,  5.9605e-08])\n",
      "tensor([-1.1921e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.9605e-08,\n",
      "        -1.1921e-07,  1.1921e-07,  0.0000e+00,  5.9605e-08,  0.0000e+00,\n",
      "         2.3842e-07, -5.9605e-08])\n",
      "tensor([-2.3842e-07, -1.1921e-07,  1.1921e-07, -2.3842e-07,  2.3842e-07,\n",
      "        -2.3842e-07,  5.9605e-08, -2.3842e-07,  0.0000e+00,  2.3842e-07,\n",
      "        -4.7684e-07,  0.0000e+00])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating group acc:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8822920262340352\n",
      "0: 0.9937915742793791\n",
      "1: 0.8053215077605321\n",
      "2: 0.6900311526479751\n",
      "3: 0.9532710280373832\n",
      "0.4\n",
      "Arch:vit_b_16\n",
      "tensor([ 1.1921e-07,  5.9605e-08,  0.0000e+00,  1.1921e-07, -5.9605e-08,\n",
      "         0.0000e+00, -4.7684e-07,  2.3842e-07,  0.0000e+00,  0.0000e+00,\n",
      "        -2.3842e-07,  0.0000e+00])\n",
      "tensor([ 0.0000e+00, -1.1921e-07,  0.0000e+00,  0.0000e+00,  1.1921e-07,\n",
      "         0.0000e+00, -3.5763e-07,  0.0000e+00,  4.7684e-07,  5.9605e-08,\n",
      "         0.0000e+00, -2.3842e-07])\n",
      "tensor([ 0.0000e+00, -1.1921e-07, -5.9605e-08,  1.1921e-07, -1.1921e-07,\n",
      "         9.5367e-07, -1.1921e-07,  0.0000e+00, -2.3842e-07,  0.0000e+00,\n",
      "        -9.5367e-07,  0.0000e+00])\n",
      "tensor([ 0.0000e+00, -1.1921e-07,  0.0000e+00, -2.3842e-07, -2.3842e-07,\n",
      "        -9.5367e-07, -4.7684e-07, -9.5367e-07, -1.1921e-07,  2.3842e-07,\n",
      "        -2.3842e-07,  2.3842e-07])\n",
      "tensor([9.5367e-07, 9.5367e-07, 0.0000e+00, 1.4305e-06, 4.7684e-07, 1.1921e-07,\n",
      "        0.0000e+00, 4.7684e-07, 0.0000e+00, 9.5367e-07, 0.0000e+00, 2.3842e-07])\n",
      "tensor([ 0.0000e+00,  1.1921e-07,  7.1526e-07,  2.3842e-07,  0.0000e+00,\n",
      "         9.5367e-07,  2.3842e-07,  0.0000e+00,  4.7684e-07,  0.0000e+00,\n",
      "        -4.7684e-07,  4.7684e-07])\n",
      "tensor([ 0.0000e+00, -7.1526e-07,  0.0000e+00, -9.5367e-07, -4.7684e-07,\n",
      "        -2.3842e-07, -2.3842e-07,  0.0000e+00, -2.3842e-07, -2.3842e-07,\n",
      "         0.0000e+00, -2.3842e-07])\n",
      "tensor([-4.7684e-07,  9.5367e-07,  7.1526e-07,  0.0000e+00, -2.3842e-07,\n",
      "         0.0000e+00,  0.0000e+00, -2.3842e-07, -2.3842e-07,  9.5367e-07,\n",
      "        -4.7684e-07, -2.3842e-07])\n",
      "tensor([ 0.0000e+00, -2.3842e-07, -4.7684e-07,  0.0000e+00, -9.5367e-07,\n",
      "        -1.1921e-07,  0.0000e+00,  9.5367e-07, -2.3842e-07,  1.1921e-07,\n",
      "         4.7684e-07, -2.3842e-07])\n",
      "tensor([-1.1921e-07, -1.1921e-07,  0.0000e+00,  2.3842e-07,  1.1921e-07,\n",
      "         2.3842e-07,  0.0000e+00,  4.7684e-07, -4.7684e-07,  0.0000e+00,\n",
      "        -1.1921e-07,  5.9605e-08])\n",
      "tensor([-1.1921e-07,  0.0000e+00,  0.0000e+00,  2.3842e-07,  5.9605e-08,\n",
      "        -3.5763e-07,  1.1921e-07,  0.0000e+00,  5.9605e-08,  0.0000e+00,\n",
      "         0.0000e+00, -5.9605e-08])\n",
      "tensor([ 2.3842e-07, -1.1921e-07,  2.3842e-07,  2.3842e-07,  0.0000e+00,\n",
      "        -1.1921e-07,  5.9605e-08, -2.3842e-07,  0.0000e+00,  0.0000e+00,\n",
      "        -2.3842e-07, -1.1921e-07])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating group acc:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8717638936831205\n",
      "0: 0.9915742793791574\n",
      "1: 0.7849223946784922\n",
      "2: 0.67601246105919\n",
      "3: 0.9517133956386293\n",
      "0.5\n",
      "Arch:vit_b_16\n",
      "tensor([ 1.1921e-07,  1.1921e-07, -5.9605e-08,  2.3842e-07, -1.1921e-07,\n",
      "         0.0000e+00, -4.7684e-07,  0.0000e+00,  1.1921e-07,  1.1921e-07,\n",
      "        -2.3842e-07, -5.9605e-08])\n",
      "tensor([ 5.9605e-08,  0.0000e+00, -4.7684e-07, -5.9605e-08,  1.1921e-07,\n",
      "         0.0000e+00, -2.3842e-07,  5.9605e-08, -4.7684e-07,  5.9605e-08,\n",
      "         0.0000e+00,  1.1921e-07])\n",
      "tensor([-5.9605e-08, -1.1921e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         9.5367e-07,  0.0000e+00,  5.9605e-08,  0.0000e+00, -1.1921e-07,\n",
      "        -9.5367e-07, -4.7684e-07])\n",
      "tensor([ 0.0000e+00,  0.0000e+00, -2.3842e-07,  1.1921e-07, -2.3842e-07,\n",
      "        -9.5367e-07, -4.7684e-07,  0.0000e+00,  0.0000e+00, -4.7684e-07,\n",
      "        -2.3842e-07,  9.5367e-07])\n",
      "tensor([ 4.7684e-07,  7.1526e-07,  2.3842e-07,  0.0000e+00,  0.0000e+00,\n",
      "        -1.1921e-07,  4.7684e-07,  4.7684e-07, -4.7684e-07,  9.5367e-07,\n",
      "         2.3842e-07, -2.3842e-07])\n",
      "tensor([ 0.0000e+00,  0.0000e+00,  4.7684e-07, -2.3842e-07,  4.7684e-07,\n",
      "         9.5367e-07,  2.3842e-07,  4.7684e-07,  4.7684e-07,  9.5367e-07,\n",
      "         0.0000e+00,  2.3842e-07])\n",
      "tensor([ 0.0000e+00, -4.7684e-07,  2.3842e-07, -4.7684e-07,  0.0000e+00,\n",
      "        -2.3842e-07, -2.3842e-07, -4.7684e-07, -4.7684e-07,  0.0000e+00,\n",
      "         9.5367e-07, -2.3842e-07])\n",
      "tensor([-7.1526e-07,  9.5367e-07,  4.7684e-07,  4.7684e-07,  0.0000e+00,\n",
      "        -4.7684e-07, -2.3842e-07, -2.3842e-07,  7.1526e-07,  2.3842e-07,\n",
      "         0.0000e+00,  0.0000e+00])\n",
      "tensor([ 2.3842e-07, -4.7684e-07,  0.0000e+00, -1.1921e-07,  0.0000e+00,\n",
      "        -2.3842e-07,  4.7684e-07,  2.3842e-07, -2.3842e-07,  2.3842e-07,\n",
      "         2.3842e-07,  2.3842e-07])\n",
      "tensor([-1.1921e-07, -1.1921e-07,  1.1921e-07,  4.7684e-07,  0.0000e+00,\n",
      "         2.3842e-07,  0.0000e+00,  7.1526e-07,  0.0000e+00,  1.1921e-07,\n",
      "         0.0000e+00,  1.1921e-07])\n",
      "tensor([-2.3842e-07,  0.0000e+00,  2.3842e-07,  1.1921e-07,  1.7881e-07,\n",
      "        -1.1921e-07,  1.1921e-07,  0.0000e+00, -5.9605e-08,  2.3842e-07,\n",
      "         0.0000e+00,  0.0000e+00])\n",
      "tensor([-1.1921e-07, -1.1921e-07,  0.0000e+00,  4.7684e-07,  1.1921e-07,\n",
      "         0.0000e+00,  5.9605e-08,  0.0000e+00,  0.0000e+00,  2.3842e-07,\n",
      "        -4.7684e-07, -1.1921e-07])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating group acc:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8816016568864342\n",
      "0: 0.9831485587583149\n",
      "1: 0.7955654101995565\n",
      "2: 0.7632398753894081\n",
      "3: 0.9454828660436138\n",
      "0.6\n",
      "Arch:vit_b_16\n",
      "tensor([ 1.1921e-07,  1.1921e-07,  1.1921e-07,  2.3842e-07, -5.9605e-08,\n",
      "         0.0000e+00,  0.0000e+00,  2.3842e-07,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00])\n",
      "tensor([ 0.0000e+00,  0.0000e+00,  4.7684e-07,  0.0000e+00,  1.1921e-07,\n",
      "         2.9802e-08, -1.1921e-07,  0.0000e+00,  4.7684e-07,  0.0000e+00,\n",
      "         0.0000e+00, -2.3842e-07])\n",
      "tensor([ 5.9605e-08, -1.1921e-07, -5.9605e-08,  0.0000e+00,  1.1921e-07,\n",
      "         9.5367e-07,  0.0000e+00,  0.0000e+00,  2.3842e-07, -1.1921e-07,\n",
      "         0.0000e+00, -4.7684e-07])\n",
      "tensor([-4.7684e-07,  0.0000e+00, -2.3842e-07, -1.1921e-07,  2.3842e-07,\n",
      "        -4.7684e-07,  0.0000e+00,  0.0000e+00, -1.1921e-07, -2.3842e-07,\n",
      "        -2.3842e-07,  2.3842e-07])\n",
      "tensor([ 4.7684e-07,  4.7684e-07,  0.0000e+00,  4.7684e-07,  0.0000e+00,\n",
      "         0.0000e+00,  4.7684e-07,  4.7684e-07, -4.7684e-07,  4.7684e-07,\n",
      "         0.0000e+00,  0.0000e+00])\n",
      "tensor([ 0.0000e+00,  0.0000e+00,  4.7684e-07,  0.0000e+00,  4.7684e-07,\n",
      "         0.0000e+00,  4.7684e-07,  2.3842e-07,  4.7684e-07,  9.5367e-07,\n",
      "        -4.7684e-07,  2.3842e-07])\n",
      "tensor([ 2.3842e-07, -4.7684e-07,  2.3842e-07,  0.0000e+00,  0.0000e+00,\n",
      "         4.7684e-07, -2.3842e-07,  0.0000e+00,  0.0000e+00, -2.3842e-07,\n",
      "        -4.7684e-07, -2.3842e-07])\n",
      "tensor([ 0.0000e+00,  0.0000e+00,  2.3842e-07,  2.3842e-07,  0.0000e+00,\n",
      "        -2.3842e-07,  0.0000e+00,  0.0000e+00,  2.3842e-07,  2.3842e-07,\n",
      "         2.3842e-07,  0.0000e+00])\n",
      "tensor([ 2.3842e-07,  0.0000e+00, -4.7684e-07,  0.0000e+00,  0.0000e+00,\n",
      "        -2.3842e-07,  0.0000e+00,  4.7684e-07, -2.3842e-07,  0.0000e+00,\n",
      "         2.3842e-07,  2.3842e-07])\n",
      "tensor([-2.3842e-07, -1.1921e-07,  2.3842e-07,  4.7684e-07,  5.9605e-08,\n",
      "         2.3842e-07,  1.1921e-07,  7.1526e-07, -2.3842e-07,  1.1921e-07,\n",
      "         0.0000e+00,  1.1921e-07])\n",
      "tensor([-1.1921e-07,  0.0000e+00,  2.3842e-07,  2.3842e-07,  2.3842e-07,\n",
      "        -1.1921e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -5.9605e-08])\n",
      "tensor([-1.1921e-07, -1.1921e-07,  0.0000e+00,  2.3842e-07,  2.3842e-07,\n",
      "         0.0000e+00,  5.9605e-08,  0.0000e+00,  0.0000e+00, -2.3842e-07,\n",
      "        -9.5367e-07,  1.1921e-07])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating group acc:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.877114256127028\n",
      "0: 0.9849223946784922\n",
      "1: 0.8062084257206208\n",
      "2: 0.6947040498442367\n",
      "3: 0.9299065420560748\n",
      "0.7\n",
      "Arch:vit_b_16\n",
      "tensor([ 0.0000e+00,  1.7881e-07,  5.9605e-08,  1.1921e-07, -5.9605e-08,\n",
      "        -5.9605e-08,  0.0000e+00,  2.3842e-07,  0.0000e+00,  2.3842e-07,\n",
      "        -2.3842e-07, -5.9605e-08])\n",
      "tensor([ 0.0000e+00, -1.1921e-07, -9.5367e-07,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -1.1921e-07,  5.9605e-08,  0.0000e+00, -1.1921e-07,\n",
      "         0.0000e+00,  0.0000e+00])\n",
      "tensor([ 0.0000e+00,  0.0000e+00, -2.9802e-08,  1.1921e-07, -1.1921e-07,\n",
      "         0.0000e+00,  0.0000e+00, -5.9605e-08,  2.3842e-07, -2.3842e-07,\n",
      "        -9.5367e-07, -4.7684e-07])\n",
      "tensor([-4.7684e-07,  1.1921e-07,  0.0000e+00,  0.0000e+00, -2.3842e-07,\n",
      "        -9.5367e-07, -4.7684e-07,  4.7684e-07,  1.1921e-07, -4.7684e-07,\n",
      "         2.3842e-07,  4.7684e-07])\n",
      "tensor([ 4.7684e-07,  2.3842e-07,  0.0000e+00,  0.0000e+00, -2.3842e-07,\n",
      "        -2.3842e-07,  0.0000e+00,  4.7684e-07, -9.5367e-07,  0.0000e+00,\n",
      "         2.3842e-07, -2.3842e-07])\n",
      "tensor([-4.7684e-07, -1.1921e-07,  7.1526e-07, -2.3842e-07,  2.3842e-07,\n",
      "         9.5367e-07, -2.3842e-07,  2.3842e-07,  0.0000e+00,  1.4305e-06,\n",
      "         4.7684e-07,  2.3842e-07])\n",
      "tensor([-2.3842e-07, -2.3842e-07,  0.0000e+00, -9.5367e-07,  4.7684e-07,\n",
      "        -2.3842e-07,  2.3842e-07, -4.7684e-07, -4.7684e-07, -2.3842e-07,\n",
      "         0.0000e+00, -4.7684e-07])\n",
      "tensor([-4.7684e-07,  9.5367e-07,  7.1526e-07,  0.0000e+00, -2.3842e-07,\n",
      "        -4.7684e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.7684e-07,\n",
      "         2.3842e-07,  0.0000e+00])\n",
      "tensor([ 0.0000e+00, -2.3842e-07,  0.0000e+00, -1.1921e-07, -4.7684e-07,\n",
      "        -1.1921e-07,  0.0000e+00,  4.7684e-07, -4.7684e-07,  3.5763e-07,\n",
      "        -2.3842e-07,  0.0000e+00])\n",
      "tensor([-1.1921e-07, -4.7684e-07,  2.3842e-07,  7.1526e-07,  5.9605e-08,\n",
      "         3.5763e-07,  1.1921e-07,  7.1526e-07, -4.7684e-07,  0.0000e+00,\n",
      "         2.3842e-07, -5.9605e-08])\n",
      "tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.1921e-07,  5.9605e-08,\n",
      "        -3.5763e-07,  1.1921e-07,  1.1921e-07, -5.9605e-08,  1.1921e-07,\n",
      "        -4.7684e-07, -5.9605e-08])\n",
      "tensor([ 1.1921e-07,  1.1921e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  5.9605e-08,  0.0000e+00, -2.3842e-07,  4.7684e-07,\n",
      "        -2.3842e-07, -1.1921e-07])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating group acc:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3781498101484294\n",
      "0: 0.3964523281596452\n",
      "1: 0.05942350332594235\n",
      "2: 0.82398753894081\n",
      "3: 0.9875389408099688\n"
     ]
    }
   ],
   "source": [
    "prev_rel = {k: v.detach().clone() for k, v in components_relevances.items()}\n",
    "for r in [0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    print(r)\n",
    "    del model\n",
    "    model = ModelLoader.get_basic_model(\"vit_b_16\", \"pruned_vit_b16_2cls_clean.pth\", device, num_classes=2)\n",
    "\n",
    "    hook_handles = apply_mask_softmax(model, global_pruning_mask)\n",
    "    \n",
    "    component_attributor = ComponentAttibution(\n",
    "            \"Relevance\",\n",
    "            \"ViT\",\n",
    "            layer_types[layer_type],\n",
    "            True\n",
    "        )\n",
    "    \n",
    "    components_relevances = component_attributor.attribute(\n",
    "            model,\n",
    "            prune_dataloader,\n",
    "            composite,\n",
    "            abs_flag=True,\n",
    "            device=device,\n",
    "        )\n",
    "    \n",
    "    \n",
    "    global_pruning_mask = pruner.generate_global_pruning_mask(\n",
    "                    model,\n",
    "                    components_relevances,\n",
    "                    r,\n",
    "                    subsequent_layer_pruning=\"Linear\",\n",
    "                    least_relevant_first=True,\n",
    "                    device=device,\n",
    "                )\n",
    "    for k in components_relevances:\n",
    "        diff = components_relevances[k] - prev_rel[k]\n",
    "        print(diff)\n",
    "    \n",
    "    hook_handles = pruner.fit_pruning_mask(model, global_pruning_mask)\n",
    "    \n",
    "    acc, acc_groups = compute_worst_accuracy(\n",
    "            model,\n",
    "            val_dataloader,\n",
    "            device,\n",
    "        )\n",
    "    print(acc)\n",
    "    for i in range(4):\n",
    "        print(f'{i}: {acc_groups[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e94b080d-5941-4c97-a2bf-f33b5bd08947",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor r in [0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\\n    print(r)\\n# 1. Grab your pruned model’s wrapped state dict\\n    pruned_sd = model.state_dict()\\n    \\n    # 2. Strip off any wrapper-module names\\n    clean_sd = {}\\n    for k, v in pruned_sd.items():\\n        # Drop functorch wrapper layers named “module”\\n        new_k = re.sub(r\\'\\\\.module\\\\.\\', \\'.\\', k)\\n        clean_sd[new_k] = v\\n    \\n    # 3. For each attention layer, rebuild the combined in_proj and out_proj keys\\n    D = 768\\n    num_layers = 12\\n    for i in range(num_layers):\\n        prefix = f\"encoder.layers.encoder_layer_{i}.self_attention\"\\n    \\n        # stack Q/K/V weights\\n        w_q = clean_sd.pop(f\"{prefix}.q_proj.proj_weight\")\\n        w_k = clean_sd.pop(f\"{prefix}.k_proj.proj_weight\")\\n        w_v = clean_sd.pop(f\"{prefix}.v_proj.proj_weight\")\\n        clean_sd[f\"{prefix}.in_proj_weight\"] = torch.cat([w_q, w_k, w_v], dim=0)\\n    \\n        # stack Q/K/V biases\\n        b_q = clean_sd.pop(f\"{prefix}.q_proj.proj_bias\")\\n        b_k = clean_sd.pop(f\"{prefix}.k_proj.proj_bias\")\\n        b_v = clean_sd.pop(f\"{prefix}.v_proj.proj_bias\")\\n        clean_sd[f\"{prefix}.in_proj_bias\"] = torch.cat([b_q, b_k, b_v], dim=0)\\n    \\n        # rename out_proj\\n        clean_sd[f\"{prefix}.out_proj.weight\"] = clean_sd.pop(f\"{prefix}.out_proj.proj_weight\")\\n        clean_sd[f\"{prefix}.out_proj.bias\"]   = clean_sd.pop(f\"{prefix}.out_proj.proj_bias\")\\n    \\n    # 4. Instantiate fresh ViT-B/16 with 2‐way head\\n    del model\\n    model = vit_b_16(weights=False, num_classes=2)\\n    \\n    # 5. Load your rebuilt dict (strict=True now that everything matches)\\n    missing, unexpected = model.load_state_dict(clean_sd, strict=True)\\n\\n    \\n    # 6. Done—now you can save and re‐load without wrappers\\n    torch.save(model.state_dict(), \"pruned_vit_b16_2cls_clean.pth\")\\n    \\n    model = ModelLoader.get_basic_model(\"vit_b_16\", \"pruned_vit_b16_2cls_clean.pth\", device, num_classes=2)\\n    \\n    component_attributor2 = ComponentAttibution(\\n            \"Relevance\",\\n            \"ViT\",\\n            layer_types[layer_type],\\n            True\\n        )\\n    \\n    components_relevances2 = component_attributor2.attribute(\\n            model,\\n            prune_dataloader,\\n            composite,\\n            abs_flag=True,\\n            device=device,\\n        )\\n    \\n    \\n    global_pruning_mask2 = pruner.generate_global_pruning_mask(\\n                    model,\\n                    components_relevances2,\\n                    r,\\n                    subsequent_layer_pruning=\"Linear\",\\n                    least_relevant_first=True,\\n                    device=device,\\n                )\\n    hook_handles = pruner.fit_pruning_mask(model, global_pruning_mask2,)\\n    \\n    acc, acc_groups = compute_worst_accuracy(\\n            model,\\n            val_dataloader,\\n            device,\\n        )\\n    print(acc)\\n    for i in range(4):\\n        print(f\\'{i}: {acc_groups[i]}\\')\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for r in [0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    print(r)\n",
    "# 1. Grab your pruned model’s wrapped state dict\n",
    "    pruned_sd = model.state_dict()\n",
    "    \n",
    "    # 2. Strip off any wrapper-module names\n",
    "    clean_sd = {}\n",
    "    for k, v in pruned_sd.items():\n",
    "        # Drop functorch wrapper layers named “module”\n",
    "        new_k = re.sub(r'\\.module\\.', '.', k)\n",
    "        clean_sd[new_k] = v\n",
    "    \n",
    "    # 3. For each attention layer, rebuild the combined in_proj and out_proj keys\n",
    "    D = 768\n",
    "    num_layers = 12\n",
    "    for i in range(num_layers):\n",
    "        prefix = f\"encoder.layers.encoder_layer_{i}.self_attention\"\n",
    "    \n",
    "        # stack Q/K/V weights\n",
    "        w_q = clean_sd.pop(f\"{prefix}.q_proj.proj_weight\")\n",
    "        w_k = clean_sd.pop(f\"{prefix}.k_proj.proj_weight\")\n",
    "        w_v = clean_sd.pop(f\"{prefix}.v_proj.proj_weight\")\n",
    "        clean_sd[f\"{prefix}.in_proj_weight\"] = torch.cat([w_q, w_k, w_v], dim=0)\n",
    "    \n",
    "        # stack Q/K/V biases\n",
    "        b_q = clean_sd.pop(f\"{prefix}.q_proj.proj_bias\")\n",
    "        b_k = clean_sd.pop(f\"{prefix}.k_proj.proj_bias\")\n",
    "        b_v = clean_sd.pop(f\"{prefix}.v_proj.proj_bias\")\n",
    "        clean_sd[f\"{prefix}.in_proj_bias\"] = torch.cat([b_q, b_k, b_v], dim=0)\n",
    "    \n",
    "        # rename out_proj\n",
    "        clean_sd[f\"{prefix}.out_proj.weight\"] = clean_sd.pop(f\"{prefix}.out_proj.proj_weight\")\n",
    "        clean_sd[f\"{prefix}.out_proj.bias\"]   = clean_sd.pop(f\"{prefix}.out_proj.proj_bias\")\n",
    "    \n",
    "    # 4. Instantiate fresh ViT-B/16 with 2‐way head\n",
    "    del model\n",
    "    model = vit_b_16(weights=False, num_classes=2)\n",
    "    \n",
    "    # 5. Load your rebuilt dict (strict=True now that everything matches)\n",
    "    missing, unexpected = model.load_state_dict(clean_sd, strict=True)\n",
    "\n",
    "    \n",
    "    # 6. Done—now you can save and re‐load without wrappers\n",
    "    torch.save(model.state_dict(), \"pruned_vit_b16_2cls_clean.pth\")\n",
    "    \n",
    "    model = ModelLoader.get_basic_model(\"vit_b_16\", \"pruned_vit_b16_2cls_clean.pth\", device, num_classes=2)\n",
    "    \n",
    "    component_attributor2 = ComponentAttibution(\n",
    "            \"Relevance\",\n",
    "            \"ViT\",\n",
    "            layer_types[layer_type],\n",
    "            True\n",
    "        )\n",
    "    \n",
    "    components_relevances2 = component_attributor2.attribute(\n",
    "            model,\n",
    "            prune_dataloader,\n",
    "            composite,\n",
    "            abs_flag=True,\n",
    "            device=device,\n",
    "        )\n",
    "    \n",
    "    \n",
    "    global_pruning_mask2 = pruner.generate_global_pruning_mask(\n",
    "                    model,\n",
    "                    components_relevances2,\n",
    "                    r,\n",
    "                    subsequent_layer_pruning=\"Linear\",\n",
    "                    least_relevant_first=True,\n",
    "                    device=device,\n",
    "                )\n",
    "    hook_handles = pruner.fit_pruning_mask(model, global_pruning_mask2,)\n",
    "    \n",
    "    acc, acc_groups = compute_worst_accuracy(\n",
    "            model,\n",
    "            val_dataloader,\n",
    "            device,\n",
    "        )\n",
    "    print(acc)\n",
    "    for i in range(4):\n",
    "        print(f'{i}: {acc_groups[i]}')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f6c9af3-4d05-40f3-868b-bdf3243a6e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model2.load_state_dict(torch.load(\"intermediate_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48be6772-7062-432f-a99a-60620cd595b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a825cf6b-edf9-4070-a323-a3d9c3fd6fca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e794c310-4678-45ec-8ffc-bc98df12c9ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'global_pruning_mask2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mglobal_pruning_mask2\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'global_pruning_mask2' is not defined"
     ]
    }
   ],
   "source": [
    "global_pruning_mask2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65b272f-9475-4178-9147-ac499c22707e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## pruned 10% with prune set 1 and 10% with prune set 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2424feb9-7911-4016-a65e-dae746c8370f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_worst, acc_groups = compute_worst_accuracy(\n",
    "        model2,\n",
    "        val_dataloader,\n",
    "        device,\n",
    "    )\n",
    "for i in range(4):\n",
    "    print(f'{i}: {acc_groups[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00201b11-d856-4b0e-8ac8-a2c34cf7e0e2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_pruning_mask = pruner.generate_global_pruning_mask(\n",
    "                model,\n",
    "                components_relevances,\n",
    "                0.15,\n",
    "                subsequent_layer_pruning=\"Linear\",\n",
    "                least_relevant_first=True,\n",
    "                device=device,\n",
    "            )\n",
    "hook_handles = pruner.fit_pruning_mask(model, global_pruning_mask,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee3acfc-a215-40a3-a267-6eb8a48bd2e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## pruned with prune set 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca423478-06c0-46aa-931b-b0018bec7adb",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_worst, acc_groups = compute_worst_accuracy(\n",
    "        model,\n",
    "        val_dataloader,\n",
    "        device,\n",
    "    )\n",
    "for i in range(4):\n",
    "    print(f'{i}: {acc_groups[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d8b9c-a35d-498d-afa7-8443a6476558",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "components_relevances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5929ced-7d2e-4ad8-a983-19089b5fc31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder.layers[0].mlp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf711be-8138-4d25-a5c2-97db0ed3b301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "components_relevances2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caa3dbf-889f-4ddc-ad09-10a166100f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "in_dim     = 2\n",
    "hidden_dim = 3\n",
    "out_dim    = 2\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim,   hidden_dim, bias=True)\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim, bias=True)\n",
    "\n",
    "        self.a1 = None\n",
    "        self.a2 = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.a1 = F.relu(self.fc1(x))\n",
    "        self.a2 = F.relu(self.fc2(self.a1))\n",
    "        return self.a2\n",
    "\n",
    "torch.manual_seed(42)\n",
    "net = SimpleMLP(in_dim, hidden_dim, out_dim)\n",
    "\n",
    "x = torch.randn(batch_size, in_dim)\n",
    "logits = net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bd6d62-c198-4414-9354-00dbb26e7074",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = net(x)\n",
    "\n",
    "print('x = ', x)\n",
    "print()\n",
    "print('w1 = ', net.fc1.weight)\n",
    "print('b1 = ', net.fc1.bias)\n",
    "print('a1 = ', net.a1)\n",
    "print()\n",
    "\n",
    "print('w2 = ', net.fc2.weight)\n",
    "print('b2 = ', net.fc2.bias)\n",
    "print('a2 = ', net.a2)\n",
    "print()\n",
    "\n",
    "print(logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfr2",
   "language": "python",
   "name": "dfr2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
