{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c79d284bb264d1f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T21:39:59.236365Z",
     "start_time": "2025-07-02T21:39:45.451882600Z"
    },
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import click\n",
    "import torch\n",
    "import tqdm.auto\n",
    "import numpy as np\n",
    "from torchvision.models import vit_b_16\n",
    " \n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "#project_root = \"C:/Users/elmop/deep_feature_reweighting/deep_feature_reweighting/external/pruning_by_explaining\"\n",
    "project_root = \"/home/primmere/ide/external/pruning_by_explaining\"\n",
    "sys.path.insert(0, project_root)                 \n",
    "sys.path.insert(0, os.path.dirname(project_root))\n",
    "\n",
    "from pruning_by_explaining.models import ModelLoader\n",
    "from pruning_by_explaining.metrics import compute_accuracy\n",
    "from pruning_by_explaining.my_metrics import compute_worst_accuracy\n",
    "from pruning_by_explaining.my_datasets import WaterBirds, get_sample_indices_for_group, WaterBirdSubset, ISIC, ISICSubset\n",
    "\n",
    "\n",
    "from pruning_by_explaining.pxp import (\n",
    "    ModelLayerUtils,\n",
    "    get_cnn_composite,\n",
    "    get_vit_composite,\n",
    ")\n",
    "\n",
    "from pruning_by_explaining.pxp import GlobalPruningOperations\n",
    "from pruning_by_explaining.pxp import ComponentAttibution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b41d0ce48cf8cb4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T08:24:20.023885700Z",
     "start_time": "2025-07-02T08:24:19.673321500Z"
    },
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n",
      "Number of unique labels: 2, Number of unique places: 2, Total groups: 4\n",
      "group 0: 3518\n",
      "group 1: 185\n",
      "group 2: 55\n",
      "group 3: 1037\n",
      "[0 1 2 3]\n",
      "Number of unique labels: 2, Number of unique places: 2, Total groups: 4\n",
      "group 0: 456\n",
      "group 1: 456\n",
      "group 2: 143\n",
      "group 3: 144\n",
      "[0 1 2 3]\n",
      "Number of unique labels: 2, Number of unique places: 2, Total groups: 4\n",
      "group 0: 2255\n",
      "group 1: 2255\n",
      "group 2: 642\n",
      "group 3: 642\n",
      "target groups: [1, 2]\n",
      "target groups: [1, 2]\n",
      "target groups: [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "num_workers = 4\n",
    "device_string = \"cuda\"\n",
    "device = torch.device(device_string)\n",
    "waterbirds = WaterBirds('/scratch_shared/primmere/waterbird', seed = 1, num_workers = num_workers)\n",
    "\n",
    "train_set = waterbirds.get_train_set()\n",
    "val_set = waterbirds.get_valid_set()\n",
    "test_set = waterbirds.get_test_set()\n",
    "\n",
    "pruning_indices = get_sample_indices_for_group(val_set, 10, device_string, [1,2])\n",
    "pruning_indices2 = get_sample_indices_for_group(val_set, 10, device_string, [1,2])\n",
    "validation_indices = get_sample_indices_for_group(test_set, 'all', device_string)\n",
    "\n",
    "\n",
    "custom_pruning_set = WaterBirdSubset(val_set, pruning_indices)\n",
    "custom_pruning_set2 = WaterBirdSubset(val_set, pruning_indices2)\n",
    "custom_val_set = WaterBirdSubset(test_set, validation_indices)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=num_workers)\n",
    "prune_dataloader = torch.utils.data.DataLoader(custom_pruning_set, batch_size=1, shuffle=True, num_workers=num_workers)\n",
    "prune_dataloader2 = torch.utils.data.DataLoader(custom_pruning_set2, batch_size=1, shuffle=True, num_workers=num_workers)\n",
    "val_dataloader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "672071d5bfd8ce01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T21:40:07.494175Z",
     "start_time": "2025-07-02T21:39:59.246373Z"
    },
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arch:vit_b_16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/primmere/ide/external/pruning_by_explaining/models/utils.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "suggested_composite = {\n",
    "        \"low_level_hidden_layer_rule\": \"Epsilon\",\n",
    "        \"mid_level_hidden_layer_rule\":\"Epsilon\",\n",
    "        \"high_level_hidden_layer_rule\": \"Epsilon\",\n",
    "        \"fully_connected_layers_rule\": \"Epsilon\",\n",
    "        \"softmax_rule\": \"Epsilon\",\n",
    "    }\n",
    "composite = get_vit_composite(\"vit_b_16\", suggested_composite)\n",
    "\n",
    "model = ModelLoader.get_basic_model(\"vit_b_16\", \"/home/primmere/ide/dfr/logs/vit_waterbirds.pth\", device, num_classes=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2053c4-6614-46aa-be9f-b59ef6189006",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abe0bc4-43a1-44f1-99d8-344f114212a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "218d21ab3e4aedec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T08:24:27.257200600Z",
     "start_time": "2025-07-02T08:24:27.243210Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "layer_types = {\n",
    "        \"Softmax\": torch.nn.Softmax,\n",
    "        \"Linear\": torch.nn.Linear,\n",
    "        \"Conv2d\": torch.nn.Conv2d,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d81f50ac019024a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T08:38:35.193425200Z",
     "start_time": "2025-07-02T08:25:19.241528900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20258526-a237-412b-957e-986133fc5f23",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating group acc:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.9937915742793791\n",
      "1: 0.7835920177383592\n",
      "2: 0.7461059190031153\n",
      "3: 0.956386292834891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n0: 0.9937915742793791\\n1: 0.7835920177383592\\n2: 0.7461059190031153\\n3: 0.956386292834891\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "acc_worst, acc_groups = compute_worst_accuracy(\n",
    "        model,\n",
    "        val_dataloader,\n",
    "        device,\n",
    "    )\n",
    "for i in range(4):\n",
    "    print(f'{i}: {acc_groups[i]}')\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "0: 0.9937915742793791\n",
    "1: 0.7835920177383592\n",
    "2: 0.7461059190031153\n",
    "3: 0.956386292834891\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad22e80f-eeb9-4057-9469-fbc18a92178b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating acc:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8803935105281325\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "acc = compute_accuracy(\n",
    "        model,\n",
    "        val_dataloader,\n",
    "        device,\n",
    "    )\n",
    "print(acc)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ef84cca0140e419",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T08:41:21.019829100Z",
     "start_time": "2025-07-02T08:41:20.959062Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "component_attributor = ComponentAttibution(\n",
    "        \"Relevance\",\n",
    "        \"ViT\",\n",
    "        layer_types['Softmax'],\n",
    "        True\n",
    "    )\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db692b4f-0232-4fc1-87e9-87e70651c0e2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/primmere/.conda/envs/dfr2/lib/python3.10/site-packages/torch/autograd/graph.py:768: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "components_relevances = component_attributor.attribute(\n",
    "        model,\n",
    "        prune_dataloader,\n",
    "        composite,\n",
    "        abs_flag=True,\n",
    "        device=device,\n",
    "    )\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "645c2f52-ba33-491c-8ada-0bc9583f7ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = component_attributor.layer_names\n",
    "pruner = GlobalPruningOperations(\n",
    "        layer_types[\"Softmax\"],\n",
    "        layer_names,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51c48a67-6402-40b0-b4d0-682982946f0f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_pruning_mask = pruner.generate_global_pruning_mask(\n",
    "                model,\n",
    "                components_relevances,\n",
    "                0.05,\n",
    "                subsequent_layer_pruning=\"Both\",\n",
    "                least_relevant_first=True,\n",
    "                device=device,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d23882af-7b24-46f7-b519-38df4c8130a7",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encoder.layers.encoder_layer_0.self_attention.softmax',\n",
       "              tensor([11,  4,  5])),\n",
       "             ('encoder.layers.encoder_layer_1.self_attention.softmax',\n",
       "              tensor([5, 3])),\n",
       "             ('encoder.layers.encoder_layer_2.self_attention.softmax',\n",
       "              tensor([0])),\n",
       "             ('encoder.layers.encoder_layer_3.self_attention.softmax',\n",
       "              tensor([], dtype=torch.int64)),\n",
       "             ('encoder.layers.encoder_layer_4.self_attention.softmax',\n",
       "              tensor([], dtype=torch.int64)),\n",
       "             ('encoder.layers.encoder_layer_5.self_attention.softmax',\n",
       "              tensor([], dtype=torch.int64)),\n",
       "             ('encoder.layers.encoder_layer_6.self_attention.softmax',\n",
       "              tensor([], dtype=torch.int64)),\n",
       "             ('encoder.layers.encoder_layer_7.self_attention.softmax',\n",
       "              tensor([], dtype=torch.int64)),\n",
       "             ('encoder.layers.encoder_layer_8.self_attention.softmax',\n",
       "              tensor([], dtype=torch.int64)),\n",
       "             ('encoder.layers.encoder_layer_9.self_attention.softmax',\n",
       "              tensor([], dtype=torch.int64)),\n",
       "             ('encoder.layers.encoder_layer_10.self_attention.softmax',\n",
       "              tensor([4])),\n",
       "             ('encoder.layers.encoder_layer_11.self_attention.softmax',\n",
       "              tensor([], dtype=torch.int64))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_pruning_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bee4756-02b2-4bd5-a4ed-e891ac93164b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<torch.utils.hooks.RemovableHandle object at 0x14d0d006cdf0>]\n"
     ]
    }
   ],
   "source": [
    "hook_handles = pruner.fit_pruning_mask(model, global_pruning_mask,)\n",
    "print(hook_handles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea6a59ca-654a-4b9d-bdeb-6d3409a82d7d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"acc_worst, acc_groups = compute_worst_accuracy(\\n        model,\\n        val_dataloader,\\n        device,\\n    )\\nfor i in range(4):\\n    print(f'{i}: {acc_groups[i]}')\\n    \""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"acc_worst, acc_groups = compute_worst_accuracy(\n",
    "        model,\n",
    "        val_dataloader,\n",
    "        device,\n",
    "    )\n",
    "for i in range(4):\n",
    "    print(f'{i}: {acc_groups[i]}')\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6748c5d1-a01c-45fb-81b1-3fd6b41bb814",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n0: 0.9868421052631579\\n1: 0.7982456140350878\\n2: 0.7272727272727273\\n3: 0.9791666666666666\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "0: 0.9868421052631579\n",
    "1: 0.7982456140350878\n",
    "2: 0.7272727272727273\n",
    "3: 0.9791666666666666\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99464959-834e-4618-8462-95f7a049ef4c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "component_attributor2 = ComponentAttibution(\n",
    "        \"Relevance\",\n",
    "        \"ViT\",\n",
    "        layer_types['Softmax'],\n",
    "        True\n",
    "    )\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e94b080d-5941-4c97-a2bf-f33b5bd08947",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/primmere/.conda/envs/dfr2/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing: []\n",
      "unexpected: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Grab your pruned model’s wrapped state dict\n",
    "pruned_sd = model.state_dict()\n",
    "\n",
    "# 2. Strip off any wrapper-module names\n",
    "clean_sd = {}\n",
    "for k, v in pruned_sd.items():\n",
    "    # Drop functorch wrapper layers named “module”\n",
    "    new_k = re.sub(r'\\.module\\.', '.', k)\n",
    "    clean_sd[new_k] = v\n",
    "\n",
    "# 3. For each attention layer, rebuild the combined in_proj and out_proj keys\n",
    "D = 768\n",
    "num_layers = 12\n",
    "for i in range(num_layers):\n",
    "    prefix = f\"encoder.layers.encoder_layer_{i}.self_attention\"\n",
    "\n",
    "    # stack Q/K/V weights\n",
    "    w_q = clean_sd.pop(f\"{prefix}.q_proj.proj_weight\")\n",
    "    w_k = clean_sd.pop(f\"{prefix}.k_proj.proj_weight\")\n",
    "    w_v = clean_sd.pop(f\"{prefix}.v_proj.proj_weight\")\n",
    "    clean_sd[f\"{prefix}.in_proj_weight\"] = torch.cat([w_q, w_k, w_v], dim=0)\n",
    "\n",
    "    # stack Q/K/V biases\n",
    "    b_q = clean_sd.pop(f\"{prefix}.q_proj.proj_bias\")\n",
    "    b_k = clean_sd.pop(f\"{prefix}.k_proj.proj_bias\")\n",
    "    b_v = clean_sd.pop(f\"{prefix}.v_proj.proj_bias\")\n",
    "    clean_sd[f\"{prefix}.in_proj_bias\"] = torch.cat([b_q, b_k, b_v], dim=0)\n",
    "\n",
    "    # rename out_proj\n",
    "    clean_sd[f\"{prefix}.out_proj.weight\"] = clean_sd.pop(f\"{prefix}.out_proj.proj_weight\")\n",
    "    clean_sd[f\"{prefix}.out_proj.bias\"]   = clean_sd.pop(f\"{prefix}.out_proj.proj_bias\")\n",
    "\n",
    "# 4. Instantiate fresh ViT-B/16 with 2‐way head\n",
    "model2 = vit_b_16(weights=False, num_classes=2)\n",
    "\n",
    "# 5. Load your rebuilt dict (strict=True now that everything matches)\n",
    "missing, unexpected = model2.load_state_dict(clean_sd, strict=True)\n",
    "print(\"missing:\", missing)         # should be empty\n",
    "print(\"unexpected:\", unexpected)   # should be empty\n",
    "\n",
    "# 6. Done—now you can save and re‐load without wrappers\n",
    "torch.save(model2.state_dict(), \"pruned_vit_b16_2cls_clean.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f6c9af3-4d05-40f3-868b-bdf3243a6e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arch:vit_b_16\n"
     ]
    }
   ],
   "source": [
    "model2 = ModelLoader.get_basic_model(\"vit_b_16\", \"pruned_vit_b16_2cls_clean.pth\", device, num_classes=2)\n",
    "#model2.load_state_dict(torch.load(\"intermediate_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48be6772-7062-432f-a99a-60620cd595b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "components_relevances2 = component_attributor2.attribute(\n",
    "        model2,\n",
    "        prune_dataloader2,\n",
    "        composite,\n",
    "        abs_flag=True,\n",
    "        device=device,\n",
    "    )\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a825cf6b-edf9-4070-a323-a3d9c3fd6fca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_pruning_mask2 = pruner.generate_global_pruning_mask(\n",
    "                model2,\n",
    "                components_relevances2,\n",
    "                0.15,\n",
    "                subsequent_layer_pruning=\"Softmax\",\n",
    "                least_relevant_first=True,\n",
    "                device=device,\n",
    "            )\n",
    "hook_handles = pruner.fit_pruning_mask(model2, global_pruning_mask2,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b65b272f-9475-4178-9147-ac499c22707e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## pruned 10% with prune set 1 and 10% with prune set 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2424feb9-7911-4016-a65e-dae746c8370f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating group acc:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.9920177383592018\n",
      "1: 0.765410199556541\n",
      "2: 0.7492211838006231\n",
      "3: 0.9626168224299065\n"
     ]
    }
   ],
   "source": [
    "acc_worst, acc_groups = compute_worst_accuracy(\n",
    "        model2,\n",
    "        val_dataloader,\n",
    "        device,\n",
    "    )\n",
    "for i in range(4):\n",
    "    print(f'{i}: {acc_groups[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00201b11-d856-4b0e-8ac8-a2c34cf7e0e2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_pruning_mask = pruner.generate_global_pruning_mask(\n",
    "                model,\n",
    "                components_relevances,\n",
    "                0.15,\n",
    "                subsequent_layer_pruning=\"Softmax\",\n",
    "                least_relevant_first=True,\n",
    "                device=device,\n",
    "            )\n",
    "hook_handles = pruner.fit_pruning_mask(model, global_pruning_mask,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ee3acfc-a215-40a3-a267-6eb8a48bd2e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## pruned with prune set 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca423478-06c0-46aa-931b-b0018bec7adb",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating group acc:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.9942350332594235\n",
      "1: 0.7906873614190687\n",
      "2: 0.7258566978193146\n",
      "3: 0.9595015576323987\n"
     ]
    }
   ],
   "source": [
    "acc_worst, acc_groups = compute_worst_accuracy(\n",
    "        model,\n",
    "        val_dataloader,\n",
    "        device,\n",
    "    )\n",
    "for i in range(4):\n",
    "    print(f'{i}: {acc_groups[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb1d8b9c-a35d-498d-afa7-8443a6476558",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encoder.layers.encoder_layer_0.self_attention.softmax',\n",
       "              tensor([0.7201, 0.4474, 0.5789, 0.9859, 0.3713, 0.3811, 2.8528, 1.5384, 0.6803,\n",
       "                      0.7912, 1.8463, 0.3318])),\n",
       "             ('encoder.layers.encoder_layer_1.self_attention.softmax',\n",
       "              tensor([0.6992, 0.9015, 3.2379, 0.3770, 1.3863, 0.1734, 1.1828, 0.6211, 3.5716,\n",
       "                      0.5637, 0.5189, 0.7685])),\n",
       "             ('encoder.layers.encoder_layer_2.self_attention.softmax',\n",
       "              tensor([0.3301, 1.2126, 0.4874, 0.5708, 1.0122, 3.6033, 1.1563, 0.4195, 1.8890,\n",
       "                      1.0220, 3.4656, 4.7292])),\n",
       "             ('encoder.layers.encoder_layer_3.self_attention.softmax',\n",
       "              tensor([2.0331, 0.8815, 1.7531, 0.8091, 1.7675, 4.7154, 3.8396, 4.4762, 0.7078,\n",
       "                      1.6967, 1.4860, 1.5935])),\n",
       "             ('encoder.layers.encoder_layer_4.self_attention.softmax',\n",
       "              tensor([2.6898, 2.5679, 1.7581, 2.6600, 2.0909, 1.2153, 2.6608, 2.5741, 2.8754,\n",
       "                      2.5540, 1.8345, 2.0556])),\n",
       "             ('encoder.layers.encoder_layer_5.self_attention.softmax',\n",
       "              tensor([2.5919, 1.1888, 2.2637, 2.2203, 2.0942, 2.3257, 2.4708, 2.7529, 2.4416,\n",
       "                      3.1752, 2.4522, 1.3107])),\n",
       "             ('encoder.layers.encoder_layer_6.self_attention.softmax',\n",
       "              tensor([2.3665, 2.3762, 2.0128, 2.9219, 3.4167, 1.8103, 2.6533, 1.7833, 1.9164,\n",
       "                      1.7062, 3.5616, 2.0855])),\n",
       "             ('encoder.layers.encoder_layer_7.self_attention.softmax',\n",
       "              tensor([1.6439, 3.4577, 1.4781, 2.3271, 1.3546, 1.7989, 1.8790, 1.7907, 1.8413,\n",
       "                      2.0933, 1.9278, 1.2741])),\n",
       "             ('encoder.layers.encoder_layer_8.self_attention.softmax',\n",
       "              tensor([1.6524, 1.4531, 2.3224, 1.9229, 2.0930, 1.0636, 2.5998, 1.9153, 2.0594,\n",
       "                      1.3831, 2.3136, 3.1770])),\n",
       "             ('encoder.layers.encoder_layer_9.self_attention.softmax',\n",
       "              tensor([0.9292, 1.2149, 0.7146, 2.0148, 0.9446, 1.3658, 0.8056, 3.0534, 1.2078,\n",
       "                      1.0562, 0.6203, 0.4985])),\n",
       "             ('encoder.layers.encoder_layer_10.self_attention.softmax',\n",
       "              tensor([0.7636, 0.5370, 0.7595, 0.6429, 0.4155, 0.7324, 0.8852, 0.9147, 0.9192,\n",
       "                      1.4733, 1.4685, 0.6801])),\n",
       "             ('encoder.layers.encoder_layer_11.self_attention.softmax',\n",
       "              tensor([1.2315, 0.7473, 0.9552, 1.2215, 0.7379, 0.6412, 0.8464, 1.8429, 1.0396,\n",
       "                      1.8587, 1.3874, 0.5679]))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components_relevances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5929ced-7d2e-4ad8-a983-19089b5fc31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=3072, bias=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.layers[0].mlp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "faf711be-8138-4d25-a5c2-97db0ed3b301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encoder.layers.encoder_layer_0.self_attention.softmax',\n",
       "              tensor([0.5133, 0.2549, 0.2424, 0.6850, 0.2456, 0.3346, 1.8293, 0.8924, 0.5772,\n",
       "                      0.5133, 1.0366, 0.1920])),\n",
       "             ('encoder.layers.encoder_layer_1.self_attention.softmax',\n",
       "              tensor([0.2676, 0.4054, 2.0262, 0.2375, 0.7000, 0.0943, 0.5374, 0.3284, 2.0870,\n",
       "                      0.3281, 0.2365, 0.4580])),\n",
       "             ('encoder.layers.encoder_layer_2.self_attention.softmax',\n",
       "              tensor([0.2033, 0.8005, 0.2194, 0.4339, 0.6586, 2.3185, 0.6027, 0.2238, 1.1260,\n",
       "                      0.6274, 2.4814, 3.0106])),\n",
       "             ('encoder.layers.encoder_layer_3.self_attention.softmax',\n",
       "              tensor([1.3536, 0.5563, 1.1859, 0.5837, 1.0808, 3.0699, 2.0863, 2.3576, 0.5441,\n",
       "                      1.2550, 0.9403, 0.9709])),\n",
       "             ('encoder.layers.encoder_layer_4.self_attention.softmax',\n",
       "              tensor([1.7165, 1.4442, 1.1895, 1.6159, 1.2547, 0.5685, 1.6240, 1.8198, 1.8490,\n",
       "                      1.6877, 0.9704, 1.1820])),\n",
       "             ('encoder.layers.encoder_layer_5.self_attention.softmax',\n",
       "              tensor([1.4339, 0.8073, 1.3013, 1.3701, 1.2233, 1.2918, 1.5111, 1.6335, 1.4185,\n",
       "                      1.9296, 1.6341, 0.8698])),\n",
       "             ('encoder.layers.encoder_layer_6.self_attention.softmax',\n",
       "              tensor([1.4934, 1.4905, 1.1543, 1.7706, 2.0547, 1.2273, 1.5795, 0.8872, 1.2338,\n",
       "                      0.9327, 2.1207, 1.2493])),\n",
       "             ('encoder.layers.encoder_layer_7.self_attention.softmax',\n",
       "              tensor([1.1010, 1.9689, 0.9734, 1.4136, 0.8576, 1.2974, 1.3560, 1.0719, 1.2350,\n",
       "                      1.3698, 1.1666, 0.7368])),\n",
       "             ('encoder.layers.encoder_layer_8.self_attention.softmax',\n",
       "              tensor([0.9489, 1.1077, 1.2184, 0.8394, 1.8812, 0.9689, 1.5693, 0.9964, 1.1283,\n",
       "                      1.0510, 1.4137, 1.8439])),\n",
       "             ('encoder.layers.encoder_layer_9.self_attention.softmax',\n",
       "              tensor([0.5116, 0.6882, 0.5228, 0.7720, 0.4345, 0.5216, 0.5018, 2.2789, 0.8323,\n",
       "                      0.5887, 0.3200, 0.3076])),\n",
       "             ('encoder.layers.encoder_layer_10.self_attention.softmax',\n",
       "              tensor([0.5350, 0.2948, 0.6408, 0.4359, 0.3460, 0.4702, 0.4297, 0.6746, 0.6366,\n",
       "                      0.7591, 0.9936, 0.5270])),\n",
       "             ('encoder.layers.encoder_layer_11.self_attention.softmax',\n",
       "              tensor([1.2334, 0.4408, 0.6737, 1.1411, 0.2935, 0.6487, 0.5331, 1.0820, 0.4516,\n",
       "                      1.1841, 1.5261, 0.5762]))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components_relevances2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caa3dbf-889f-4ddc-ad09-10a166100f10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfr2",
   "language": "python",
   "name": "dfr2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
